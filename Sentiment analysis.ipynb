{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitbaseconda7ccc1197ea424c04aa8e970980241a68",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #파이썬으로 gz 파일 압축 풀기\n",
    "# import tarfile\n",
    "# with tarfile.open(r'E:\\Programming\\python\\aclImdb_v1\\aclImdb_v1.tar.gz', 'r:gz') as tar:\n",
    "#     tar.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "0% [##############################] 100% | ETA: 00:00:00\nTotal time elapsed: 00:07:25\n"
    }
   ],
   "source": [
    "import pyprind\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "basepath = r'E:\\Programming\\python\\aclImdb_v1\\aclImdb'\n",
    "\n",
    "##영화 리뷰를 한 줄씩 읽어서 데이터 프레임화 하기\n",
    "labels = {'pos' : 1, 'neg' : 0}\n",
    "pbar = pyprind.ProgBar(50000)\n",
    "df = pd.DataFrame()\n",
    "for s in ('test', 'train'):\n",
    "    for l in ('pos', 'neg'):\n",
    "        path = os.path.join(basepath, s, l)\n",
    "        for file in sorted(os.listdir(path)):\n",
    "            with open(os.path.join(path, file),\n",
    "                      'r', encoding='utf-8') as infile:\n",
    "                txt = infile.read()\n",
    "            df = df.append([[txt, labels[l]]],\n",
    "                           ignore_index=True)\n",
    "            pbar.update()\n",
    "df.columns = ['review', 'sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#클래스 레이블 순서대로 데이터가 나열되있으므로 permutation 함수로 데이터 섞기\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "df.to_csv(r'E:\\Programming\\python\\Machine-Learning\\data\\movie_data.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Election is a Chinese mob movie, or triads in ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I was just watching a Forensic Files marathon ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Police Story is a stunning series of set piece...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                                              review  sentiment\n0  Election is a Chinese mob movie, or triads in ...          1\n1  I was just watching a Forensic Files marathon ...          0\n2  Police Story is a stunning series of set piece...          1"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#데이터 csv파일로 저장하기\n",
    "df = pd.read_csv(r'E:\\Programming\\python\\Machine-Learning\\data\\movie_data.csv', encoding='utf-8')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(50000, 2)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#데이터 프레임 행과 열의 개수 확인하기\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Bag of Word####\n",
    "#1. 전체 문서에 대해 고유한 토큰(예컨데, 단어로 이루어진 어휘 사전) 생성\n",
    "#2. 특정 문서에 각 단어가 얼마나 자주 등장하는지 헤아려 문서의 특성 벡터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###BoW 만들기\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer #CountVectorizer은 문서 또는 문장으로 이루어진 텍스트 데이터 배열을 입력 받아 BoW 모델 생성\n",
    "\n",
    "count = CountVectorizer()\n",
    "docs = np.array([\n",
    "        'The sun is shining',\n",
    "        'The weather is sweet',\n",
    "        'The sun is shining, the weather is sweet, and one and one is two'])\n",
    "bag = count.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "{'the': 6, 'sun': 4, 'is': 1, 'shining': 3, 'weather': 8, 'sweet': 5, 'and': 0, 'one': 2, 'two': 7}\n"
    }
   ],
   "source": [
    "print(count.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[[0 1 0 1 1 0 1 0 0]\n [0 1 0 0 0 1 1 0 1]\n [2 3 2 1 1 1 2 1 1]]\n"
    }
   ],
   "source": [
    "print(bag.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "###tf(단어 빈도)-idf(역문서 빈도)###\n",
    "#특성 벡터에서 자주 등장하는 단어의 가중치를 낮추는 기법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[[0.   0.43 0.   0.56 0.56 0.   0.43 0.   0.  ]\n [0.   0.43 0.   0.   0.   0.56 0.43 0.   0.56]\n [0.5  0.45 0.5  0.19 0.19 0.19 0.3  0.25 0.19]]\n"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer(use_idf=True,\n",
    "                         norm='l2',\n",
    "                         smooth_idf=True)\n",
    "np.set_printoptions(precision=2)\n",
    "print(tfidf.fit_transform(count.fit_transform(docs)).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##불필요한 문자 제거를 통한 텍스트 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'and I suggest that you go see it before you judge.'"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#랜덤하게 섞은 데이터셋의 첫 번째 문서에서 마지막 50개의 문자를 출력\n",
    "df.loc[0, 'review'][-50:] #HTML 마크업 문자와 글자가 아닌 문자 또한 포함되어있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##이모티콘은 정보가 될수 있으므로 제외하고 그 외의 모든 구두점 문자 제거\n",
    "#정규표현식 사용\n",
    "import re\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text) #html 마크업을 삭제하는 정규표현식\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', ''))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'and i suggest that you go see it before you judge '"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor(df.loc[0, 'review'][-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['runners', 'like', 'running', 'and', 'thus', 'they', 'run']"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##공백 문자를 기준으로 단어를 나누기\n",
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "tokenizer('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['runner', 'like', 'run', 'and', 'thu', 'they', 'run']"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##nltke의 어간추출 알고리즘\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "tokenizer_porter('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##불용어##\n",
    "#모든 텍스트에 흔하게 등장하는 단어\n",
    "#tf-idf 보다 기본 단어 빈도나 정규화된 단어 빈도를 사용할 때 유용\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['runner', 'like', 'run', 'run', 'lot']"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#불용어 적용하기\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "[w for w in tokenizer_porter('a runner likes running and runs a lot')[-10:] if w not in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "##영화 리뷰를 긍정과 부정 리뷰로 분리하는 로지스틱 회귀 모델 훈련 시키기##\n",
    "#1. 정제된 텍스트 문서가 저장된 dataframe을 2만 5천개는 훈련 세트, 2만 5천개는 테스트 세트로 나누기\n",
    "\n",
    "X_train = df.loc[:25000, 'review'].values\n",
    "y_train = df.loc[:25000, 'sentiment'].values\n",
    "X_test = df.loc[25000:, 'review'].values\n",
    "y_test = df.loc[25000:, 'sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed: 16.4min\n[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed: 98.1min\n[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed: 135.7min finished\n"
    },
    {
     "data": {
      "text/plain": "GridSearchCV(cv=5, error_score=nan,\n             estimator=Pipeline(memory=None,\n                                steps=[('vect',\n                                        TfidfVectorizer(analyzer='word',\n                                                        binary=False,\n                                                        decode_error='strict',\n                                                        dtype=<class 'numpy.float64'>,\n                                                        encoding='utf-8',\n                                                        input='content',\n                                                        lowercase=False,\n                                                        max_df=1.0,\n                                                        max_features=None,\n                                                        min_df=1,\n                                                        ngram_range=(1, 1),\n                                                        norm='l2',\n                                                        preprocessor=None,\n                                                        smooth_idf=True,\n                                                        stop_words=None,\n                                                        strip_acc...\n                                                'yours', 'yourself',\n                                                'yourselves', 'he', 'him',\n                                                'his', 'himself', 'she',\n                                                \"she's\", 'her', 'hers',\n                                                'herself', 'it', \"it's\", 'its',\n                                                'itself', ...],\n                                               None],\n                          'vect__tokenizer': [<function tokenizer at 0x000002A1C0EC9F78>,\n                                              <function tokenizer_porter at 0x000002A1C2130798>],\n                          'vect__use_idf': [False]}],\n             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n             scoring='accuracy', verbose=1)"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#GridSearchCV 객체에서 5-겹 계층별 교차 검증을 사용하여 최적의 매개변수 조함 확인\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#CountVectorizer와 TfidfTransformer을 합친 기능인 TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(strip_accents=None, \n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)\n",
    "param_grid = [{'vect__ngram_range': [(1,1)],          ##TfidfVectorizer의 기본 매개 변수를 셋팅하여 tf-idf를 계산\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer,\n",
    "                                   tokenizer_porter],\n",
    "               'clf__penalty': ['l1', 'l2'],\n",
    "               'clf__C': [1.0, 10.0, 100.0]}, \n",
    "              {'vect__ngram_range': [(1,1)], \n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer,\n",
    "                                   tokenizer_porter],\n",
    "               'vect__use_idf': [False],              ##단어 빈도를 사용해 모델을 훈련 시키기 위해 좌와 같이 지정함.\n",
    "               'vect__norm': [None],\n",
    "               'clf__penalty': ['l1', 'l2'],\n",
    "               'clf__C': [1.0, 10.0, 100.0]}\n",
    "              ]\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                    ('clf', LogisticRegression(solver='liblinear', random_state=0))])\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=5, verbose=1,\n",
    "                           n_jobs=-1)\n",
    "gs_lr_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "최적의 매개변수 조합: {'clf__C': 1.0, 'clf__penalty': 'l1', 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer_porter at 0x000002A1C2130798>} \n"
    }
   ],
   "source": [
    "print('최적의 매개변수 조합: %s ' % gs_lr_tfidf.best_params_) ##책의 최적의 조합은 C =10.0, L2규제 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "CV 정확도: 0.876\n"
    }
   ],
   "source": [
    "print('CV 정확도: %.3f' %gs_lr_tfidf.best_score_)  ##책은 0.897"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "테스트 정확도: 0.877\n"
    }
   ],
   "source": [
    "clf = gs_lr_tfidf.best_estimator_\n",
    "print('테스트 정확도: %.3f' % clf.score(X_test, y_test))  ##cordms 0.899"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "##외부 메모리 학습 기법##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer 함수를 만들어 movie_data.csv 파일로 부터 읽은 텍스트 데이터를 정제하고 불용어를 제외한 다음에 단어 토큰으로 분리\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "def tokenizer(text):\n",
    "    text = re.sub('[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) \\\n",
    "           + ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized\n",
    "\n",
    "#한 번에 문서 하나씩 읽어서 반환하는 함수\n",
    "def stream_docs(path):\n",
    "    with open(path, 'r', encoding='utf-8') as csv:\n",
    "        next(csv) #헤더 넘기기\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "('\"Election is a Chinese mob movie, or triads in this case. Every two years an election is held to decide on a new leader, and at first it seems a toss up between Big D (Tony Leung Ka Fai, or as I know him, \"\"The Other Tony Leung\"\") and Lok (Simon Yam, who was Judge in Full Contact!). Though once Lok wins, Big D refuses to accept the choice and goes to whatever lengths he can to secure recognition as the new leader. Unlike any other Asian film I watch featuring gangsters, this one is not an action movie. It has its bloody moments, when necessary, as in Goodfellas, but it\\'s basically just a really effective drama. There are a lot of characters, which is really hard to keep track of, but I think that plays into the craziness of it all a bit. A 100-year-old baton, which is the symbol of power I mentioned before, changes hands several times before things settle down. And though it may appear that the film ends at the 65 or 70-minute mark, there are still a couple big surprises waiting. Simon Yam was my favorite character here and sort of anchors the picture.<br /><br />Election was quite the award winner at last year\\'s Hong Kong Film Awards, winning for best actor (Tony Leung), best picture, best director (Johnny To, who did Heroic Trio!!), and best screenplay. It also had nominations for cinematography, editing, film score (which I loved), and three more acting performances (including Yam).\"',\n 1)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(stream_docs(path=r'E:\\Programming\\python\\Machine-Learning\\data\\movie_data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#size 매개변수에서 지정한만큼 문서를 반환하는 함수\n",
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        pass\n",
    "    return docs, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##CountVectorizer나 TfidfVectorizer는 전체 어휘사전을 메모리에 가지고있어야 하기에 사용이 불가.\n",
    "##대신 HashingVectorizer를 사용\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "vect = HashingVectorizer(decode_error='ignore',    ##HashingVectorizer 초기화하기\n",
    "                         n_features=2**21,\n",
    "                         preprocessor=None,\n",
    "                         tokenizer=tokenizer)\n",
    "clf = SGDClassifier(loss='log', random_state=1, max_iter=1)  ##로지스틱 모델 초기화하기(log를 사용해서))\n",
    "doc_stream = stream_docs(path=r'E:\\Programming\\python\\Machine-Learning\\data\\movie_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "0% [##############################] 100% | ETA: 00:00:00\nTotal time elapsed: 00:00:23\n"
    }
   ],
   "source": [
    "import pyprind\n",
    "pbar = pyprind.ProgBar(45)\n",
    "classes = np.array([0, 1])\n",
    "for _ in range(45):\n",
    "    X_train, y_train = get_minibatch(doc_stream, size=1000)\n",
    "    if not X_train:\n",
    "        break\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes=classes)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "정확도: 0.802\n"
    }
   ],
   "source": [
    "X_test, y_test = get_minibatch(doc_stream, size=5000)\n",
    "X_test = vect.transform(X_test)\n",
    "print('정확도: %.3f' % clf.score(X_test, y_test))  ##책은 0.867"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = clf.partial_fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "##토픽 모델링##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "####잠재 디클레이 할당####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'E:\\Programming\\python\\Machine-Learning\\data\\movie_data.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BoW만들기\n",
    "count = CountVectorizer(stop_words='english',\n",
    "                        max_df=.1,  #단어의 최대 문서 빈도를 10%로 지정 => 여러 문서에 너무 자주 등장하는 단어 제외\n",
    "                        max_features=5000) #가장 자주 등장하는 단어 수 제한 => LDA 추론 성능 향상\n",
    "X = count.fit_transform(df['review'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components=10, #추정할 토픽의 개수\n",
    "                                random_state=123,\n",
    "                                learning_method='batch')\n",
    "X_topics = lda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(10, 5000)"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "토픽 1:\nworst minutes awful script stupid\n토픽 2:\nfamily mother father children girl\n토픽 3:\namerican war dvd music tv\n토픽 4:\nhuman audience cinema art sense\n토픽 5:\npolice guy car dead murder\n토픽 6:\nhorror house sex girl woman\n토픽 7:\nrole performance comedy actor performances\n토픽 8:\nseries episode war episodes tv\n토픽 9:\nbook version original read novel\n토픽 10:\naction fight guy guys cool\n"
    }
   ],
   "source": [
    "n_top_words = 5\n",
    "feature_names = count.get_feature_names()\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print('토픽 %d:' % (topic_idx + 1))\n",
    "    print(' '.join([feature_names[i]\n",
    "                    for i in topic.argsort()\\\n",
    "                        [:-n_top_words -1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n공포 영화 #1\nHouse of Dracula works from the same basic premise as House of Frankenstein from the year before; namely that Universal's three most famous monsters; Dracula, Frankenstein's Monster and The Wolf Man are appearing in the movie together. Naturally, the film is rather messy therefore, but the fact that ...\n\n공포 영화 #2\nOkay, what the hell kind of TRASH have I been watching now? \"The Witches' Mountain\" has got to be one of the most incoherent and insane Spanish exploitation flicks ever and yet, at the same time, it's also strangely compelling. There's absolutely nothing that makes sense here and I even doubt there  ...\n\n공포 영화 #3\n<br /><br />Horror movie time, Japanese style. Uzumaki/Spiral was a total freakfest from start to finish. A fun freakfest at that, but at times it was a tad too reliant on kitsch rather than the horror. The story is difficult to summarize succinctly: a carefree, normal teenage girl starts coming fac ...\n"
    }
   ],
   "source": [
    "#공포 토픽(토픽 6)확인하기\n",
    "horror = X_topics[:, 5].argsort()[::-1]  ##토픽6이므로 인덱스는 5이다\n",
    "for iter_idx, movie_idx in enumerate(horror[:3]):\n",
    "    print('\\n공포 영화 #%d' % (iter_idx + 1))\n",
    "    print(df['review'][movie_idx][:300], '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "###############################\n",
    "########9장 모델 저장하기#######\n",
    "###############################\n",
    "###############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####학습된 사이킷런 추정기 저장####\n",
    "\n",
    "##pickle 모듈 사용\n",
    "##-파이썬 객체의 구조를 압축된 바이트코드로 직렬화하고 복원할 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "##저장할 디렉토리 만들기##\n",
    "dest = os.path.join('movieclassifier', 'pkl_objects')\n",
    "if not os.path.exists(dest):\n",
    "    os.makedirs(dest)\n",
    "\n",
    "##불용어 저장하기\n",
    "pickle.dump(stop,  #저장할 객체 받기\n",
    "            open(os.path.join(dest, 'stopwords.pkl'), 'wb'),  #wb는 이진모드로 파일을 연다는 의미\n",
    "            protocol=4) #파이썬 3.4버전과 그 이상에서 사용할 수 있는 최신의 pickle 프로토콜 사용\n",
    "\n",
    "##훈련된 로지스틱 회귀 모델 저장하기\n",
    "pickle.dump(clf,\n",
    "            open(os.path.join(dest, 'classifier.pkl'), 'wb'),\n",
    "            protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##HashinVectorizer는 학습 과정이 없기에 pickle로 직렬화 할 필요없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}