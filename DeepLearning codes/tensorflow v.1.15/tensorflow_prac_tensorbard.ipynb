{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Board\n",
    "\n",
    "### 보기힘든 loss등의 결과 값들을 그래프를 통해 보는 것!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1단계\n",
    "----------------------\n",
    "### 어떤 함수를 log할지 정하기\n",
    "#### w2_hist = tf.summary.histogram('weight2', w2)\n",
    "#### cost_summ = tf.summary.scalar('cost', cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2단계\n",
    "----------------------\n",
    "### 하나하나 쓰지 않고 한 번에 쓰기 위해 merge하기\n",
    "#### summary = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3단계\n",
    "----------------------\n",
    "### 세션을 열고 summary를 어디위치에 기록할지 저장\n",
    "#### writer = tf.summary.FileWriter('./logs')   => 파일 위치\n",
    "#### writer.add_graph(sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4단계\n",
    "----------------------\n",
    "### summary 텐서를 실행시키기\n",
    "#### s, _ = sess.run([summary, optimizer], feed_dict=feed_dict)\n",
    "#### writer.add_summary(s, global_step=global_step)             => 실제 기록하는 부분"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5단계\n",
    "----------------------\n",
    "### 명령어로 실행하기\n",
    "#### !tensorboard --logdir=./logs\n",
    "\n",
    "------------------------\n",
    "## 원격으로 launch하기\n",
    "#### !ssh -L local_port:127.0.0.1:remote_port username@server.com\n",
    "#### 예시 => !ssh -L 7007:127.0.0.1:6006    godkim@server.com\n",
    "####         ! tensorboard --logdir=./logs/xor_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram\n",
    "--------------------\n",
    "### 하나의 값이 아닌 여러개의 값의 분포를 파악 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph\n",
    "---------------------\n",
    "### tf.name_scope를 통해 압축시켜서 그래프를 깔끔하게 볼 수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Runs\n",
    "---------------------\n",
    "### 하이퍼파라미터만 다르게 해서 실행시키는 걸 비교할때\n",
    "#### tensorboard --logdir=./logs/xor_logs\n",
    "#### tensorboard --logdir=./logs/xor_logs_r0_01         => learning_rate만 0.01로 바꾼 것\n",
    "#### tensorboard --logdir=./logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR tensorboard 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0 0.6999822\n100 0.63682127\n200 0.501098\n300 0.42449814\n400 0.39287806\n500 0.37748414\n600 0.36881733\n700 0.36342162\n800 0.35981405\n900 0.3572715\n1000 0.35540554\n1100 0.35399193\n1200 0.3528926\n1300 0.35201937\n1400 0.3513133\n1500 0.3507336\n1600 0.3502513\n1700 0.34984535\n1800 0.3495005\n1900 0.3492049\n2000 0.3489494\n2100 0.34872693\n2200 0.34853238\n2300 0.34836084\n2400 0.34820884\n2500 0.34807396\n2600 0.34795332\n2700 0.34784526\n2800 0.34774762\n2900 0.3476599\n3000 0.34758002\n3100 0.34750777\n3200 0.3474419\n3300 0.3473817\n3400 0.3473267\n3500 0.34727615\n3600 0.3472298\n3700 0.34718686\n3800 0.34714746\n3900 0.3471111\n4000 0.34707743\n4100 0.34704608\n4200 0.34701717\n4300 0.34699023\n4400 0.34696537\n4500 0.346942\n4600 0.34692016\n4700 0.34689993\n4800 0.34688082\n4900 0.3468633\n5000 0.3468467\n5100 0.34683114\n5200 0.34681642\n5300 0.3468029\n5400 0.3467899\n5500 0.34677804\n5600 0.3467667\n5700 0.34675628\n5800 0.34674624\n5900 0.346737\n6000 0.3467281\n6100 0.34671974\n6200 0.34671214\n6300 0.3467046\n6400 0.34669775\n6500 0.346691\n6600 0.34668475\n6700 0.3466789\n6800 0.34667355\n6900 0.34666818\n7000 0.34666327\n7100 0.34665862\n7200 0.34665436\n7300 0.34665024\n7400 0.346646\n7500 0.34664214\n7600 0.34663874\n7700 0.34663534\n7800 0.34663218\n7900 0.3466292\n8000 0.34662625\n8100 0.34662372\n8200 0.34662104\n8300 0.34661865\n8400 0.34661633\n8500 0.34661448\n8600 0.3466121\n8700 0.34661013\n8800 0.34660858\n8900 0.34660643\n9000 0.34660488\n9100 0.34660333\n9200 0.34660175\n9300 0.34660023\n9400 0.3465991\n9500 0.34659785\n9600 0.34659663\n9700 0.34659553\n9800 0.34659445\n9900 0.3465933\n10000 0.34659246\n\nHypothesis:\n[[1.8050598e-05]\n [9.9997401e-01]\n [5.0011826e-01]\n [5.0013375e-01]] \nPredicted:\n[[0.]\n [1.]\n [1.]\n [1.]] \nAccuracy:\n0.75\n"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 2], name=\"x\")\n",
    "Y = tf.placeholder(tf.float32, [None, 1], name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"Layer1\"):\n",
    "    W1 = tf.Variable(tf.random_normal([2, 2]), name=\"weight_1\")\n",
    "    b1 = tf.Variable(tf.random_normal([2]), name=\"bias_1\")\n",
    "    layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "    tf.summary.histogram(\"W1\", W1)\n",
    "    tf.summary.histogram(\"b1\", b1)\n",
    "    tf.summary.histogram(\"Layer1\", layer1)\n",
    "\n",
    "\n",
    "with tf.name_scope(\"Layer2\"):\n",
    "    W2 = tf.Variable(tf.random_normal([2, 1]), name=\"weight_2\")\n",
    "    b2 = tf.Variable(tf.random_normal([1]), name=\"bias_2\")\n",
    "    hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "    tf.summary.histogram(\"W2\", W2)\n",
    "    tf.summary.histogram(\"b2\", b2)\n",
    "    tf.summary.histogram(\"Hypothesis\", hypothesis)\n",
    "\n",
    "# cost/loss function\n",
    "with tf.name_scope(\"Cost\"):\n",
    "    cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "    tf.summary.scalar(\"Cost\", cost)\n",
    "\n",
    "with tf.name_scope(\"Train\"):\n",
    "    train = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # tensorboard --logdir=./logs/xor_logs\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(\"./logs/xor_logs_r0_01\")\n",
    "    writer.add_graph(sess.graph)  # Show the graph\n",
    "\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        _, summary, cost_val = sess.run(\n",
    "            [train, merged_summary, cost], feed_dict={X: x_data, Y: y_data}\n",
    "        )\n",
    "        writer.add_summary(summary, global_step=step)\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(step, cost_val)\n",
    "\n",
    "    # Accuracy report\n",
    "    h, p, a = sess.run(\n",
    "        [hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data}\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nHypothesis:\\n{h} \\nPredicted:\\n{p} \\nAccuracy:\\n{a}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37664bitbaseconda7ccc1197ea424c04aa8e970980241a68",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}