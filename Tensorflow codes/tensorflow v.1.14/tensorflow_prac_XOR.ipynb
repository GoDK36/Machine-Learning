{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR by NN(Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR Logistic Regression으로 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6978928 [[-0.34640825]\n",
      " [-0.17744146]]\n",
      "100 0.69503284 [[-0.20780644]\n",
      " [-0.11751376]]\n",
      "200 0.69398606 [[-0.13374066]\n",
      " [-0.08549749]]\n",
      "300 0.6935235 [[-0.08689647]\n",
      " [-0.06112275]]\n",
      "400 0.69331694 [[-0.05684593]\n",
      " [-0.0430769 ]]\n",
      "500 0.693224 [[-0.03740232]\n",
      " [-0.03004667]]\n",
      "600 0.6931821 [[-0.02472828]\n",
      " [-0.0207988 ]]\n",
      "700 0.69316304 [[-0.01641439]\n",
      " [-0.01431521]]\n",
      "800 0.6931544 [[-0.01093145]\n",
      " [-0.00981004]]\n",
      "900 0.6931505 [[-0.00729944]\n",
      " [-0.00670037]]\n",
      "1000 0.6931487 [[-0.00488471]\n",
      " [-0.00456468]]\n",
      "1100 0.6931479 [[-0.00327449]\n",
      " [-0.00310351]]\n",
      "1200 0.69314754 [[-0.00219811]\n",
      " [-0.00210679]]\n",
      "1300 0.6931473 [[-0.00147723]\n",
      " [-0.00142843]]\n",
      "1400 0.6931472 [[-0.00099365]\n",
      " [-0.00096758]]\n",
      "1500 0.6931472 [[-0.00066883]\n",
      " [-0.00065491]]\n",
      "1600 0.69314724 [[-0.00045048]\n",
      " [-0.00044305]]\n",
      "1700 0.6931472 [[-0.0003035 ]\n",
      " [-0.00029953]]\n",
      "1800 0.6931472 [[-0.0002046 ]\n",
      " [-0.00020248]]\n",
      "1900 0.6931472 [[-0.00013793]\n",
      " [-0.00013681]]\n",
      "2000 0.6931472 [[-9.302251e-05]\n",
      " [-9.241859e-05]]\n",
      "2100 0.6931471 [[-6.274486e-05]\n",
      " [-6.241807e-05]]\n",
      "2200 0.6931472 [[-4.231836e-05]\n",
      " [-4.214504e-05]]\n",
      "2300 0.6931471 [[-2.8545219e-05]\n",
      " [-2.8453840e-05]]\n",
      "2400 0.6931472 [[-1.9254345e-05]\n",
      " [-1.9206183e-05]]\n",
      "2500 0.6931472 [[-1.29884065e-05]\n",
      " [-1.29625932e-05]]\n",
      "2600 0.6931472 [[-8.7624385e-06]\n",
      " [-8.7485450e-06]]\n",
      "2700 0.6931472 [[-5.9148274e-06]\n",
      " [-5.9039148e-06]]\n",
      "2800 0.6931472 [[-3.9776733e-06]\n",
      " [-3.9771903e-06]]\n",
      "2900 0.6931472 [[-2.673824e-06]\n",
      " [-2.673341e-06]]\n",
      "3000 0.69314724 [[-1.8125318e-06]\n",
      " [-1.8120488e-06]]\n",
      "3100 0.6931472 [[-1.2358543e-06]\n",
      " [-1.2353713e-06]]\n",
      "3200 0.6931472 [[-8.3500987e-07]\n",
      " [-8.3452693e-07]]\n",
      "3300 0.69314724 [[-5.5784807e-07]\n",
      " [-5.5736513e-07]]\n",
      "3400 0.6931472 [[-3.5370169e-07]\n",
      " [-3.5321875e-07]]\n",
      "3500 0.6931472 [[-2.3896231e-07]\n",
      " [-2.3847937e-07]]\n",
      "3600 0.6931472 [[-1.8978830e-07]\n",
      " [-1.8930535e-07]]\n",
      "3700 0.6931472 [[-1.3912415e-07]\n",
      " [-1.3864121e-07]]\n",
      "3800 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "3900 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "4000 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "4100 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "4200 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "4300 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "4400 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "4500 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "4600 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "4700 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "4800 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "4900 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "5000 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "5100 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "5200 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "5300 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "5400 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "5500 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "5600 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "5700 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "5800 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "5900 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "6000 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "6100 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "6200 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "6300 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "6400 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "6500 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "6600 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "6700 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "6800 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "6900 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "7000 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "7100 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "7200 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "7300 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "7400 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "7500 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "7600 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "7700 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "7800 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "7900 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "8000 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "8100 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "8200 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "8300 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "8400 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "8500 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "8600 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "8700 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "8800 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "8900 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "9000 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "9100 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "9200 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "9300 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "9400 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "9500 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "9600 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "9700 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "9800 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "9900 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "10000 0.6931472 [[-1.3316367e-07]\n",
      " [-1.3268073e-07]]\n",
      "\n",
      "Hypothesis:  [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]] \n",
      "Correct:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]] \n",
      "Accuracy:  0.5\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "W = tf.Variable(tf.random_normal([2, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)                             # 0.5보다 커서 True면 1이고 아니면 0\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))        # 같아서 True면 1 아니면 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 100 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(W))\n",
    "\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.83046 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "100 0.69793504 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "200 0.6928933 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "300 0.68902355 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "400 0.6855906 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "500 0.6821211 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "600 0.67826796 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "700 0.67375535 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "800 0.66835487 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "900 0.66189057 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "1000 0.6542642 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "1100 0.6454868 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "1200 0.63569796 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "1300 0.62515616 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "1400 0.6141949 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "1500 0.6031568 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "1600 0.59232837 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "1700 0.58189523 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "1800 0.5719203 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "1900 0.5623418 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "2000 0.5529766 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "2100 0.543519 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "2200 0.5335148 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "2300 0.52229005 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "2400 0.5088055 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "2500 0.4914801 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "2600 0.46831667 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "2700 0.43785763 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "2800 0.40045738 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "2900 0.35851586 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "3000 0.3156191 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "3100 0.2751521 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "3200 0.23923954 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "3300 0.20860249 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "3400 0.18301475 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "3500 0.16181745 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "3600 0.14425436 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "3700 0.1296303 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "3800 0.1173628 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "3900 0.10698472 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "4000 0.09812853 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "4100 0.09050673 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "4200 0.083894424 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "4300 0.07811468 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "4400 0.07302733 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "4500 0.0685207 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "4600 0.06450485 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "4700 0.060906906 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "4800 0.05766723 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "4900 0.05473666 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "5000 0.052074485 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "5100 0.04964649 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "5200 0.04742411 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "5300 0.045382954 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "5400 0.043502353 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "5500 0.04176466 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "5600 0.04015452 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "5700 0.038658746 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "5800 0.037265934 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "5900 0.03596599 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "6000 0.034750167 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "6100 0.033610746 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "6200 0.032540917 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "6300 0.031534597 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "6400 0.030586466 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "6500 0.029691618 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "6600 0.028845882 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "6700 0.02804534 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "6800 0.027286604 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "6900 0.026566483 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "7000 0.025882185 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "7100 0.02523113 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "7200 0.024610989 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "7300 0.02401967 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "7400 0.023455273 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "7500 0.02291602 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "7600 0.022400245 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "7700 0.021906532 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "7800 0.021433473 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "7900 0.020979866 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "8000 0.0205445 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "8100 0.02012639 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "8200 0.01972448 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "8300 0.019337887 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "8400 0.01896574 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "8500 0.018607339 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "8600 0.018261835 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "8700 0.017928606 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "8800 0.017607003 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "8900 0.017296456 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "9000 0.01699642 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "9100 0.01670636 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "9200 0.016425824 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "9300 0.016154334 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "9400 0.015891459 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "9500 0.015636755 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "9600 0.015389925 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "9700 0.015150618 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "9800 0.014918434 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "9900 0.014693141 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "10000 0.014474387 [[-1.068441 ]\n",
      " [-0.9125451]]\n",
      "\n",
      "Hypothesis:  [[0.01688832]\n",
      " [0.98744106]\n",
      " [0.9866006 ]\n",
      " [0.01462846]] \n",
      "Correct:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# layer 1 구현\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([2, 2]), name='weight1')              # 크기 설정이 중요하다!! 여기선 입력층이 2 은닉층이 2\n",
    "b1 = tf.Variable(tf.random_normal([2]), name='bias1')\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "# layer 2 구현\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([2, 1]), name='weight2')              # 입력되는 은닉층이 2, 출력층이 1\n",
    "b2 = tf.Variable(tf.random_normal([1]), name='bias2')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)                             # 0.5보다 커서 True면 1이고 아니면 0\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))        # 같아서 True면 1 아니면 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 100 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(W))\n",
    "\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wide NN for XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7133176 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "100 0.6822116 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "200 0.67535603 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "300 0.6674463 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "400 0.6577157 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "500 0.64533705 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "600 0.6294004 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "700 0.6089405 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "800 0.58301437 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "900 0.55090714 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "1000 0.5124982 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "1100 0.4686069 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "1200 0.42105323 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "1300 0.37234104 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "1400 0.32508558 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "1500 0.28143013 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "1600 0.24268956 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "1700 0.2093242 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "1800 0.18115151 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "1900 0.15762635 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "2000 0.13806921 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "2100 0.121806055 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "2200 0.10823576 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "2300 0.09685214 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "2400 0.0872416 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "2500 0.0790717 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "2600 0.07207775 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "2700 0.06604869 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "2800 0.060816634 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "2900 0.056247097 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "3000 0.052232057 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "3100 0.048684046 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "3200 0.04553206 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "3300 0.04271795 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "3400 0.04019368 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "3500 0.037919573 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "3600 0.035862446 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "3700 0.033994503 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "3800 0.03229228 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "3900 0.03073589 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "4000 0.02930839 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "4100 0.02799522 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "4200 0.026783843 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "4300 0.025663557 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "4400 0.024624866 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "4500 0.023659669 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "4600 0.02276071 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "4700 0.021921748 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "4800 0.021137293 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "4900 0.020402344 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "5000 0.019712595 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "5100 0.0190642 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "5200 0.018453715 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "5300 0.01787797 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "5400 0.017334305 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "5500 0.016820164 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "5600 0.016333258 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "5700 0.015871618 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "5800 0.015433421 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "5900 0.015016984 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "6000 0.014620751 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "6100 0.014243366 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "6200 0.01388358 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "6300 0.013540166 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "6400 0.013212154 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "6500 0.012898546 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "6600 0.012598405 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "6700 0.012310963 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "6800 0.012035412 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "6900 0.011771107 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "7000 0.011517357 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "7100 0.011273628 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "7200 0.0110392645 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "7300 0.010813806 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "7400 0.010596751 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "7500 0.01038767 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "7600 0.010186136 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "7700 0.009991769 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "7800 0.009804231 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "7900 0.00962313 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "8000 0.0094481725 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "8100 0.009279072 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "8200 0.009115508 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "8300 0.008957239 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "8400 0.008804109 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "8500 0.008655726 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "8600 0.008511997 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "8700 0.008372651 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "8800 0.008237549 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "8900 0.008106448 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "9000 0.007979184 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "9100 0.007855633 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "9200 0.0077356435 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "9300 0.0076190345 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "9400 0.007505684 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "9500 0.007395411 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "9600 0.007288214 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "9700 0.0071838526 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "9800 0.00708228 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "9900 0.0069833603 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "10000 0.006887063 [[-0.5176797]\n",
      " [-0.9735144]]\n",
      "\n",
      "Hypothesis:  [[0.00437601]\n",
      " [0.992964  ]\n",
      " [0.99317557]\n",
      " [0.00921132]] \n",
      "Correct:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# layer 1 구현\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([2, 10]), name='weight1')              # 은닉층이 10으로 커짐\n",
    "b1 = tf.Variable(tf.random_normal([10]), name='bias1')\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "# layer 2 구현\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([10, 1]), name='weight2')              \n",
    "b2 = tf.Variable(tf.random_normal([1]), name='bias2')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)                             # 0.5보다 커서 True면 1이고 아니면 0\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))        # 같아서 True면 1 아니면 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 100 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(W))\n",
    "\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep NN for XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.1279043 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "100 0.69417346 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "200 0.6910819 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "300 0.6880106 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "400 0.6847177 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "500 0.68091655 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "600 0.67621183 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "700 0.6699969 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "800 0.66128176 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "900 0.6484108 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "1000 0.6285579 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "1100 0.59678125 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "1200 0.54518795 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "1300 0.46570188 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "1400 0.3621237 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "1500 0.2592859 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "1600 0.18065003 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "1700 0.12861305 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "1800 0.095340945 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "1900 0.07361489 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "2000 0.05886843 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "2100 0.04844877 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "2200 0.04081364 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "2300 0.03504034 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "2400 0.030556403 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "2500 0.02699373 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "2600 0.024107538 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "2700 0.021729989 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "2800 0.019743012 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "2900 0.018061422 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "3000 0.01662249 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "3100 0.015379103 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "3200 0.01429547 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "3300 0.0133436285 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "3400 0.012501839 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "3500 0.011752578 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "3600 0.011081955 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "3700 0.01047857 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "3800 0.009933131 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "3900 0.009437929 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "4000 0.008986498 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "4100 0.008573469 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "4200 0.008194363 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "4300 0.0078451745 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "4400 0.0075226873 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "4500 0.007224024 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "4600 0.006946656 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "4700 0.0066884803 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "4800 0.006447637 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "4900 0.0062224925 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "5000 0.006011613 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "5100 0.005813625 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "5200 0.0056275018 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "5300 0.0054522622 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "5400 0.0052869413 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "5500 0.005130741 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "5600 0.0049829823 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "5700 0.0048429877 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "5800 0.004710215 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "5900 0.0045840777 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "6000 0.0044641383 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "6100 0.0043499465 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "6200 0.0042411108 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "6300 0.0041372995 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "6400 0.004038168 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "6500 0.0039433846 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "6600 0.0038527101 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "6700 0.0037659027 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "6800 0.0036826478 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "6900 0.00360281 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "7000 0.003526209 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "7100 0.0034525895 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "7200 0.0033817869 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "7300 0.0033136806 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "7400 0.0032481204 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "7500 0.0031849567 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "7600 0.003124099 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "7700 0.0030653826 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "7800 0.0030087174 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "7900 0.002953969 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "8000 0.002901076 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "8100 0.002849935 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "8200 0.0028005152 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "8300 0.0027526517 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "8400 0.0027063447 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "8500 0.0026614293 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "8600 0.0026179354 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "8700 0.002575788 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "8800 0.0025348824 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "8900 0.0024951587 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "9000 0.0024566017 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "9100 0.0024191968 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "9200 0.0023828237 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "9300 0.0023474975 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "9400 0.0023131287 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "9500 0.002279687 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "9600 0.0022471724 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "9700 0.002215495 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "9800 0.0021846702 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "9900 0.0021546527 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "10000 0.0021254127 [[-0.9209695]\n",
      " [-1.5413922]]\n",
      "\n",
      "Hypothesis:  [[0.00174833]\n",
      " [0.99801815]\n",
      " [0.9976931 ]\n",
      " [0.00245539]] \n",
      "Correct:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# layer 1 구현\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([2, 10]), name='weight1')              # 은닉층이 10으로 커짐\n",
    "b1 = tf.Variable(tf.random_normal([10]), name='bias1')\n",
    "layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "\n",
    "# layer 2 구현\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([10, 10]), name='weight2')              \n",
    "b2 = tf.Variable(tf.random_normal([10]), name='bias2')\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "# layer 3 구현\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([10, 10]), name='weight3')              # 은닉층이 10으로 커짐\n",
    "b3 = tf.Variable(tf.random_normal([10]), name='bias3')\n",
    "layer3 = tf.sigmoid(tf.matmul(layer2, W3) + b3)\n",
    "\n",
    "# layer 4 구현\n",
    "\n",
    "W4 = tf.Variable(tf.random_normal([10, 1]), name='weight4')              \n",
    "b4 = tf.Variable(tf.random_normal([1]), name='bias4')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer3, W4) + b4)\n",
    "\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)                             # 0.5보다 커서 True면 1이고 아니면 0\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))        # 같아서 True면 1 아니면 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 100 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(W))\n",
    "\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbaseconda7ccc1197ea424c04aa8e970980241a68"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
