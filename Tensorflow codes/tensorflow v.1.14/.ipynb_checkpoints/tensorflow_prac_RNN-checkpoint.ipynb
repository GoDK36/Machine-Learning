{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[[0.00357953, 0.07444545]]], dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\client\\session.py:1735: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "hidden_size = 2                                                 # 출력값의 차원 크기가 2\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_size)\n",
    "\n",
    "x_data = np.array([[[1, 0, 0, 0]]], dtype=np.float32)\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, x_data, dtype=tf.float32)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "pp.pprint(outputs.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5, 4)\n",
      "array([[[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]]], dtype=float32)\n",
      "array([[[ 0.10761482,  0.06132578],\n",
      "        [-0.01994868,  0.09601615],\n",
      "        [-0.19202279,  0.13032795],\n",
      "        [-0.3013741 ,  0.14101297],\n",
      "        [-0.14858474,  0.08591167]]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "## sequence를 5로 설정\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "hidden_size = 2                                                 # 출력값의 차원 크기가 2\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_size)\n",
    "\n",
    "# one hot encoding\n",
    "h = [1, 0, 0, 0]\n",
    "e = [0, 1, 0, 0]\n",
    "l = [0, 0, 1, 0]\n",
    "o = [0, 0, 0, 1]\n",
    "\n",
    "x_data = np.array([[h, e, l, l, o]], dtype=np.float32)           # 시퀀스가 5(h,e,l,l,o))\n",
    "print(x_data.shape)\n",
    "pp.pprint(x_data)\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, x_data, dtype=tf.float32)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "pp.pprint(outputs.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]],\n",
      "\n",
      "       [[0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.]],\n",
      "\n",
      "       [[0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.]]], dtype=float32)\n",
      "array([[[ 0.12688796,  0.08382372],\n",
      "        [ 0.05363106,  0.00423421],\n",
      "        [ 0.20319764, -0.11588437],\n",
      "        [ 0.27452397, -0.15988448],\n",
      "        [ 0.30266342, -0.07262529]],\n",
      "\n",
      "       [[-0.02958702, -0.05652118],\n",
      "        [ 0.11445915, -0.0403403 ],\n",
      "        [ 0.2309059 , -0.12669346],\n",
      "        [ 0.2936819 , -0.16212037],\n",
      "        [ 0.3359268 , -0.17272379]],\n",
      "\n",
      "       [[ 0.14190492, -0.12362701],\n",
      "        [ 0.2325951 , -0.16936038],\n",
      "        [ 0.09701882, -0.11774141],\n",
      "        [ 0.05325726, -0.11268082],\n",
      "        [ 0.19307072, -0.17873302]]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "## batch size 설정\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "hidden_size = 2   \n",
    "\n",
    "x_data = np.array([[h, e, l, l, o],\n",
    "                    [e, o, l, l, l],\n",
    "                    [l, l, e, e, l]], dtype=np.float32)                         # batch_size가 3이다\n",
    "\n",
    "pp.pprint(x_data)\n",
    "\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_size, state_is_tuple=True)\n",
    "\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, x_data, dtype=tf.float32)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "pp.pprint(outputs.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "h = [1, 0, 0, 0, 0]\n",
    "i = [0, 1, 0, 0, 0]\n",
    "e = [0, 0, 1, 0, 0]\n",
    "l = [0, 0, 0, 1, 0]\n",
    "o = [0, 0, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.5967595\n"
     ]
    }
   ],
   "source": [
    "# seq_loss\n",
    "\n",
    "y_data = tf.constant([[1, 1, 1]])\n",
    "\n",
    "prediction = tf.constant([[[0.2, 0.7], [0.6, 0.2], [0.2, 0.9]]], dtype=tf.float32)\n",
    "\n",
    "weights = tf.constant([[1, 1, 1]], dtype=tf.float32)\n",
    "\n",
    "sequence_loss = tf.contrib.seq2seq.sequence_loss(logits=prediction, targets=y_data, weights=weights)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print(\"Loss: \", sequence_loss.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss1: 0.5130153 Loss2:  0.3711007\n"
     ]
    }
   ],
   "source": [
    "# seq_loss\n",
    "\n",
    "y_data = tf.constant([[1, 1, 1]])\n",
    "\n",
    "prediction1 = tf.constant([[[0.3, 0.7], [0.3, 0.7], [0.3, 0.7]]], dtype=tf.float32)\n",
    "prediction2 = tf.constant([[[0.1, 0.9], [0.1, 0.9], [0.1, 0.9]]], dtype=tf.float32)\n",
    "\n",
    "weights = tf.constant([[1, 1, 1]], dtype=tf.float32)\n",
    "\n",
    "sequence_loss1 = tf.contrib.seq2seq.sequence_loss(logits=prediction1, targets=y_data, weights=weights)\n",
    "sequence_loss2 = tf.contrib.seq2seq.sequence_loss(logits=prediction2, targets=y_data, weights=weights)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print(\"Loss1: \", sequence_loss1.eval(),\n",
    "        \"Loss2: \", sequence_loss2.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RNN model\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "hidden_size = 5\n",
    "input_dim = 5\n",
    "batch_size = 1\n",
    "sequence_length = 6\n",
    "\n",
    "idx2char = ['h', 'i', 'e', 'l', 'o']\n",
    "x_data = [[0, 1, 0, 2, 3, 3]]\n",
    "x_one_hot = [[[1, 0, 0, 0, 0],\n",
    "            [0, 1, 0, 0, 0],\n",
    "            [1, 0, 0, 0, 0],\n",
    "            [0, 0, 1, 0, 0],\n",
    "            [0, 0, 0, 1, 0],\n",
    "            [0, 0, 0, 1, 0]]]\n",
    "y_data = [[1, 0, 2, 3, 3, 4]]\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, sequence_length, input_dim])\n",
    "Y = tf.placeholder(tf.int32, [None, sequence_length])\n",
    "\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_size, state_is_tuple=True)\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X, initial_state=initial_state, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, _states = tf.nn.dynamic_rnn(cell, X, initial_state=initial_state, dtype=tf.float32)\n",
    "weights = tf.ones([batch_size, sequence_length])\n",
    "\n",
    "sequence_loss = tf.contrib.seq2seq.sequence_loss(logits=outputs, targets=Y, weights=weights)      # rnn에 나온 output을 바로 넣으면 안됨!\n",
    "loss = tf.reduce_mean(sequence_loss)\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.1).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ihello\n",
      "1810 loss: 0.6290708 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1811 loss: 0.6290689 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1812 loss: 0.62906677 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1813 loss: 0.6290648 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1814 loss: 0.6290627 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1815 loss: 0.6290607 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1816 loss: 0.62905866 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1817 loss: 0.62905663 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1818 loss: 0.6290546 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1819 loss: 0.6290526 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1820 loss: 0.62905055 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1821 loss: 0.6290485 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1822 loss: 0.6290465 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1823 loss: 0.6290445 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1824 loss: 0.62904245 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1825 loss: 0.6290405 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1826 loss: 0.6290384 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1827 loss: 0.6290365 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1828 loss: 0.6290344 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1829 loss: 0.62903243 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1830 loss: 0.6290304 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1831 loss: 0.6290284 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1832 loss: 0.62902635 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1833 loss: 0.6290243 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1834 loss: 0.6290223 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1835 loss: 0.62902033 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1836 loss: 0.62901837 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1837 loss: 0.62901634 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1838 loss: 0.6290143 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1839 loss: 0.6290123 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1840 loss: 0.62901026 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1841 loss: 0.6290083 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1842 loss: 0.6290063 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1843loss: 0.6290043 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1844 loss: 0.6290023 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1845 loss: 0.62900037 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1846 loss: 0.62899834 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1847 loss: 0.6289964 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1848 loss: 0.6289943 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1849 loss: 0.6289924 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1850 loss: 0.62899035 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1851 loss: 0.6289884 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1852 loss: 0.6289864 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1853 loss: 0.6289844 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1854 loss: 0.6289825 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1855 loss: 0.6289804 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1856 loss: 0.62897843 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1857 loss: 0.6289765 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1858 loss: 0.6289745 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1859 loss: 0.6289726 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1860 loss: 0.62897056 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1861 loss: 0.6289686 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1862 loss: 0.6289666 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1863 loss: 0.6289646 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1864 loss: 0.6289627 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1865 loss: 0.62896067 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1866 loss: 0.6289587 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1867 loss: 0.62895674 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1868 loss: 0.6289548 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1869 loss: 0.6289528 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1870 loss: 0.6289509 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1871 loss: 0.6289489 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1872 loss: 0.62894696 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1873 loss: 0.628945 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1874 loss: 0.628943 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1875 loss: 0.6289411 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1876 loss: 0.6289391 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1877 loss: 0.6289372 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1878 loss: 0.62893516 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1879 loss: 0.62893325 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1880 loss: 0.6289313 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1881 loss: 0.6289293 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1882 loss: 0.6289274 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1883 loss: 0.6289255 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1884 loss: 0.6289235 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1885 loss: 0.62892157 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1886 loss: 0.6289196 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1887 loss: 0.62891763 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1888 loss: 0.6289157 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1889 loss: 0.6289138 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1890 loss: 0.6289118 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1891 loss: 0.6289099 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1892 loss: 0.6289079 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1893 loss: 0.628906 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1894 loss: 0.6289041 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1895 loss: 0.62890214 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1896 loss: 0.62890023 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1897 loss: 0.62889826 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1898 loss: 0.62889636 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1899 loss: 0.62889445 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1900 loss: 0.6288925 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1901 loss: 0.6288906 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1902 loss: 0.6288886 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1903 loss: 0.6288867 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1904 loss: 0.6288848 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1905 loss: 0.6288828 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1906 loss: 0.6288809 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1907 loss: 0.628879 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1908 loss: 0.6288771 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1909 loss: 0.6288752 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1910 loss: 0.6288732 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1911 loss: 0.6288714 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1912 loss: 0.6288694 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1913 loss: 0.62886757 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1914 loss: 0.6288656 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1915 loss: 0.62886375 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1916 loss: 0.6288617 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1917 loss: 0.6288598 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1918 loss: 0.628858 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1919 loss: 0.62885606 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1920 loss: 0.62885416 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1921 loss: 0.6288523 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1922 loss: 0.62885034 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1923 loss: 0.6288485 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1924 loss: 0.6288466 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1925 loss: 0.62884456 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1926 loss: 0.6288428 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1927 loss: 0.6288409 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1928 loss: 0.62883896 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1929 loss: 0.62883705 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1930 loss: 0.62883514 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1931 loss: 0.6288333 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1932 loss: 0.6288314 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1933 loss: 0.62882954 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1934 loss: 0.6288276 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1935 loss: 0.6288257 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1936 loss: 0.6288238 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1937 loss: 0.6288219 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1938 loss: 0.62882 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1939 loss: 0.6288182 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1940 loss: 0.6288163 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1941 loss: 0.6288144 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1942 loss: 0.6288125 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1943 loss: 0.62881064 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1944 loss: 0.6288088 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1945 loss: 0.6288069 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1946 loss: 0.628805 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1947 loss: 0.6288032 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1948 loss: 0.6288013 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1949 loss: 0.6287995 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1950 loss: 0.62879753 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1951 loss: 0.6287957 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1952 loss: 0.62879384 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1953 loss: 0.628792 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1954 loss: 0.62879 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1955 loss: 0.6287882 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1956 loss: 0.6287863 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1957 loss: 0.6287844 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1958 loss: 0.6287826 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1959 loss: 0.6287807 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1960 loss: 0.6287789 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1961 loss: 0.628777 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1962 loss: 0.6287752 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1963 loss: 0.6287734 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1964 loss: 0.62877136 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1965 loss: 0.6287696 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1966 loss: 0.62876767 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1967 loss: 0.6287659 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1968 loss: 0.62876403 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1969 loss: 0.6287622 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1970 loss: 0.62876034 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1971 loss: 0.6287585 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1972 loss: 0.6287566 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1973 loss: 0.6287548 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1974 loss: 0.62875295 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1975 loss: 0.6287511 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1976 loss: 0.6287492 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1977 loss: 0.6287474 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1978 loss: 0.62874556 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1979 loss: 0.62874365 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1980 loss: 0.62874186 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1981 loss: 0.62874 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1982 loss: 0.6287382 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1983 loss: 0.6287363 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1984 loss: 0.6287345 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1985 loss: 0.6287327 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1986 loss: 0.62873083 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1987 loss: 0.62872905 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1988 loss: 0.62872714 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1989 loss: 0.62872535 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1990 loss: 0.62872356 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1991 loss: 0.6287217 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1992 loss: 0.62871987 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1993 loss: 0.6287181 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1994 loss: 0.62871623 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1995 loss: 0.6287144 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1996 loss: 0.62871253 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1997 loss: 0.62871075 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1998 loss: 0.6287089 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n",
      "1999 loss: 0.6287071 prediction:  [[1 0 2 3 3 4]] true Y:  [[1, 0, 2, 3, 3, 4]]\n",
      "\\Presiction str:  ihello\n"
     ]
    }
   ],
   "source": [
    "prediction = tf.argmax(outputs, axis=2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(2000):\n",
    "        l, _ = sess.run([loss, train], feed_dict={X: x_one_hot, Y: y_data})\n",
    "        result = sess.run(prediction, feed_dict={X: x_one_hot})\n",
    "        print(i, \"loss:\", l, \"prediction: \", result, \"true Y: \", y_data)\n",
    "\n",
    "        result_str = [idx2char[c] for c in np.squeeze(result)]\n",
    "        print(\"\\Presiction str: \", ''.join(result_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2599 Loss: 1.002454 Prediction:  if you want you\n",
      "2600 Loss: 1.0024539 Prediction:  if you want you\n",
      "2601 Loss: 1.0024539 Prediction:  if you want you\n",
      "2602 Loss: 1.0024539 Prediction:  if you want you\n",
      "2603 Loss: 1.0024539 Prediction:  if you want you\n",
      "2604 Loss: 1.0024539 Prediction:  if you want you\n",
      "2605 Loss: 1.0024539 Prediction:  if you want you\n",
      "2606 Loss: 1.0024539 Prediction:  if you want you\n",
      "2607 Loss: 1.0024539 Prediction:  if you want you\n",
      "2608 Loss: 1.002454 Prediction:  if you want you\n",
      "2609 Loss: 1.0024542 Prediction:  if you want you\n",
      "2610 Loss: 1.0024543 Prediction:  if you want you\n",
      "2611 Loss: 1.0024545 Prediction:  if you want you\n",
      "2612 Loss: 1.0024551 Prediction:  if you want you\n",
      "2613 Loss: 1.0024558 Prediction:  if you want you\n",
      "2614 Loss: 1.0024569 Prediction:  if you want you\n",
      "2615 Loss: 1.0024585 Prediction:  if you want you\n",
      "2616 Loss: 1.0024616 Prediction:  if you want you\n",
      "2617 Loss: 1.0024647 Prediction:  if you want you\n",
      "2618 Loss: 1.0024718 Prediction:  if you want you\n",
      "2619 Loss: 1.002477 Prediction:  if you want you\n",
      "2620 Loss: 1.0024923 Prediction:  if you want you\n",
      "2621 Loss: 1.0024972 Prediction:  if you want you\n",
      "2622 Loss: 1.0025208 Prediction:  if you want you\n",
      "2623 Loss: 1.0025142 Prediction:  if you want you\n",
      "2624 Loss: 1.0025303 Prediction:  if you want you\n",
      "2625 Loss: 1.002506 Prediction:  if you want you\n",
      "2626 Loss: 1.0025009 Prediction:  if you want you\n",
      "2627 Loss: 1.002478 Prediction:  if you want you\n",
      "2628 Loss: 1.0024667 Prediction:  if you want you\n",
      "2629 Loss: 1.0024569 Prediction:  if you want you\n",
      "2630 Loss: 1.0024531 Prediction:  if you want you\n",
      "2631 Loss: 1.0024536 Prediction:  if you want you\n",
      "2632 Loss: 1.002457 Prediction:  if you want you\n",
      "2633 Loss: 1.0024626 Prediction:  if you want you\n",
      "2634 Loss: 1.0024656 Prediction:  if you want you\n",
      "2635 Loss: 1.0024693 Prediction:  if you want you\n",
      "2636 Loss: 1.0024663 Prediction:  if you want you\n",
      "2637 Loss: 1.0024645 Prediction:  if you want you\n",
      "2638 Loss: 1.002459 Prediction:  if you want you\n",
      "2639 Loss: 1.0024555 Prediction:  if you want you\n",
      "2640 Loss: 1.002453 Prediction:  if you want you\n",
      "2641 Loss: 1.0024524 Prediction:  if you want you\n",
      "2642 Loss: 1.0024534 Prediction:  if you want you\n",
      "2643 Loss: 1.002455 Prediction:  if you want you\n",
      "2644 Loss: 1.0024568 Prediction:  if you want you\n",
      "2645 Loss: 1.0024571 Prediction:  if you want you\n",
      "2646 Loss: 1.002457 Prediction:  if you want you\n",
      "2647 Loss: 1.0024555 Prediction:  if you want you\n",
      "2648 Loss: 1.002454 Prediction:  if you want you\n",
      "2649 Loss: 1.0024527 Prediction:  if you want you\n",
      "2650 Loss: 1.0024521 Prediction:  if you want you\n",
      "2651 Loss: 1.0024523 Prediction:  if you want you\n",
      "2652 Loss: 1.0024526 Prediction:  if you want you\n",
      "2653 Loss: 1.0024533 Prediction:  if you want you\n",
      "2654 Loss: 1.0024537 Prediction:  if you want you\n",
      "2655 Loss: 1.0024539 Prediction:  if you want you\n",
      "2656 Loss: 1.0024534 Prediction:  if you want you\n",
      "2657 Loss: 1.002453 Prediction:  if you want you\n",
      "2658 Loss: 1.0024524 Prediction:  if you want you\n",
      "2659 Loss: 1.002452 Prediction:  if you want you\n",
      "2660 Loss: 1.0024517 Prediction:  if you want you\n",
      "2661 Loss: 1.0024517 Prediction:  if you want you\n",
      "2662 Loss: 1.0024518 Prediction:  if you want you\n",
      "2663 Loss: 1.002452 Prediction:  if you want you\n",
      "2664 Loss: 1.0024523 Prediction:  if you want you\n",
      "2665 Loss: 1.0024523 Prediction:  if you want you\n",
      "2666 Loss: 1.0024521 Prediction:  if you want you\n",
      "2667 Loss: 1.002452 Prediction:  if you want you\n",
      "2668 Loss: 1.0024517 Prediction:  if you want you\n",
      "2669 Loss: 1.0024515 Prediction:  if you want you\n",
      "2670 Loss: 1.0024514 Prediction:  if you want you\n",
      "2671 Loss: 1.0024513 Prediction:  if you want you\n",
      "2672 Loss: 1.0024513 Prediction:  if you want you\n",
      "2673 Loss: 1.0024513 Prediction:  if you want you\n",
      "2674 Loss: 1.0024513 Prediction:  if you want you\n",
      "2675 Loss: 1.0024513 Prediction:  if you want you\n",
      "2676 Loss: 1.0024513 Prediction:  if you want you\n",
      "2677 Loss: 1.0024513 Prediction:  if you want you\n",
      "2678 Loss: 1.0024513 Prediction:  if you want you\n",
      "2679 Loss: 1.0024512 Prediction:  if you want you\n",
      "2680 Loss: 1.0024512 Prediction:  if you want you\n",
      "2681 Loss: 1.0024511 Prediction:  if you want you\n",
      "2682 Loss: 1.0024508 Prediction:  if you want you\n",
      "2683 Loss: 1.0024508 Prediction:  if you want you\n",
      "2684 Loss: 1.0024508 Prediction:  if you want you\n",
      "2685 Loss: 1.0024507 Prediction:  if you want you\n",
      "2686 Loss: 1.0024506 Prediction:  if you want you\n",
      "2687 Loss: 1.0024507 Prediction:  if you want you\n",
      "2688 Loss: 1.0024507 Prediction:  if you want you\n",
      "2689 Loss: 1.0024507 Prediction:  if you want you\n",
      "2690 Loss: 1.0024506 Prediction:  if you want you\n",
      "2691 Loss: 1.0024506 Prediction:  if you want you\n",
      "2692 Loss: 1.0024506 Prediction:  if you want you\n",
      "2693 Loss: 1.0024506 Prediction:  if you want you\n",
      "2694 Loss: 1.0024505 Prediction:  if you want you\n",
      "2695 Loss: 1.0024505 Prediction:  if you want you\n",
      "2696 Loss: 1.0024505 Prediction:  if you want you\n",
      "2697 Loss: 1.0024505 Prediction:  if you want you\n",
      "2698 Loss: 1.0024503 Prediction:  if you want you\n",
      "2699 Loss: 1.0024503 Prediction:  if you want you\n",
      "2700 Loss: 1.0024503 Prediction:  if you want you\n",
      "2701 Loss: 1.0024502 Prediction:  if you want you\n",
      "2702 Loss: 1.0024502 Prediction:  if you want you\n",
      "2703 Loss: 1.0024502 Prediction:  if you want you\n",
      "2704 Loss: 1.0024501 Prediction:  if you want you\n",
      "2705 Loss: 1.0024501 Prediction:  if you want you\n",
      "2706 Loss: 1.0024501 Prediction:  if you want you\n",
      "2707 Loss: 1.00245 Prediction:  if you want you\n",
      "2708 Loss: 1.0024501 Prediction:  if you want you\n",
      "2709 Loss: 1.0024499 Prediction:  if you want you\n",
      "2710 Loss: 1.0024499 Prediction:  if you want you\n",
      "2711 Loss: 1.0024499 Prediction:  if you want you\n",
      "2712 Loss: 1.0024499 Prediction:  if you want you\n",
      "2713 Loss: 1.0024498 Prediction:  if you want you\n",
      "2714 Loss: 1.0024498 Prediction:  if you want you\n",
      "2715 Loss: 1.0024498 Prediction:  if you want you\n",
      "2716 Loss: 1.0024498 Prediction:  if you want you\n",
      "2717 Loss: 1.0024498 Prediction:  if you want you\n",
      "2718 Loss: 1.0024498 Prediction:  if you want you\n",
      "2719 Loss: 1.0024499 Prediction:  if you want you\n",
      "2720 Loss: 1.0024499 Prediction:  if you want you\n",
      "2721 Loss: 1.00245 Prediction:  if you want you\n",
      "2722 Loss: 1.0024502 Prediction:  if you want you\n",
      "2723 Loss: 1.0024506 Prediction:  if you want you\n",
      "2724 Loss: 1.0024511 Prediction:  if you want you\n",
      "2725 Loss: 1.0024515 Prediction:  if you want you\n",
      "2726 Loss: 1.0024527 Prediction:  if you want you\n",
      "2727 Loss: 1.002454 Prediction:  if you want you\n",
      "2728 Loss: 1.0024568 Prediction:  if you want you\n",
      "2729 Loss: 1.0024592 Prediction:  if you want you\n",
      "2730 Loss: 1.0024655 Prediction:  if you want you\n",
      "2731 Loss: 1.0024694 Prediction:  if you want you\n",
      "2732 Loss: 1.0024822 Prediction:  if you want you\n",
      "2733 Loss: 1.0024855 Prediction:  if you want you\n",
      "2734 Loss: 1.002506 Prediction:  if you want you\n",
      "2735 Loss: 1.0024998 Prediction:  if you want you\n",
      "2736 Loss: 1.002517 Prediction:  if you want you\n",
      "2737 Loss: 1.0024966 Prediction:  if you want you\n",
      "2738 Loss: 1.002497 Prediction:  if you want you\n",
      "2739 Loss: 1.0024757 Prediction:  if you want you\n",
      "2740 Loss: 1.0024673 Prediction:  if you want you\n",
      "2741 Loss: 1.0024561 Prediction:  if you want you\n",
      "2742 Loss: 1.0024508 Prediction:  if you want you\n",
      "2743 Loss: 1.0024488 Prediction:  if you want you\n",
      "2744 Loss: 1.0024496 Prediction:  if you want you\n",
      "2745 Loss: 1.0024526 Prediction:  if you want you\n",
      "2746 Loss: 1.0024558 Prediction:  if you want you\n",
      "2747 Loss: 1.0024601 Prediction:  if you want you\n",
      "2748 Loss: 1.0024606 Prediction:  if you want you\n",
      "2749 Loss: 1.0024626 Prediction:  if you want you\n",
      "2750 Loss: 1.0024593 Prediction:  if you want you\n",
      "2751 Loss: 1.0024575 Prediction:  if you want you\n",
      "2752 Loss: 1.0024536 Prediction:  if you want you\n",
      "2753 Loss: 1.0024511 Prediction:  if you want you\n",
      "2754 Loss: 1.0024492 Prediction:  if you want you\n",
      "2755 Loss: 1.0024484 Prediction:  if you want you\n",
      "2756 Loss: 1.0024487 Prediction:  if you want you\n",
      "2757 Loss: 1.0024495 Prediction:  if you want you\n",
      "2758 Loss: 1.0024508 Prediction:  if you want you\n",
      "2759 Loss: 1.0024514 Prediction:  if you want you\n",
      "2760 Loss: 1.0024523 Prediction:  if you want you\n",
      "2761 Loss: 1.0024518 Prediction:  if you want you\n",
      "2762 Loss: 1.0024515 Prediction:  if you want you\n",
      "2763 Loss: 1.0024506 Prediction:  if you want you\n",
      "2764 Loss: 1.0024496 Prediction:  if you want you\n",
      "2765 Loss: 1.0024488 Prediction:  if you want you\n",
      "2766 Loss: 1.0024483 Prediction:  if you want you\n",
      "2767 Loss: 1.002448 Prediction:  if you want you\n",
      "2768 Loss: 1.002448 Prediction:  if you want you\n",
      "2769 Loss: 1.0024482 Prediction:  if you want you\n",
      "2770 Loss: 1.0024484 Prediction:  if you want you\n",
      "2771 Loss: 1.0024487 Prediction:  if you want you\n",
      "2772 Loss: 1.0024489 Prediction:  if you want you\n",
      "2773 Loss: 1.002449 Prediction:  if you want you\n",
      "2774 Loss: 1.0024489 Prediction:  if you want you\n",
      "2775 Loss: 1.0024487 Prediction:  if you want you\n",
      "2776 Loss: 1.0024486 Prediction:  if you want you\n",
      "2777 Loss: 1.0024483 Prediction:  if you want you\n",
      "2778 Loss: 1.002448 Prediction:  if you want you\n",
      "2779 Loss: 1.0024477 Prediction:  if you want you\n",
      "2780 Loss: 1.0024476 Prediction:  if you want you\n",
      "2781 Loss: 1.0024475 Prediction:  if you want you\n",
      "2782 Loss: 1.0024475 Prediction:  if you want you\n",
      "2783 Loss: 1.0024475 Prediction:  if you want you\n",
      "2784 Loss: 1.0024475 Prediction:  if you want you\n",
      "2785 Loss: 1.0024475 Prediction:  if you want you\n",
      "2786 Loss: 1.0024475 Prediction:  if you want you\n",
      "2787 Loss: 1.0024475 Prediction:  if you want you\n",
      "2788 Loss: 1.0024475 Prediction:  if you want you\n",
      "2789 Loss: 1.0024475 Prediction:  if you want you\n",
      "2790 Loss: 1.0024476 Prediction:  if you want you\n",
      "2791 Loss: 1.0024475 Prediction:  if you want you\n",
      "2792 Loss: 1.0024475 Prediction:  if you want you\n",
      "2793 Loss: 1.0024475 Prediction:  if you want you\n",
      "2794 Loss: 1.0024475 Prediction:  if you want you\n",
      "2795 Loss: 1.0024474 Prediction:  if you want you\n",
      "2796 Loss: 1.0024474 Prediction:  if you want you\n",
      "2797 Loss: 1.0024474 Prediction:  if you want you\n",
      "2798 Loss: 1.0024474 Prediction:  if you want you\n",
      "2799 Loss: 1.0024472 Prediction:  if you want you\n",
      "2800 Loss: 1.0024472 Prediction:  if you want you\n",
      "2801 Loss: 1.0024474 Prediction:  if you want you\n",
      "2802 Loss: 1.0024472 Prediction:  if you want you\n",
      "2803 Loss: 1.0024472 Prediction:  if you want you\n",
      "2804 Loss: 1.0024472 Prediction:  if you want you\n",
      "2805 Loss: 1.0024472 Prediction:  if you want you\n",
      "2806 Loss: 1.0024474 Prediction:  if you want you\n",
      "2807 Loss: 1.0024475 Prediction:  if you want you\n",
      "2808 Loss: 1.0024476 Prediction:  if you want you\n",
      "2809 Loss: 1.0024477 Prediction:  if you want you\n",
      "2810 Loss: 1.0024482 Prediction:  if you want you\n",
      "2811 Loss: 1.0024484 Prediction:  if you want you\n",
      "2812 Loss: 1.002449 Prediction:  if you want you\n",
      "2813 Loss: 1.0024495 Prediction:  if you want you\n",
      "2814 Loss: 1.0024507 Prediction:  if you want you\n",
      "2815 Loss: 1.0024514 Prediction:  if you want you\n",
      "2816 Loss: 1.0024537 Prediction:  if you want you\n",
      "2817 Loss: 1.002455 Prediction:  if you want you\n",
      "2818 Loss: 1.0024593 Prediction:  if you want you\n",
      "2819 Loss: 1.0024607 Prediction:  if you want you\n",
      "2820 Loss: 1.002468 Prediction:  if you want you\n",
      "2821 Loss: 1.0024686 Prediction:  if you want you\n",
      "2822 Loss: 1.0024787 Prediction:  if you want you\n",
      "2823 Loss: 1.0024755 Prediction:  if you want you\n",
      "2824 Loss: 1.0024855 Prediction:  if you want you\n",
      "2825 Loss: 1.0024769 Prediction:  if you want you\n",
      "2826 Loss: 1.0024818 Prediction:  if you want you\n",
      "2827 Loss: 1.0024704 Prediction:  if you want you\n",
      "2828 Loss: 1.0024693 Prediction:  if you want you\n",
      "2829 Loss: 1.0024601 Prediction:  if you want you\n",
      "2830 Loss: 1.0024565 Prediction:  if you want you\n",
      "2831 Loss: 1.0024515 Prediction:  if you want you\n",
      "2832 Loss: 1.0024489 Prediction:  if you want you\n",
      "2833 Loss: 1.0024468 Prediction:  if you want you\n",
      "2834 Loss: 1.0024459 Prediction:  if you want you\n",
      "2835 Loss: 1.0024458 Prediction:  if you want you\n",
      "2836 Loss: 1.0024463 Prediction:  if you want you\n",
      "2837 Loss: 1.002447 Prediction:  if you want you\n",
      "2838 Loss: 1.0024478 Prediction:  if you want you\n",
      "2839 Loss: 1.0024493 Prediction:  if you want you\n",
      "2840 Loss: 1.0024499 Prediction:  if you want you\n",
      "2841 Loss: 1.0024514 Prediction:  if you want you\n",
      "2842 Loss: 1.0024514 Prediction:  if you want you\n",
      "2843 Loss: 1.0024526 Prediction:  if you want you\n",
      "2844 Loss: 1.002452 Prediction:  if you want you\n",
      "2845 Loss: 1.0024526 Prediction:  if you want you\n",
      "2846 Loss: 1.0024514 Prediction:  if you want you\n",
      "2847 Loss: 1.0024517 Prediction:  if you want you\n",
      "2848 Loss: 1.0024505 Prediction:  if you want you\n",
      "2849 Loss: 1.0024502 Prediction:  if you want you\n",
      "2850 Loss: 1.0024492 Prediction:  if you want you\n",
      "2851 Loss: 1.0024488 Prediction:  if you want you\n",
      "2852 Loss: 1.0024478 Prediction:  if you want you\n",
      "2853 Loss: 1.0024475 Prediction:  if you want you\n",
      "2854 Loss: 1.0024469 Prediction:  if you want you\n",
      "2855 Loss: 1.0024467 Prediction:  if you want you\n",
      "2856 Loss: 1.0024463 Prediction:  if you want you\n",
      "2857 Loss: 1.0024459 Prediction:  if you want you\n",
      "2858 Loss: 1.0024457 Prediction:  if you want you\n",
      "2859 Loss: 1.0024456 Prediction:  if you want you\n",
      "2860 Loss: 1.0024455 Prediction:  if you want you\n",
      "2861 Loss: 1.0024453 Prediction:  if you want you\n",
      "2862 Loss: 1.0024452 Prediction:  if you want you\n",
      "2863 Loss: 1.0024452 Prediction:  if you want you\n",
      "2864 Loss: 1.0024451 Prediction:  if you want you\n",
      "2865 Loss: 1.0024451 Prediction:  if you want you\n",
      "2866 Loss: 1.002445 Prediction:  if you want you\n",
      "2867 Loss: 1.002445 Prediction:  if you want you\n",
      "2868 Loss: 1.002445 Prediction:  if you want you\n",
      "2869 Loss: 1.0024449 Prediction:  if you want you\n",
      "2870 Loss: 1.002445 Prediction:  if you want you\n",
      "2871 Loss: 1.002445 Prediction:  if you want you\n",
      "2872 Loss: 1.0024449 Prediction:  if you want you\n",
      "2873 Loss: 1.002445 Prediction:  if you want you\n",
      "2874 Loss: 1.0024451 Prediction:  if you want you\n",
      "2875 Loss: 1.0024452 Prediction:  if you want you\n",
      "2876 Loss: 1.0024452 Prediction:  if you want you\n",
      "2877 Loss: 1.0024455 Prediction:  if you want you\n",
      "2878 Loss: 1.0024458 Prediction:  if you want you\n",
      "2879 Loss: 1.0024464 Prediction:  if you want you\n",
      "2880 Loss: 1.002447 Prediction:  if you want you\n",
      "2881 Loss: 1.0024482 Prediction:  if you want you\n",
      "2882 Loss: 1.0024493 Prediction:  if you want you\n",
      "2883 Loss: 1.0024521 Prediction:  if you want you\n",
      "2884 Loss: 1.0024539 Prediction:  if you want you\n",
      "2885 Loss: 1.0024596 Prediction:  if you want you\n",
      "2886 Loss: 1.0024619 Prediction:  if you want you\n",
      "2887 Loss: 1.0024726 Prediction:  if you want you\n",
      "2888 Loss: 1.0024729 Prediction:  if you want you\n",
      "2889 Loss: 1.0024887 Prediction:  if you want you\n",
      "2890 Loss: 1.0024816 Prediction:  if you want you\n",
      "2891 Loss: 1.002495 Prediction:  if you want you\n",
      "2892 Loss: 1.0024793 Prediction:  if you want you\n",
      "2893 Loss: 1.0024827 Prediction:  if you want you\n",
      "2894 Loss: 1.002467 Prediction:  if you want you\n",
      "2895 Loss: 1.0024635 Prediction:  if you want you\n",
      "2896 Loss: 1.0024543 Prediction:  if you want you\n",
      "2897 Loss: 1.0024503 Prediction:  if you want you\n",
      "2898 Loss: 1.0024467 Prediction:  if you want you\n",
      "2899 Loss: 1.0024447 Prediction:  if you want you\n",
      "2900 Loss: 1.0024439 Prediction:  if you want you\n",
      "2901 Loss: 1.0024439 Prediction:  if you want you\n",
      "2902 Loss: 1.0024446 Prediction:  if you want you\n",
      "2903 Loss: 1.0024457 Prediction:  if you want you\n",
      "2904 Loss: 1.0024472 Prediction:  if you want you\n",
      "2905 Loss: 1.0024484 Prediction:  if you want you\n",
      "2906 Loss: 1.0024507 Prediction:  if you want you\n",
      "2907 Loss: 1.0024514 Prediction:  if you want you\n",
      "2908 Loss: 1.002454 Prediction:  if you want you\n",
      "2909 Loss: 1.0024536 Prediction:  if you want you\n",
      "2910 Loss: 1.0024561 Prediction:  if you want you\n",
      "2911 Loss: 1.0024545 Prediction:  if you want you\n",
      "2912 Loss: 1.0024562 Prediction:  if you want you\n",
      "2913 Loss: 1.0024539 Prediction:  if you want you\n",
      "2914 Loss: 1.0024546 Prediction:  if you want you\n",
      "2915 Loss: 1.0024523 Prediction:  if you want you\n",
      "2916 Loss: 1.0024523 Prediction:  if you want you\n",
      "2917 Loss: 1.0024501 Prediction:  if you want you\n",
      "2918 Loss: 1.0024496 Prediction:  if you want you\n",
      "2919 Loss: 1.002448 Prediction:  if you want you\n",
      "2920 Loss: 1.0024475 Prediction:  if you want you\n",
      "2921 Loss: 1.0024464 Prediction:  if you want you\n",
      "2922 Loss: 1.0024459 Prediction:  if you want you\n",
      "2923 Loss: 1.0024453 Prediction:  if you want you\n",
      "2924 Loss: 1.002445 Prediction:  if you want you\n",
      "2925 Loss: 1.0024446 Prediction:  if you want you\n",
      "2926 Loss: 1.0024444 Prediction:  if you want you\n",
      "2927 Loss: 1.0024441 Prediction:  if you want you\n",
      "2928 Loss: 1.002444 Prediction:  if you want you\n",
      "2929 Loss: 1.0024439 Prediction:  if you want you\n",
      "2930 Loss: 1.0024438 Prediction:  if you want you\n",
      "2931 Loss: 1.0024438 Prediction:  if you want you\n",
      "2932 Loss: 1.0024439 Prediction:  if you want you\n",
      "2933 Loss: 1.0024438 Prediction:  if you want you\n",
      "2934 Loss: 1.0024439 Prediction:  if you want you\n",
      "2935 Loss: 1.0024439 Prediction:  if you want you\n",
      "2936 Loss: 1.0024441 Prediction:  if you want you\n",
      "2937 Loss: 1.0024444 Prediction:  if you want you\n",
      "2938 Loss: 1.0024449 Prediction:  if you want you\n",
      "2939 Loss: 1.0024452 Prediction:  if you want you\n",
      "2940 Loss: 1.0024462 Prediction:  if you want you\n",
      "2941 Loss: 1.0024468 Prediction:  if you want you\n",
      "2942 Loss: 1.0024484 Prediction:  if you want you\n",
      "2943 Loss: 1.0024493 Prediction:  if you want you\n",
      "2944 Loss: 1.0024524 Prediction:  if you want you\n",
      "2945 Loss: 1.0024534 Prediction:  if you want you\n",
      "2946 Loss: 1.0024588 Prediction:  if you want you\n",
      "2947 Loss: 1.0024594 Prediction:  if you want you\n",
      "2948 Loss: 1.0024676 Prediction:  if you want you\n",
      "2949 Loss: 1.0024658 Prediction:  if you want you\n",
      "2950 Loss: 1.0024754 Prediction:  if you want you\n",
      "2951 Loss: 1.0024694 Prediction:  if you want you\n",
      "2952 Loss: 1.0024768 Prediction:  if you want you\n",
      "2953 Loss: 1.0024672 Prediction:  if you want you\n",
      "2954 Loss: 1.0024697 Prediction:  if you want you\n",
      "2955 Loss: 1.0024602 Prediction:  if you want you\n",
      "2956 Loss: 1.0024592 Prediction:  if you want you\n",
      "2957 Loss: 1.0024529 Prediction:  if you want you\n",
      "2958 Loss: 1.0024508 Prediction:  if you want you\n",
      "2959 Loss: 1.0024474 Prediction:  if you want you\n",
      "2960 Loss: 1.0024457 Prediction:  if you want you\n",
      "2961 Loss: 1.002444 Prediction:  if you want you\n",
      "2962 Loss: 1.0024432 Prediction:  if you want you\n",
      "2963 Loss: 1.0024426 Prediction:  if you want you\n",
      "2964 Loss: 1.0024422 Prediction:  if you want you\n",
      "2965 Loss: 1.002442 Prediction:  if you want you\n",
      "2966 Loss: 1.002442 Prediction:  if you want you\n",
      "2967 Loss: 1.0024421 Prediction:  if you want you\n",
      "2968 Loss: 1.0024421 Prediction:  if you want you\n",
      "2969 Loss: 1.0024425 Prediction:  if you want you\n",
      "2970 Loss: 1.0024426 Prediction:  if you want you\n",
      "2971 Loss: 1.0024431 Prediction:  if you want you\n",
      "2972 Loss: 1.0024434 Prediction:  if you want you\n",
      "2973 Loss: 1.0024439 Prediction:  if you want you\n",
      "2974 Loss: 1.0024445 Prediction:  if you want you\n",
      "2975 Loss: 1.0024455 Prediction:  if you want you\n",
      "2976 Loss: 1.002446 Prediction:  if you want you\n",
      "2977 Loss: 1.002448 Prediction:  if you want you\n",
      "2978 Loss: 1.0024489 Prediction:  if you want you\n",
      "2979 Loss: 1.0024519 Prediction:  if you want you\n",
      "2980 Loss: 1.0024527 Prediction:  if you want you\n",
      "2981 Loss: 1.0024577 Prediction:  if you want you\n",
      "2982 Loss: 1.002458 Prediction:  if you want you\n",
      "2983 Loss: 1.0024647 Prediction:  if you want you\n",
      "2984 Loss: 1.0024627 Prediction:  if you want you\n",
      "2985 Loss: 1.0024703 Prediction:  if you want you\n",
      "2986 Loss: 1.0024649 Prediction:  if you want you\n",
      "2987 Loss: 1.0024705 Prediction:  if you want you\n",
      "2988 Loss: 1.0024629 Prediction:  if you want you\n",
      "2989 Loss: 1.0024648 Prediction:  if you want you\n",
      "2990 Loss: 1.0024574 Prediction:  if you want you\n",
      "2991 Loss: 1.0024568 Prediction:  if you want you\n",
      "2992 Loss: 1.0024515 Prediction:  if you want you\n",
      "2993 Loss: 1.0024499 Prediction:  if you want you\n",
      "2994 Loss: 1.0024469 Prediction:  if you want you\n",
      "2995 Loss: 1.0024456 Prediction:  if you want you\n",
      "2996 Loss: 1.0024439 Prediction:  if you want you\n",
      "2997 Loss: 1.0024432 Prediction:  if you want you\n",
      "2998 Loss: 1.0024425 Prediction:  if you want you\n",
      "2999 Loss: 1.0024419 Prediction:  if you want you\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "## data 생성\n",
    "\n",
    "sample = ' if you want you'\n",
    "idx2char = list(set(sample))\n",
    "char2idx = {c: i for i, c in enumerate(idx2char)}\n",
    "\n",
    "# 하이퍼파라미터\n",
    "dic_size = len(char2idx)\n",
    "rnn_hidden_size = len(char2idx)\n",
    "num_classes = len(char2idx)\n",
    "batch_size = 1\n",
    "sequence_length = len(sample) - 1\n",
    "\n",
    "sample_idx = [char2idx[c] for c in sample]\n",
    "x_data = [sample_idx[:-1]]\n",
    "y_data = [sample_idx[1:]]\n",
    "\n",
    "X = tf.placeholder(tf.int32, [None, sequence_length])\n",
    "Y = tf.placeholder(tf.int32, [None, sequence_length])\n",
    "\n",
    "X_one_hot = tf.one_hot(X, num_classes)              # shape 확인 필요!\n",
    "\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=rnn_hidden_size, state_is_tuple=True)\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X_one_hot, initial_state=initial_state, dtype=tf.float32)\n",
    "\n",
    "weights = tf.ones([batch_size, sequence_length])\n",
    "sequence_loss = tf.contrib.seq2seq.sequence_loss(logits=outputs, targets=Y, weights=weights)\n",
    "loss = tf.reduce_mean(sequence_loss)\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.1).minimize(loss)\n",
    "\n",
    "prediction = tf.argmax(outputs, axis=2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(3000):\n",
    "        l, _ = sess.run([loss, train], feed_dict={X: x_data, Y: y_data})\n",
    "        result = sess.run(prediction, feed_dict={X: x_data})\n",
    "\n",
    "        result_str = [idx2char[c] for c in np.squeeze(result)] \n",
    "        print(i, \"Loss:\", l, \"Prediction: \", ''.join(result_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## long sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' 0.22896533\n",
      "'496 23  ip, don't 0.22896533\n",
      "t496 24 ep, don't  0.22896533\n",
      " 496 25 l, don't d 0.22896533\n",
      "d496 26   don't dr 0.22896533\n",
      "r496 27  don't dru 0.22896533\n",
      "u496 28 ton't drum 0.22896533\n",
      "m496 29  n't drum  0.22896533\n",
      " 496 30  't arum u 0.22896533\n",
      "u496 31 dt arum up 0.22896533\n",
      "p496 32 t arum up  0.22896533\n",
      " 496 33  arum up p 0.22896533\n",
      "p496 34 toum up pe 0.22896533\n",
      "e496 35  um up peo 0.22896533\n",
      "o496 36  m up peop 0.22896533\n",
      "p496 37 m up peopl 0.22896533\n",
      "l496 38  tp people 0.22896533\n",
      "e496 39 tp people  0.22896533\n",
      " 496 40 m people t 0.22896533\n",
      "t496 41  people to 0.22896533\n",
      "o496 42 teople tog 0.22896533\n",
      "g496 43  ople toge 0.22896533\n",
      "e496 44 mple toget 0.22896533\n",
      "t496 45  le togeth 0.22896533\n",
      "h496 46  e togethe 0.22896533\n",
      "e496 47 ectogether 0.22896533\n",
      "r496 48 meogether  0.22896533\n",
      " 496 49 to ether t 0.22896533\n",
      "t496 50   ether to 0.22896533\n",
      "o496 51  ether to  0.22896533\n",
      " 496 52 ether to c 0.22896533\n",
      "c496 53 mher to co 0.22896533\n",
      "o496 54  er te col 0.22896533\n",
      "l496 55 er to coll 0.22896533\n",
      "l496 56 m te colle 0.22896533\n",
      "e496 57  te collec 0.22896533\n",
      "c496 58 to collect 0.22896533\n",
      "t496 59   collect  0.22896533\n",
      " 496 60  collect w 0.22896533\n",
      "w496 61 tollect wo 0.22896533\n",
      "o496 62 ollect woo 0.22896533\n",
      "o496 63  lect wood 0.22896533\n",
      "d496 64 eect wood  0.22896533\n",
      " 496 65 ect wood a 0.22896533\n",
      "a496 66 mt wood an 0.22896533\n",
      "n496 67 o wood and 0.22896533\n",
      "d496 68  aood and  0.22896533\n",
      " 496 69 tord and d 0.22896533\n",
      "d496 70 ord and do 0.22896533\n",
      "o496 71  d and don 0.22896533\n",
      "n496 72   and don' 0.22896533\n",
      "'496 73  and don't 0.22896533\n",
      "t496 74 tnd don't  0.22896533\n",
      " 496 75 nd don't a 0.22896533\n",
      "a496 76 d don't as 0.22896533\n",
      "s496 77  aon't ass 0.22896533\n",
      "s496 78 ton't dssi 0.22896533\n",
      "i496 79  n't dssig 0.22896533\n",
      "g496 80  't assign 0.22896533\n",
      "n496 81 dt assign  0.22896533\n",
      " 496 82 t assign t 0.22896533\n",
      "t496 83  assign th 0.22896533\n",
      "h496 84 tnsign the 0.22896533\n",
      "e496 85 nkign them 0.22896533\n",
      "m496 86  ign them  0.22896533\n",
      " 496 87  tn them t 0.22896533\n",
      "t496 88 ln them ta 0.22896533\n",
      "a496 89 e them tas 0.22896533\n",
      "s496 90 dthem task 0.22896533\n",
      "k496 91 toem tasks 0.22896533\n",
      "s496 92  er tasks  0.22896533\n",
      " 496 93 er tasks a 0.22896533\n",
      "a496 94 m tasks an 0.22896533\n",
      "n496 95  tasks and 0.22896533\n",
      "d496 96 tosks and  0.22896533\n",
      " 496 97  sks and w 0.22896533\n",
      "w496 98 nks and wo 0.22896533\n",
      "o496 99  s and wor 0.22896533\n",
      "r496 100 , and work 0.22896533\n",
      "k496 101  and work, 0.22896533\n",
      ",496 102 tnd dork,  0.22896533\n",
      " 496 103 nd dork, b 0.22896533\n",
      "b496 104 d dork, bu 0.22896533\n",
      "u496 105  aork, but 0.22896533\n",
      "t496 106 tork, but  0.22896533\n",
      " 496 107 ork, but r 0.22896533\n",
      "r496 108   , but ra 0.22896533\n",
      "a496 109  , but rat 0.22896533\n",
      "t496 110 , but rath 0.22896533\n",
      "h496 111  dut rathe 0.22896533\n",
      "e496 112 tut rather 0.22896533\n",
      "r496 113 ut rather  0.22896533\n",
      " 496 114 m rather t 0.22896533\n",
      "t496 115  aather te 0.22896533\n",
      "e496 116 tather tea 0.22896533\n",
      "a496 117  ther teac 0.22896533\n",
      "c496 118 nher teach 0.22896533\n",
      "h496 119  er teach  0.22896533\n",
      " 496 120 er toach t 0.22896533\n",
      "t496 121 m teach th 0.22896533\n",
      "h496 122  teach the 0.22896533\n",
      "e496 123 toach them 0.22896533\n",
      "m496 124  ach them  0.22896533\n",
      " 496 125 mch them t 0.22896533\n",
      "t496 126 nh them to 0.22896533\n",
      "o496 127 o them to  0.22896533\n",
      " 496 128 ethem to l 0.22896533\n",
      "l496 129 toem ta lo 0.22896533\n",
      "o496 130  er ta lon 0.22896533\n",
      "n496 131 er ta long 0.22896533\n",
      "g496 132 m ta long  0.22896533\n",
      " 496 133  ta long f 0.22896533\n",
      "f496 134 to cong fo 0.22896533\n",
      "o496 135   cong for 0.22896533\n",
      "r496 136  cong for  0.22896533\n",
      " 496 137 tong for t 0.22896533\n",
      "t496 138 eng for th 0.22896533\n",
      "h496 139  ' for the 0.22896533\n",
      "e496 140 d for the  0.22896533\n",
      " 496 141 efor the e 0.22896533\n",
      "e496 142 tor the en 0.22896533\n",
      "n496 143  r the end 0.22896533\n",
      "d496 144   the endl 0.22896533\n",
      "l496 145  tee endle 0.22896533\n",
      "e496 146 toemendles 0.22896533\n",
      "s496 147  erendless 0.22896533\n",
      "s496 148 erendless  0.22896533\n",
      " 496 149 mendless i 0.22896533\n",
      "i496 150 tndless im 0.22896533\n",
      "m496 151 mdless imm 0.22896533\n",
      "m496 152 d ess imme 0.22896533\n",
      "e496 153  ess immen 0.22896533\n",
      "n496 154 ecs immens 0.22896533\n",
      "s496 155 ms immensi 0.22896533\n",
      "i496 156  iimmensit 0.22896533\n",
      "t496 157  ammensity 0.22896533\n",
      "y496 158 tmmensity  0.22896533\n",
      " 496 159 lmensity o 0.22896533\n",
      "o496 160  ensity of 0.22896533\n",
      "f496 161  nsity of  0.22896533\n",
      " 496 162 mdity of t 0.22896533\n",
      "t496 163 dity of th 0.22896533\n",
      "h496 164  ty of the 0.22896533\n",
      "e496 165 ly of the  0.22896533\n",
      " 496 166   of the s 0.22896533\n",
      "s496 167  of the se 0.22896533\n",
      "e496 168 tf the sea 0.22896533\n",
      "a496 169   the sea. 0.22896533\n",
      ".497 0 t you want 0.22893883\n",
      "t you want497 1 oyou want  0.22893883\n",
      " 497 2 tou want t 0.22893883\n",
      "t497 3 ou want to 0.22893883\n",
      "o497 4 n want to  0.22893883\n",
      " 497 5 twant to b 0.22893883\n",
      "b497 6 tont to bu 0.22893883\n",
      "u497 7 ont to bui 0.22893883\n",
      "i497 8 nd to buil 0.22893883\n",
      "l497 9 d to build 0.22893883\n",
      "d497 10 hwo build  0.22893883\n",
      " 497 11 to luild a 0.22893883\n",
      "a497 12 h luild a  0.22893883\n",
      " 497 13 nluild a s 0.22893883\n",
      "s497 14 tuild a sh 0.22893883\n",
      "h497 15 uild a shi 0.22893883\n",
      "i497 16 tld a ship 0.22893883\n",
      "p497 17 td a ship, 0.22893883\n",
      ",497 18 e a ship,  0.22893883\n",
      " 497 19  anship, d 0.22893883\n",
      "d497 20 tnship, do 0.22893883\n",
      "o497 21 nship, don 0.22893883\n",
      "n497 22 thip, don' 0.22893883\n",
      "'497 23 sip, don't 0.22893883\n",
      "t497 24 ep, don't  0.22893883\n",
      " 497 25 t, don't d 0.22893883\n",
      "d497 26 l don't dr 0.22893883\n",
      "r497 27  bon't dru 0.22893883\n",
      "u497 28 ton't drum 0.22893883\n",
      "m497 29  n't drum  0.22893883\n",
      " 497 30 n't drum u 0.22893883\n",
      "u497 31 dt drum up 0.22893883\n",
      "p497 32 t drum up  0.22893883\n",
      " 497 33 hwrum up p 0.22893883\n",
      "p497 34 toum up pe 0.22893883\n",
      "e497 35  um up peo 0.22893883\n",
      "o497 36  m up peop 0.22893883\n",
      "p497 37 t up peopl 0.22893883\n",
      "l497 38  tp people 0.22893883\n",
      "e497 39 tp people  0.22893883\n",
      " 497 40 t people t 0.22893883\n",
      "t497 41 lpeople to 0.22893883\n",
      "o497 42 teople tog 0.22893883\n",
      "g497 43 lople toge 0.22893883\n",
      "e497 44 rple toget 0.22893883\n",
      "t497 45 nle togeth 0.22893883\n",
      "h497 46 le togethe 0.22893883\n",
      "e497 47 estogether 0.22893883\n",
      "r497 48 rtogether  0.22893883\n",
      " 497 49 to ether t 0.22893883\n",
      "t497 50 h ether to 0.22893883\n",
      "o497 51 nether to  0.22893883\n",
      " 497 52 ether to c 0.22893883\n",
      "c497 53 rher to co 0.22893883\n",
      "o497 54 her to col 0.22893883\n",
      "l497 55 er to coll 0.22893883\n",
      "l497 56 r to colle 0.22893883\n",
      "e497 57  th collec 0.22893883\n",
      "c497 58 to lollect 0.22893883\n",
      "t497 59 h lollect  0.22893883\n",
      " 497 60 nlollect w 0.22893883\n",
      "w497 61 tollect wo 0.22893883\n",
      "o497 62 ollect woo 0.22893883\n",
      "o497 63 nlect wood 0.22893883\n",
      "d497 64 eect wood  0.22893883\n",
      " 497 65 est wood a 0.22893883\n",
      "a497 66 rt wood an 0.22893883\n",
      "n497 67 o wood and 0.22893883\n",
      "d497 68 hwood and  0.22893883\n",
      " 497 69 tood and d 0.22893883\n",
      "d497 70 ood and do 0.22893883\n",
      "o497 71 nd and don 0.22893883\n",
      "n497 72 n and don' 0.22893883\n",
      "'497 73  and don't 0.22893883\n",
      "t497 74 tnd won't  0.22893883\n",
      " 497 75 nd won't a 0.22893883\n",
      "a497 76 d won't as 0.22893883\n",
      "s497 77  aon't ass 0.22893883\n",
      "s497 78 ton't dssi 0.22893883\n",
      "i497 79  n't dssig 0.22893883\n",
      "g497 80 n't dssign 0.22893883\n",
      "n497 81 dt dssign  0.22893883\n",
      " 497 82 t dssign t 0.22893883\n",
      "t497 83 hwssign th 0.22893883\n",
      "h497 84 tnsign the 0.22893883\n",
      "e497 85 nkign them 0.22893883\n",
      "m497 86 s gn them  0.22893883\n",
      " 497 87 sgn them t 0.22893883\n",
      "t497 88 tn them ta 0.22893883\n",
      "a497 89 e them tas 0.22893883\n",
      "s497 90 dthem task 0.22893883\n",
      "k497 91 toem tosks 0.22893883\n",
      "s497 92 her tosks  0.22893883\n",
      " 497 93 er tosks a 0.22893883\n",
      "a497 94 r tosks an 0.22893883\n",
      "n497 95  tosks and 0.22893883\n",
      "d497 96 tosks and  0.22893883\n",
      " 497 97 hsks and w 0.22893883\n",
      "w497 98 nks and wo 0.22893883\n",
      "o497 99 ss and wor 0.22893883\n",
      "r497 100 , and work 0.22893883\n",
      "k497 101 sind work, 0.22893883\n",
      ",497 102 tnd work,  0.22893883\n",
      " 497 103 nd work, b 0.22893883\n",
      "b497 104 d work, bu 0.22893883\n",
      "u497 105  aork, but 0.22893883\n",
      "t497 106 took, but  0.22893883\n",
      " 497 107 ook, but r 0.22893883\n",
      "r497 108 nk, but ra 0.22893883\n",
      "a497 109  , but rat 0.22893883\n",
      "t497 110 , but rath 0.22893883\n",
      "h497 111  but rathe 0.22893883\n",
      "e497 112 tui rather 0.22893883\n",
      "r497 113 ui rather  0.22893883\n",
      " 497 114 t rather t 0.22893883\n",
      "t497 115 hwather te 0.22893883\n",
      "e497 116 tather tea 0.22893883\n",
      "a497 117  ther teac 0.22893883\n",
      "c497 118 nher teach 0.22893883\n",
      "h497 119 her toach  0.22893883\n",
      " 497 120 er toach t 0.22893883\n",
      "t497 121 r toach th 0.22893883\n",
      "h497 122  thach the 0.22893883\n",
      "e497 123 toach them 0.22893883\n",
      "m497 124 hach them  0.22893883\n",
      " 497 125 rch them t 0.22893883\n",
      "t497 126 nh them to 0.22893883\n",
      "o497 127 o them to  0.22893883\n",
      " 497 128 ethem to l 0.22893883\n",
      "l497 129 toem to lo 0.22893883\n",
      "o497 130 her to lon 0.22893883\n",
      "n497 131 er to long 0.22893883\n",
      "g497 132 r to long  0.22893883\n",
      " 497 133  to long f 0.22893883\n",
      "f497 134 to long fo 0.22893883\n",
      "o497 135 h long for 0.22893883\n",
      "r497 136 nlong for  0.22893883\n",
      " 497 137 tong for t 0.22893883\n",
      "t497 138 eng for th 0.22893883\n",
      "h497 139 n' for the 0.22893883\n",
      "e497 140 d for the  0.22893883\n",
      " 497 141 efor the e 0.22893883\n",
      "e497 142 tor the en 0.22893883\n",
      "n497 143 or the end 0.22893883\n",
      "d497 144 nkthe endl 0.22893883\n",
      "l497 145  the endle 0.22893883\n",
      "e497 146 toemendles 0.22893883\n",
      "s497 147 herendless 0.22893883\n",
      "s497 148 erendless  0.22893883\n",
      " 497 149 rtndless i 0.22893883\n",
      "i497 150 tndless im 0.22893883\n",
      "m497 151 rsless imm 0.22893883\n",
      "m497 152 d ess imme 0.22893883\n",
      "e497 153  ess immen 0.22893883\n",
      "n497 154 ess immens 0.22893883\n",
      "s497 155 rs immensi 0.22893883\n",
      "i497 156 s immensit 0.22893883\n",
      "t497 157 simmensity 0.22893883\n",
      "y497 158 tmmensity  0.22893883\n",
      " 497 159 tmensity o 0.22893883\n",
      "o497 160  ensity of 0.22893883\n",
      "f497 161  nsity of  0.22893883\n",
      " 497 162 rsity of t 0.22893883\n",
      "t497 163 dity of th 0.22893883\n",
      "h497 164 sgy of the 0.22893883\n",
      "e497 165 ty of the  0.22893883\n",
      " 497 166 h of the s 0.22893883\n",
      "s497 167 oof the se 0.22893883\n",
      "e497 168 tf the sea 0.22893883\n",
      "a497 169 n the sea. 0.22893883\n",
      ".498 0 p you want 0.22892901\n",
      "p you want498 1  you want  0.22892901\n",
      " 498 2 tou want t 0.22892901\n",
      "t498 3 ou want to 0.22892901\n",
      "o498 4 n want to  0.22892901\n",
      " 498 5 twant to b 0.22892901\n",
      "b498 6 tont to bu 0.22892901\n",
      "u498 7 ont to bui 0.22892901\n",
      "i498 8 nd to buil 0.22892901\n",
      "l498 9 d to build 0.22892901\n",
      "d498 10  ao build  0.22892901\n",
      " 498 11 to luild a 0.22892901\n",
      "a498 12   luild a  0.22892901\n",
      " 498 13 ncuild a s 0.22892901\n",
      "s498 14 tuild a sh 0.22892901\n",
      "h498 15 uild a shi 0.22892901\n",
      "i498 16 tld a ship 0.22892901\n",
      "p498 17 pd a ship, 0.22892901\n",
      ",498 18 e a ship,  0.22892901\n",
      " 498 19  a ship, d 0.22892901\n",
      "d498 20 tnship, do 0.22892901\n",
      "o498 21 nship, don 0.22892901\n",
      "n498 22 thip, don' 0.22892901\n",
      "'498 23 iip, don't 0.22892901\n",
      "t498 24 ep, don't  0.22892901\n",
      " 498 25 p, don't d 0.22892901\n",
      "d498 26   don't dr 0.22892901\n",
      "r498 27  bon't dru 0.22892901\n",
      "u498 28 ton't arum 0.22892901\n",
      "m498 29  n't arum  0.22892901\n",
      " 498 30 n't arum u 0.22892901\n",
      "u498 31 dt arum up 0.22892901\n",
      "p498 32 t arum up  0.22892901\n",
      " 498 33  arum up p 0.22892901\n",
      "p498 34 toum up pe 0.22892901\n",
      "e498 35  um up peo 0.22892901\n",
      "o498 36  m up peop 0.22892901\n",
      "p498 37 t up peopl 0.22892901\n",
      "l498 38  tp people 0.22892901\n",
      "e498 39 tp people  0.22892901\n",
      " 498 40 t people t 0.22892901\n",
      "t498 41  people to 0.22892901\n",
      "o498 42 teople tog 0.22892901\n",
      "g498 43  ople toge 0.22892901\n",
      "e498 44 mple toget 0.22892901\n",
      "t498 45 nle togeth 0.22892901\n",
      "h498 46  e togethe 0.22892901\n",
      "e498 47 e together 0.22892901\n",
      "r498 48 meogether  0.22892901\n",
      " 498 49 to ether t 0.22892901\n",
      "t498 50   ether to 0.22892901\n",
      "o498 51 nether to  0.22892901\n",
      " 498 52 nther to c0.22892901\n",
      "c498 53 mher to co 0.22892901\n",
      "o498 54  em te col 0.22892901\n",
      "l498 55 em te coll 0.22892901\n",
      "l498 56 m te colle 0.22892901\n",
      "e498 57  te collec 0.22892901\n",
      "c498 58 to lollect 0.22892901\n",
      "t498 59   lollect  0.22892901\n",
      " 498 60 ncollect w 0.22892901\n",
      "w498 61 tollect wo 0.22892901\n",
      "o498 62 ollect woo 0.22892901\n",
      "o498 63 nlect wood 0.22892901\n",
      "d498 64 eect wood  0.22892901\n",
      " 498 65 e t wood a 0.22892901\n",
      "a498 66 mt wood an 0.22892901\n",
      "n498 67 o wood and 0.22892901\n",
      "d498 68  aood and  0.22892901\n",
      " 498 69 tood and d 0.22892901\n",
      "d498 70 ord and do 0.22892901\n",
      "o498 71 nd and don 0.22892901\n",
      "n498 72 n and don' 0.22892901\n",
      "'498 73  a d don't 0.22892901\n",
      "t498 74 tnd won't  0.22892901\n",
      " 498 75 nd won't a 0.22892901\n",
      "a498 76 d won't as 0.22892901\n",
      "s498 77  aon't ass 0.22892901\n",
      "s498 78 ton't assi 0.22892901\n",
      "i498 79  n't assig 0.22892901\n",
      "g498 80 n't assign 0.22892901\n",
      "n498 81 dt assign  0.22892901\n",
      " 498 82 t assign t 0.22892901\n",
      "t498 83  assign th 0.22892901\n",
      "h498 84 tnsign the 0.22892901\n",
      "e498 85 nkign them 0.22892901\n",
      "m498 86 iign them  0.22892901\n",
      " 498 87 ign them t 0.22892901\n",
      "t498 88 pn them ta 0.22892901\n",
      "a498 89 n them tas 0.22892901\n",
      "s498 90 dthem task 0.22892901\n",
      "k498 91 toem tasks 0.22892901\n",
      "s498 92  em tasks  0.22892901\n",
      " 498 93 em tasks a 0.22892901\n",
      "a498 94 m tasks an 0.22892901\n",
      "n498 95  tasks and 0.22892901\n",
      "d498 96 tosks and  0.22892901\n",
      " 498 97  sks and w 0.22892901\n",
      "w498 98 nks and wo 0.22892901\n",
      "o498 99 is and wor 0.22892901\n",
      "r498 100 , and work 0.22892901\n",
      "k498 101 iand work, 0.22892901\n",
      ",498 102 tnd work,  0.22892901\n",
      " 498 103 nd work, b 0.22892901\n",
      "b498 104 d work, bu 0.22892901\n",
      "u498 105  aork, but 0.22892901\n",
      "t498 106 took, but  0.22892901\n",
      " 498 107 ork, but r 0.22892901\n",
      "r498 108 n , but ra 0.22892901\n",
      "a498 109  , but rat 0.22892901\n",
      "t498 110 , but rath 0.22892901\n",
      "h498 111  but rathe 0.22892901\n",
      "e498 112 tui rather 0.22892901\n",
      "r498 113 ui rather  0.22892901\n",
      " 498 114 t rather t 0.22892901\n",
      "t498 115  aather te 0.22892901\n",
      "e498 116 tather tea 0.22892901\n",
      "a498 117  ther teac 0.22892901\n",
      "c498 118 nher teach 0.22892901\n",
      "h498 119  em teach  0.22892901\n",
      " 498 120 em teach t 0.22892901\n",
      "t498 121 m teach th 0.22892901\n",
      "h498 122  teach the 0.22892901\n",
      "e498 123 toach them 0.22892901\n",
      "m498 124  ach them  0.22892901\n",
      " 498 125 mch them t 0.22892901\n",
      "t498 126 nh them to 0.22892901\n",
      "o498 127 o them to  0.22892901\n",
      " 498 128 ethem to l 0.22892901\n",
      "l498 129 toem ta lo 0.22892901\n",
      "o498 130  em ta lon 0.22892901\n",
      "n498 131 em ta long 0.22892901\n",
      "g498 132 m ta long  0.22892901\n",
      " 498 133  ta long f 0.22892901\n",
      "f498 134 to long fo 0.22892901\n",
      "o498 135   long for 0.22892901\n",
      "r498 136 ncong for  0.22892901\n",
      " 498 137 tong for t 0.22892901\n",
      "t498 138 eng for th 0.22892901\n",
      "h498 139 n' for the 0.22892901\n",
      "e498 140 d for the  0.22892901\n",
      " 498 141 nfor the e 0.22892901\n",
      "e498 142 tor the en 0.22892901\n",
      "n498 143  r the end 0.22892901\n",
      "d498 144 n the endl 0.22892901\n",
      "l498 145  tee endle 0.22892901\n",
      "e498 146 toemendles 0.22892901\n",
      "s498 147  emendless 0.22892901\n",
      "s498 148 emendless  0.22892901\n",
      " 498 149 mendless i 0.22892901\n",
      "i498 150 tndless im 0.22892901\n",
      "m498 151 msless imm 0.22892901\n",
      "m498 152 d ess imme 0.22892901\n",
      "e498 153  ess immen 0.22892901\n",
      "n498 154 e s immens 0.22892901\n",
      "s498 155 ms immensi 0.22892901\n",
      "i498 156 iiimmensit 0.22892901\n",
      "t498 157 iammensity 0.22892901\n",
      "y498 158 tmmensity  0.22892901\n",
      " 498 159 pmensity o 0.22892901\n",
      "o498 160  ensity of 0.22892901\n",
      "f498 161  nsity of  0.22892901\n",
      " 498 162 msity of t 0.22892901\n",
      "t498 163 dity of th 0.22892901\n",
      "h498 164 igy of the 0.22892901\n",
      "e498 165 py of the  0.22892901\n",
      " 498 166   of the s 0.22892901\n",
      "s498 167 oof the se 0.22892901\n",
      "e498 168 tf the sea 0.22892901\n",
      "a498 169 n the sea. 0.22892901\n",
      ".499 0 l you want 0.22893216\n",
      "l you want499 1 oyou want  0.22893216\n",
      " 499 2 tou want t 0.22893216\n",
      "t499 3  u want to 0.22893216\n",
      "o499 4 n want to  0.22893216\n",
      " 499 5  want to b 0.22893216\n",
      "b499 6 tont to bu 0.22893216\n",
      "u499 7 ont to bui 0.22893216\n",
      "i499 8 nd to buil 0.22893216\n",
      "l499 9 d to build 0.22893216\n",
      "d499 10 hdo build  0.22893216\n",
      " 499 11 to cuild a 0.22893216\n",
      "a499 12 h build a  0.22893216\n",
      " 499 13 nbuild a s 0.22893216\n",
      "s499 14 tutld a sh 0.22893216\n",
      "h499 15 utld a shi 0.22893216\n",
      "i499 16  ld a ship 0.22893216\n",
      "p499 17 ld a ship, 0.22893216\n",
      ",499 18 e a ship,  0.22893216\n",
      " 499 19  anship, d 0.22893216\n",
      "d499 20 tnship, do 0.22893216\n",
      "o499 21 nship, don 0.22893216\n",
      "n499 22 thip, don' 0.22893216\n",
      "'499 23  ip, don't 0.22893216\n",
      "t499 24 ep, don't  0.22893216\n",
      " 499 25 l, don't d 0.22893216\n",
      "d499 26   don't dr 0.22893216\n",
      "r499 27  don't dru 0.22893216\n",
      "u499 28 ton't drum 0.22893216\n",
      "m499 29  n't drum  0.22893216\n",
      " 499 30 n't drum u 0.22893216\n",
      "u499 31 dt drum up 0.22893216\n",
      "p499 32 t drum up  0.22893216\n",
      " 499 33 hdrum up p 0.22893216\n",
      "p499 34 toum up pe 0.22893216\n",
      "e499 35  um up peo 0.22893216\n",
      "o499 36  m up peop 0.22893216\n",
      "p499 37   up peopl 0.22893216\n",
      "l499 38  tp people 0.22893216\n",
      "e499 39 tp people  0.22893216\n",
      " 499 40   people t 0.22893216\n",
      "t499 41  people to 0.22893216\n",
      "o499 42 teople tog 0.22893216\n",
      "g499 43  ople toge 0.22893216\n",
      "e499 44 rple toget 0.22893216\n",
      "t499 45 nle togeth 0.22893216\n",
      "h499 46  e togethe 0.22893216\n",
      "e499 47 e together 0.22893216\n",
      "r499 48 rtogether  0.22893216\n",
      " 499 49 to ether t 0.22893216\n",
      "t499 50 h ether to 0.22893216\n",
      "o499 51 nether to  0.22893216\n",
      " 499 52  ther to c 0.22893216\n",
      "c499 53 rher to co 0.22893216\n",
      "o499 54 her to col 0.22893216\n",
      "l499 55 er to coll 0.22893216\n",
      "l499 56 r to colle 0.22893216\n",
      "e499 57  to collec 0.22893216\n",
      "c499 58 to collect 0.22893216\n",
      "t499 59 h bollect  0.22893216\n",
      " 499 60 nbollect w 0.22893216\n",
      "w499 61 tollect wo 0.22893216\n",
      "o499 62 ollect woo 0.22893216\n",
      "o499 63 nlect wood 0.22893216\n",
      "d499 64 eect wood  0.22893216\n",
      " 499 65 e t wood a 0.22893216\n",
      "a499 66 rt wood an 0.22893216\n",
      "n499 67 o wood and 0.22893216\n",
      "d499 68 hdood and  0.22893216\n",
      " 499 69 tord and d 0.22893216\n",
      "d499 70 ord and do 0.22893216\n",
      "o499 71 nd and don 0.22893216\n",
      "n499 72 n and don' 0.22893216\n",
      "'499 73  and don't 0.22893216\n",
      "t499 74 tnd don't  0.22893216\n",
      " 499 75 nd don't a 0.22893216\n",
      "a499 76 d don't as 0.22893216\n",
      "s499 77  aon't ass 0.22893216\n",
      "s499 78 ton't dssi 0.22893216\n",
      "i499 79  n't dssig 0.22893216\n",
      "g499 80 n't dssign 0.22893216\n",
      "n499 81 dt dssign  0.22893216\n",
      " 499 82 t dssign t 0.22893216\n",
      "t499 83 hdssign th 0.22893216\n",
      "h499 84 tnsign the 0.22893216\n",
      "e499 85 nkign them 0.22893216\n",
      "m499 86   gn them  0.22893216\n",
      " 499 87  tn them t 0.22893216\n",
      "t499 88 ln them ta 0.22893216\n",
      "a499 89   them tas 0.22893216\n",
      "s499 90 dthem task 0.22893216\n",
      "k499 91 toem tasks 0.22893216\n",
      "s499 92 her tosks  0.22893216\n",
      " 499 93 er tasks a 0.22893216\n",
      "a499 94 r tasks an 0.22893216\n",
      "n499 95  tosks and 0.22893216\n",
      "d499 96 tosks and  0.22893216\n",
      " 499 97 hsks and w 0.22893216\n",
      "w499 98 nks and wo 0.22893216\n",
      "o499 99  s and wor 0.22893216\n",
      "r499 100 , and work 0.22893216\n",
      "k499 101  ind work, 0.22893216\n",
      ",499 102 tnd dork,  0.22893216\n",
      " 499 103 nd dork, b 0.22893216\n",
      "b499 104 d dork, bu 0.22893216\n",
      "u499 105  aork, but 0.22893216\n",
      "t499 106 tork, but  0.22893216\n",
      " 499 107 ork, but r 0.22893216\n",
      "r499 108 nk, but ra 0.22893216\n",
      "a499 109  , but rat 0.22893216\n",
      "t499 110 , but rath 0.22893216\n",
      "h499 111  dut rathe 0.22893216\n",
      "e499 112 tut rather 0.22893216\n",
      "r499 113 ut rather  0.22893216\n",
      " 499 114   rather t 0.22893216\n",
      "t499 115 hdather te 0.22893216\n",
      "e499 116 tather tea 0.22893216\n",
      "a499 117  ther teac 0.22893216\n",
      "c499 118 nher teach 0.22893216\n",
      "h499 119 her toach  0.22893216\n",
      " 499 120 er toach t 0.22893216\n",
      "t499 121 r toach th 0.22893216\n",
      "h499 122  toach the 0.22893216\n",
      "e499 123 toach them 0.22893216\n",
      "m499 124 hach them  0.22893216\n",
      " 499 125 rch them t 0.22893216\n",
      "t499 126 nh them to 0.22893216\n",
      "o499 127 o them to  0.22893216\n",
      " 499 128 ethem to l 0.22893216\n",
      "l499 129 toem ta lo 0.22893216\n",
      "o499 130 her to lon 0.22893216\n",
      "n499 131 er ta long 0.22893216\n",
      "g499 132 r ta long  0.22893216\n",
      " 499 133  to long f 0.22893216\n",
      "f499 134 to cong fo 0.22893216\n",
      "o499 135 h bong for 0.22893216\n",
      "r499 136 nbong for  0.22893216\n",
      " 499 137 tong for t 0.22893216\n",
      "t499 138 eng for th 0.22893216\n",
      "h499 139 n' for the 0.22893216\n",
      "e499 140 d for the  0.22893216\n",
      " 499 141  for the e 0.22893216\n",
      "e499 142 tor the en 0.22893216\n",
      "n499 143 or the end 0.22893216\n",
      "d499 144 nkthe endl 0.22893216\n",
      "l499 145  toe endle 0.22893216\n",
      "e499 146 toemendles 0.22893216\n",
      "s499 147 herendless 0.22893216\n",
      "s499 148 erendless  0.22893216\n",
      " 499 149 rtndless i 0.22893216\n",
      "i499 150 tndless im 0.22893216\n",
      "m499 151 rdless imm 0.22893216\n",
      "m499 152 d ess imme 0.22893216\n",
      "e499 153  ess immen 0.22893216\n",
      "n499 154 e s immens 0.22893216\n",
      "s499 155 rs immensi 0.22893216\n",
      "i499 156   immensit 0.22893216\n",
      "t499 157  immensity 0.22893216\n",
      "y499 158 tmmensity  0.22893216\n",
      " 499 159 lmensity o 0.22893216\n",
      "o499 160  ensity of 0.22893216\n",
      "f499 161  nsity of  0.22893216\n",
      " 499 162 rdity of t 0.22893216\n",
      "t499 163 dity of th 0.22893216\n",
      "h499 164  ty of the 0.22893216\n",
      "e499 165 ly of the  0.22893216\n",
      " 499 166 h of the s 0.22893216\n",
      "s499 167  of the se 0.22893216\n",
      "e499 168 tf the sea 0.22893216\n",
      "a499 169 n the sea. 0.22893216\n",
      "."
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "sentence = (\"if you want to build a ship, don't drum up people together to \"\n",
    "            \"collect wood and don't assign them tasks and work, but rather \"\n",
    "            \"teach them to long for the endless immensity of the sea.\")\n",
    "\n",
    "# making dataset\n",
    "\n",
    "char_set = list(set(sentence))\n",
    "char_dic = {w: i for i, w in enumerate(char_set)}\n",
    "\n",
    "# 하이퍼파라미터\n",
    "data_dim = len(char_set)\n",
    "hidden_size = len(char_set)\n",
    "num_classes = len(char_set)\n",
    "sequence_length = 10        # 임의 설정\n",
    "\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for i in range(0, len(sentence) - sequence_length):\n",
    "    x_str = sentence[i : i + sequence_length]\n",
    "    y_str = sentence[i + 1 : i + sequence_length + 1]\n",
    "    print(i, x_str, '->', y_str)\n",
    "\n",
    "    x = [char_dic[c] for c in x_str]\n",
    "    y = [char_dic[c] for c in y_str]\n",
    "\n",
    "    dataX.append(x)\n",
    "    dataY.append(y)\n",
    "\n",
    "\n",
    "batch_size = len(dataX)\n",
    "\n",
    "X = tf.placeholder(tf.int32, [None, sequence_length])\n",
    "Y = tf.placeholder(tf.int32, [None, sequence_length])\n",
    "\n",
    "X_one_hot = tf.one_hot(X, num_classes)              # shape 확인 필요!\n",
    "\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_size, state_is_tuple=True)\n",
    "cell = tf.contrib.rnn.MultiRNNCell([cell] * 2, state_is_tuple=True)                             # Stacked RNN / RNN Cell 두 개 생김\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X_one_hot, dtype=tf.float32)\n",
    "\n",
    "# Softmax 추가\n",
    "X_for_softmax = tf.reshape(outputs, [-1, hidden_size])\n",
    "softmax_w = tf.get_variable(\"softmax_w\", [hidden_size, num_classes])\n",
    "softmax_b = tf.get_variable(\"softmax_b\", [num_classes])\n",
    "outputs = tf.matmul(X_for_softmax, softmax_w) + softmax_b                   # softmax output\n",
    "outputs = tf.reshape(outputs, [batch_size, sequence_length, num_classes])   # 다시 펼치기 == RNN의 shape와 맞추기\n",
    "\n",
    "weights = tf.ones([batch_size, sequence_length])\n",
    "sequence_loss = tf.contrib.seq2seq.sequence_loss(logits=outputs, targets=Y, weights=weights)\n",
    "loss = tf.reduce_mean(sequence_loss)\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.1).minimize(loss)\n",
    "\n",
    "prediction = tf.argmax(outputs, axis=2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(500):\n",
    "        _, l, results = sess.run([train, loss, outputs], feed_dict={X: dataX, Y: dataY})\n",
    "        for j, result in enumerate(results):\n",
    "            index = np.argmax(result, axis=1)\n",
    "            print(i, j, ''.join([char_set[t] for t in index]), l)\n",
    "            if j is 0:\n",
    "                print(''.join([char_set[t] for t in index]), end='')\n",
    "            else:\n",
    "                print(char_set[index[-1]], end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.09676202  0.03932744]\n",
      "  [ 0.03754191  0.10301512]\n",
      "  [-0.04537313  0.19881344]\n",
      "  [-0.11513372  0.21921165]\n",
      "  [-0.24339826  0.291442  ]]\n",
      "\n",
      " [[-0.05960281  0.08597831]\n",
      "  [-0.18274271  0.2554186 ]\n",
      "  [-0.18734144  0.28542256]\n",
      "  [ 0.          0.        ]\n",
      "  [ 0.          0.        ]]\n",
      "\n",
      " [[-0.05984093  0.11423831]\n",
      "  [-0.11729841  0.17971288]\n",
      "  [-0.16624354  0.17002793]\n",
      "  [-0.19449021  0.21243842]\n",
      "  [ 0.          0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# one hot encoding\n",
    "h = [1, 0, 0, 0]\n",
    "e = [0, 1, 0, 0]\n",
    "l = [0, 0, 1, 0]\n",
    "o = [0, 0, 0, 1]\n",
    "\n",
    "x_data = np.array([[h, e, l, l, o],\n",
    "                    [e, o, l, l, l],\n",
    "                    [l, l, e, e, l]], dtype=np.float32)  \n",
    "\n",
    "hidden_size = 2\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_size, state_is_tuple=True)\n",
    "\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, x_data, sequence_length=[5, 3, 4], dtype=tf.float32)     # sequence(5)에 맞춰서 결과 출력\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print(outputs.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Data 에측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MinMaxScaler(data):\n",
    "    ''' Min Max Normalization\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : numpy.ndarray\n",
    "        input data to be normalized\n",
    "        shape: [Batch size, dimension]\n",
    "    Returns\n",
    "    ----------\n",
    "    data : numpy.ndarry\n",
    "        normalized data\n",
    "        shape: [Batch size, dimension]\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] http://sebastianraschka.com/Articles/2014_about_feature_scaling.html\n",
    "    '''\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    # noise term prevents the zero division\n",
    "    return numerator / (denominator + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80962104 0.81567855 0.82065085 0.12909384 0.82731875]\n",
      " [0.83153522 0.84819566 0.85525015 0.15887783 0.86482773]] -> [0.86476956]\n",
      "[[0.7283466  0.74578848 0.74984431 0.12913866 0.75177622]\n",
      " [0.76657973 0.7744036  0.77860135 0.12324101 0.78680799]\n",
      " [0.78703667 0.7886533  0.79163    0.14563951 0.7768696 ]\n",
      " [0.7761671  0.79596594 0.78711094 0.15621583 0.81207623]\n",
      " [0.80962104 0.81567855 0.82065085 0.12909384 0.82731875]\n",
      " [0.83153522 0.84819566 0.85525015 0.15887783 0.86482773]\n",
      " [0.84622227 0.85345622 0.87090207 0.18725464 0.86476956]] -> [0.88467514]\n",
      "[[0.76657973 0.7744036  0.77860135 0.12324101 0.78680799]\n",
      " [0.78703667 0.7886533  0.79163    0.14563951 0.7768696 ]\n",
      " [0.7761671  0.79596594 0.78711094 0.15621583 0.81207623]\n",
      " [0.80962104 0.81567855 0.82065085 0.12909384 0.82731875]\n",
      " [0.83153522 0.84819566 0.85525015 0.15887783 0.86482773]\n",
      " [0.84622227 0.85345622 0.87090207 0.18725464 0.86476956]\n",
      " [0.87204144 0.89140731 0.89726759 0.18928924 0.88467514]] -> [0.88750224]\n",
      "[[0.78703667 0.7886533  0.79163    0.14563951 0.7768696 ]\n",
      " [0.7761671  0.79596594 0.78711094 0.15621583 0.81207623]\n",
      " [0.80962104 0.81567855 0.82065085 0.12909384 0.82731875]\n",
      " [0.83153522 0.84819566 0.85525015 0.15887783 0.86482773]\n",
      " [0.84622227 0.85345622 0.87090207 0.18725464 0.86476956]\n",
      " [0.87204144 0.89140731 0.89726759 0.18928924 0.88467514]\n",
      " [0.88224083 0.89030894 0.89923357 0.14848077 0.88750224]] -> [0.88977543]\n",
      "[[0.7761671  0.79596594 0.78711094 0.15621583 0.81207623]\n",
      " [0.80962104 0.81567855 0.82065085 0.12909384 0.82731875]\n",
      " [0.83153522 0.84819566 0.85525015 0.15887783 0.86482773]\n",
      " [0.84622227 0.85345622 0.87090207 0.18725464 0.86476956]\n",
      " [0.87204144 0.89140731 0.89726759 0.18928924 0.88467514]\n",
      " [0.88224083 0.89030894 0.89923357 0.14848077 0.88750224]\n",
      " [0.88206599 0.88741853 0.89603502 0.14482388 0.88977543]] -> [0.86922858]\n",
      "[[0.80962104 0.81567855 0.82065085 0.12909384 0.82731875]\n",
      " [0.83153522 0.84819566 0.85525015 0.15887783 0.86482773]\n",
      " [0.84622227 0.85345622 0.87090207 0.18725464 0.86476956]\n",
      " [0.87204144 0.89140731 0.89726759 0.18928924 0.88467514]\n",
      " [0.88224083 0.89030894 0.89923357 0.14848077 0.88750224]\n",
      " [0.88206599 0.88741853 0.89603502 0.14482388 0.88977543]\n",
      " [0.89098316 0.88122154 0.88831759 0.21694004 0.86922858]] -> [0.87913778]\n",
      "[[0.83153522 0.84819566 0.85525015 0.15887783 0.86482773]\n",
      " [0.84622227 0.85345622 0.87090207 0.18725464 0.86476956]\n",
      " [0.87204144 0.89140731 0.89726759 0.18928924 0.88467514]\n",
      " [0.88224083 0.89030894 0.89923357 0.14848077 0.88750224]\n",
      " [0.88206599 0.88741853 0.89603502 0.14482388 0.88977543]\n",
      " [0.89098316 0.88122154 0.88831759 0.21694004 0.86922858]\n",
      " [0.86131735 0.87198367 0.87652152 0.10916913 0.87913778]] -> [0.88560778]\n",
      "[[0.84622227 0.85345622 0.87090207 0.18725464 0.86476956]\n",
      " [0.87204144 0.89140731 0.89726759 0.18928924 0.88467514]\n",
      " [0.88224083 0.89030894 0.89923357 0.14848077 0.88750224]\n",
      " [0.88206599 0.88741853 0.89603502 0.14482388 0.88977543]\n",
      " [0.89098316 0.88122154 0.88831759 0.21694004 0.86922858]\n",
      " [0.86131735 0.87198367 0.87652152 0.10916913 0.87913778]\n",
      " [0.88037575 0.87484532 0.89706216 0.08220848 0.88560778]] -> [0.88018695]\n",
      "[[0.87204144 0.89140731 0.89726759 0.18928924 0.88467514]\n",
      " [0.88224083 0.89030894 0.89923357 0.14848077 0.88750224]\n",
      " [0.88206599 0.88741853 0.89603502 0.14482388 0.88977543]\n",
      " [0.89098316 0.88122154 0.88831759 0.21694004 0.86922858]\n",
      " [0.86131735 0.87198367 0.87652152 0.10916913 0.87913778]\n",
      " [0.88037575 0.87484532 0.89706216 0.08220848 0.88560778]\n",
      " [0.87769482 0.86913965 0.87895692 0.1076275  0.88018695]] -> [0.87056929]\n",
      "[[0.88224083 0.89030894 0.89923357 0.14848077 0.88750224]\n",
      " [0.88206599 0.88741853 0.89603502 0.14482388 0.88977543]\n",
      " [0.89098316 0.88122154 0.88831759 0.21694004 0.86922858]\n",
      " [0.86131735 0.87198367 0.87652152 0.10916913 0.87913778]\n",
      " [0.88037575 0.87484532 0.89706216 0.08220848 0.88560778]\n",
      " [0.87769482 0.86913965 0.87895692 0.1076275  0.88018695]\n",
      " [0.86755359 0.85943939 0.88329992 0.08615219 0.87056929]] -> [0.86663467]\n",
      "[[0.88206599 0.88741853 0.89603502 0.14482388 0.88977543]\n",
      " [0.89098316 0.88122154 0.88831759 0.21694004 0.86922858]\n",
      " [0.86131735 0.87198367 0.87652152 0.10916913 0.87913778]\n",
      " [0.88037575 0.87484532 0.89706216 0.08220848 0.88560778]\n",
      " [0.87769482 0.86913965 0.87895692 0.1076275  0.88018695]\n",
      " [0.86755359 0.85943939 0.88329992 0.08615219 0.87056929]\n",
      " [0.8632991  0.8577629  0.87948527 0.05516716 0.86663467]] -> [0.87141442]\n",
      "[[0.89098316 0.88122154 0.88831759 0.21694004 0.86922858]\n",
      " [0.86131735 0.87198367 0.87652152 0.10916913 0.87913778]\n",
      " [0.88037575 0.87484532 0.89706216 0.08220848 0.88560778]\n",
      " [0.87769482 0.86913965 0.87895692 0.1076275  0.88018695]\n",
      " [0.86755359 0.85943939 0.88329992 0.08615219 0.87056929]\n",
      " [0.8632991  0.8577629  0.87948527 0.05516716 0.86663467]\n",
      " [0.86265791 0.87256178 0.88059139 0.07001882 0.87141442]] -> [0.85247048]\n",
      "[[0.86131735 0.87198367 0.87652152 0.10916913 0.87913778]\n",
      " [0.88037575 0.87484532 0.89706216 0.08220848 0.88560778]\n",
      " [0.87769482 0.86913965 0.87895692 0.1076275  0.88018695]\n",
      " [0.86755359 0.85943939 0.88329992 0.08615219 0.87056929]\n",
      " [0.8632991  0.8577629  0.87948527 0.05516716 0.86663467]\n",
      " [0.86265791 0.87256178 0.88059139 0.07001882 0.87141442]\n",
      " [0.87145858 0.86206958 0.86751296 0.10081563 0.85247048]] -> [0.84588379]\n",
      "[[0.88037575 0.87484532 0.89706216 0.08220848 0.88560778]\n",
      " [0.87769482 0.86913965 0.87895692 0.1076275  0.88018695]\n",
      " [0.86755359 0.85943939 0.88329992 0.08615219 0.87056929]\n",
      " [0.8632991  0.8577629  0.87948527 0.05516716 0.86663467]\n",
      " [0.86265791 0.87256178 0.88059139 0.07001882 0.87141442]\n",
      " [0.87145858 0.86206958 0.86751296 0.10081563 0.85247048]\n",
      " [0.84123926 0.83807922 0.85495374 0.06581518 0.84588379]] -> [0.81391235]\n",
      "[[0.87769482 0.86913965 0.87895692 0.1076275  0.88018695]\n",
      " [0.86755359 0.85943939 0.88329992 0.08615219 0.87056929]\n",
      " [0.8632991  0.8577629  0.87948527 0.05516716 0.86663467]\n",
      " [0.86265791 0.87256178 0.88059139 0.07001882 0.87141442]\n",
      " [0.87145858 0.86206958 0.86751296 0.10081563 0.85247048]\n",
      " [0.84123926 0.83807922 0.85495374 0.06581518 0.84588379]\n",
      " [0.83954902 0.82897454 0.82998215 0.15705835 0.81391235]] -> [0.85564731]\n",
      "[[0.86755359 0.85943939 0.88329992 0.08615219 0.87056929]\n",
      " [0.8632991  0.8577629  0.87948527 0.05516716 0.86663467]\n",
      " [0.86265791 0.87256178 0.88059139 0.07001882 0.87141442]\n",
      " [0.87145858 0.86206958 0.86751296 0.10081563 0.85247048]\n",
      " [0.84123926 0.83807922 0.85495374 0.06581518 0.84588379]\n",
      " [0.83954902 0.82897454 0.82998215 0.15705835 0.81391235]\n",
      " [0.82806742 0.84877377 0.84579849 0.1465627  0.85564731]] -> [0.85786232]\n",
      "[[0.8632991  0.8577629  0.87948527 0.05516716 0.86663467]\n",
      " [0.86265791 0.87256178 0.88059139 0.07001882 0.87141442]\n",
      " [0.87145858 0.86206958 0.86751296 0.10081563 0.85247048]\n",
      " [0.84123926 0.83807922 0.85495374 0.06581518 0.84588379]\n",
      " [0.83954902 0.82897454 0.82998215 0.15705835 0.81391235]\n",
      " [0.82806742 0.84877377 0.84579849 0.1465627  0.85564731]\n",
      " [0.85589714 0.85371644 0.86739547 0.09478354 0.85786232]] -> [0.87861321]\n",
      "[[0.86265791 0.87256178 0.88059139 0.07001882 0.87141442]\n",
      " [0.87145858 0.86206958 0.86751296 0.10081563 0.85247048]\n",
      " [0.84123926 0.83807922 0.85495374 0.06581518 0.84588379]\n",
      " [0.83954902 0.82897454 0.82998215 0.15705835 0.81391235]\n",
      " [0.82806742 0.84877377 0.84579849 0.1465627  0.85564731]\n",
      " [0.85589714 0.85371644 0.86739547 0.09478354 0.85786232]\n",
      " [0.84925306 0.86279219 0.87285355 0.117191   0.87861321]] -> [0.91396551]\n",
      "[[0.87145858 0.86206958 0.86751296 0.10081563 0.85247048]\n",
      " [0.84123926 0.83807922 0.85495374 0.06581518 0.84588379]\n",
      " [0.83954902 0.82897454 0.82998215 0.15705835 0.81391235]\n",
      " [0.82806742 0.84877377 0.84579849 0.1465627  0.85564731]\n",
      " [0.85589714 0.85371644 0.86739547 0.09478354 0.85786232]\n",
      " [0.84925306 0.86279219 0.87285355 0.117191   0.87861321]\n",
      " [0.87600459 0.9015816  0.893934   0.14453706 0.91396551]] -> [0.91542274]\n",
      "[[0.84123926 0.83807922 0.85495374 0.06581518 0.84588379]\n",
      " [0.83954902 0.82897454 0.82998215 0.15705835 0.81391235]\n",
      " [0.82806742 0.84877377 0.84579849 0.1465627  0.85564731]\n",
      " [0.85589714 0.85371644 0.86739547 0.09478354 0.85786232]\n",
      " [0.84925306 0.86279219 0.87285355 0.117191   0.87861321]\n",
      " [0.87600459 0.9015816  0.893934   0.14453706 0.91396551]\n",
      " [0.90846783 0.90755313 0.9251148  0.11333692 0.91542274]] -> [0.91000172]\n",
      "[[0.83954902 0.82897454 0.82998215 0.15705835 0.81391235]\n",
      " [0.82806742 0.84877377 0.84579849 0.1465627  0.85564731]\n",
      " [0.85589714 0.85371644 0.86739547 0.09478354 0.85786232]\n",
      " [0.84925306 0.86279219 0.87285355 0.117191   0.87861321]\n",
      " [0.87600459 0.9015816  0.893934   0.14453706 0.91396551]\n",
      " [0.90846783 0.90755313 0.9251148  0.11333692 0.91542274]\n",
      " [0.91272232 0.90513675 0.92711016 0.10454423 0.91000172]] -> [0.9190948]\n",
      "[[0.82806742 0.84877377 0.84579849 0.1465627  0.85564731]\n",
      " [0.85589714 0.85371644 0.86739547 0.09478354 0.85786232]\n",
      " [0.84925306 0.86279219 0.87285355 0.117191   0.87861321]\n",
      " [0.87600459 0.9015816  0.893934   0.14453706 0.91396551]\n",
      " [0.90846783 0.90755313 0.9251148  0.11333692 0.91542274]\n",
      " [0.91272232 0.90513675 0.92711016 0.10454423 0.91000172]\n",
      " [0.90438801 0.90230421 0.92083055 0.09411132 0.9190948 ]] -> [0.91457743]\n",
      "[[0.85589714 0.85371644 0.86739547 0.09478354 0.85786232]\n",
      " [0.84925306 0.86279219 0.87285355 0.117191   0.87861321]\n",
      " [0.87600459 0.9015816  0.893934   0.14453706 0.91396551]\n",
      " [0.90846783 0.90755313 0.9251148  0.11333692 0.91542274]\n",
      " [0.91272232 0.90513675 0.92711016 0.10454423 0.91000172]\n",
      " [0.90438801 0.90230421 0.92083055 0.09411132 0.9190948 ]\n",
      " [0.91062425 0.90010747 0.91437488 0.12038182 0.91457743]] -> [0.91900746]\n",
      "[[0.84925306 0.86279219 0.87285355 0.117191   0.87861321]\n",
      " [0.87600459 0.9015816  0.893934   0.14453706 0.91396551]\n",
      " [0.90846783 0.90755313 0.9251148  0.11333692 0.91542274]\n",
      " [0.91272232 0.90513675 0.92711016 0.10454423 0.91000172]\n",
      " [0.90438801 0.90230421 0.92083055 0.09411132 0.9190948 ]\n",
      " [0.91062425 0.90010747 0.91437488 0.12038182 0.91457743]\n",
      " [0.91161494 0.91118925 0.93644146 0.0969974  0.91900746]] -> [0.90947714]\n",
      "[[0.87600459 0.9015816  0.893934   0.14453706 0.91396551]\n",
      " [0.90846783 0.90755313 0.9251148  0.11333692 0.91542274]\n",
      " [0.91272232 0.90513675 0.92711016 0.10454423 0.91000172]\n",
      " [0.90438801 0.90230421 0.92083055 0.09411132 0.9190948 ]\n",
      " [0.91062425 0.90010747 0.91437488 0.12038182 0.91457743]\n",
      " [0.91161494 0.91118925 0.93644146 0.0969974  0.91900746]\n",
      " [0.91044941 0.89938487 0.91789617 0.12081205 0.90947714]] -> [0.91373231]\n",
      "[[0.90846783 0.90755313 0.9251148  0.11333692 0.91542274]\n",
      " [0.91272232 0.90513675 0.92711016 0.10454423 0.91000172]\n",
      " [0.90438801 0.90230421 0.92083055 0.09411132 0.9190948 ]\n",
      " [0.91062425 0.90010747 0.91437488 0.12038182 0.91457743]\n",
      " [0.91161494 0.91118925 0.93644146 0.0969974  0.91900746]\n",
      " [0.91044941 0.89938487 0.91789617 0.12081205 0.90947714]\n",
      " [0.90674843 0.89668234 0.91971547 0.11521018 0.91373231]] -> [0.90238046]\n",
      "[[0.91272232 0.90513675 0.92711016 0.10454423 0.91000172]\n",
      " [0.90438801 0.90230421 0.92083055 0.09411132 0.9190948 ]\n",
      " [0.91062425 0.90010747 0.91437488 0.12038182 0.91457743]\n",
      " [0.91161494 0.91118925 0.93644146 0.0969974  0.91900746]\n",
      " [0.91044941 0.89938487 0.91789617 0.12081205 0.90947714]\n",
      " [0.90674843 0.89668234 0.91971547 0.11521018 0.91373231]\n",
      " [0.90473768 0.90614832 0.92209231 0.08103433 0.90238046]] -> [0.91067217]\n",
      "[[0.90438801 0.90230421 0.92083055 0.09411132 0.9190948 ]\n",
      " [0.91062425 0.90010747 0.91437488 0.12038182 0.91457743]\n",
      " [0.91161494 0.91118925 0.93644146 0.0969974  0.91900746]\n",
      " [0.91044941 0.89938487 0.91789617 0.12081205 0.90947714]\n",
      " [0.90674843 0.89668234 0.91971547 0.11521018 0.91373231]\n",
      " [0.90473768 0.90614832 0.92209231 0.08103433 0.90238046]\n",
      " [0.90995388 0.89871995 0.92176957 0.14673299 0.91067217]] -> [0.95231962]\n",
      "[[0.91062425 0.90010747 0.91437488 0.12038182 0.91457743]\n",
      " [0.91161494 0.91118925 0.93644146 0.0969974  0.91900746]\n",
      " [0.91044941 0.89938487 0.91789617 0.12081205 0.90947714]\n",
      " [0.90674843 0.89668234 0.91971547 0.11521018 0.91373231]\n",
      " [0.90473768 0.90614832 0.92209231 0.08103433 0.90238046]\n",
      " [0.90995388 0.89871995 0.92176957 0.14673299 0.91067217]\n",
      " [0.91094476 0.93907013 0.92778501 0.16973201 0.95231962]] -> [0.96560951]\n",
      "[[0.91161494 0.91118925 0.93644146 0.0969974  0.91900746]\n",
      " [0.91044941 0.89938487 0.91789617 0.12081205 0.90947714]\n",
      " [0.90674843 0.89668234 0.91971547 0.11521018 0.91373231]\n",
      " [0.90473768 0.90614832 0.92209231 0.08103433 0.90238046]\n",
      " [0.90995388 0.89871995 0.92176957 0.14673299 0.91067217]\n",
      " [0.91094476 0.93907013 0.92778501 0.16973201 0.95231962]\n",
      " [0.9548021  0.95360897 0.96910396 0.1302411  0.96560951]] -> [1.]\n",
      "[[0.91044941 0.89938487 0.91789617 0.12081205 0.90947714]\n",
      " [0.90674843 0.89668234 0.91971547 0.11521018 0.91373231]\n",
      " [0.90473768 0.90614832 0.92209231 0.08103433 0.90238046]\n",
      " [0.90995388 0.89871995 0.92176957 0.14673299 0.91067217]\n",
      " [0.91094476 0.93907013 0.92778501 0.16973201 0.95231962]\n",
      " [0.9548021  0.95360897 0.96910396 0.1302411  0.96560951]\n",
      " [0.97613342 0.98213729 0.99034596 0.1332437  1.        ]] -> [0.98974125]\n",
      "[[0.90674843 0.89668234 0.91971547 0.11521018 0.91373231]\n",
      " [0.90473768 0.90614832 0.92209231 0.08103433 0.90238046]\n",
      " [0.90995388 0.89871995 0.92176957 0.14673299 0.91067217]\n",
      " [0.91094476 0.93907013 0.92778501 0.16973201 0.95231962]\n",
      " [0.9548021  0.95360897 0.96910396 0.1302411  0.96560951]\n",
      " [0.97613342 0.98213729 0.99034596 0.1332437  1.        ]\n",
      " [1.         0.98858285 0.99606803 0.24437573 0.98974125]] -> [0.96397742]\n",
      "[[0.90473768 0.90614832 0.92209231 0.08103433 0.90238046]\n",
      " [0.90995388 0.89871995 0.92176957 0.14673299 0.91067217]\n",
      " [0.91094476 0.93907013 0.92778501 0.16973201 0.95231962]\n",
      " [0.9548021  0.95360897 0.96910396 0.1302411  0.96560951]\n",
      " [0.97613342 0.98213729 0.99034596 0.1332437  1.        ]\n",
      " [1.         0.98858285 0.99606803 0.24437573 0.98974125]\n",
      " [0.99096632 1.         0.97678915 0.26386125 0.96397742]] -> [0.90280311]\n",
      "[[0.90995388 0.89871995 0.92176957 0.14673299 0.91067217]\n",
      " [0.91094476 0.93907013 0.92778501 0.16973201 0.95231962]\n",
      " [0.9548021  0.95360897 0.96910396 0.1302411  0.96560951]\n",
      " [0.97613342 0.98213729 0.99034596 0.1332437  1.        ]\n",
      " [1.         0.98858285 0.99606803 0.24437573 0.98974125]\n",
      " [0.99096632 1.         0.97678915 0.26386125 0.96397742]\n",
      " [0.93253824 0.92453146 0.91622355 0.28869768 0.90280311]] -> [0.88668611]\n",
      "[[0.91094476 0.93907013 0.92778501 0.16973201 0.95231962]\n",
      " [0.9548021  0.95360897 0.96910396 0.1302411  0.96560951]\n",
      " [0.97613342 0.98213729 0.99034596 0.1332437  1.        ]\n",
      " [1.         0.98858285 0.99606803 0.24437573 0.98974125]\n",
      " [0.99096632 1.         0.97678915 0.26386125 0.96397742]\n",
      " [0.93253824 0.92453146 0.91622355 0.28869768 0.90280311]\n",
      " [0.88066709 0.88236031 0.88899262 0.19141346 0.88668611]] -> [0.88349487]\n",
      "[[0.9548021  0.95360897 0.96910396 0.1302411  0.96560951]\n",
      " [0.97613342 0.98213729 0.99034596 0.1332437  1.        ]\n",
      " [1.         0.98858285 0.99606803 0.24437573 0.98974125]\n",
      " [0.99096632 1.         0.97678915 0.26386125 0.96397742]\n",
      " [0.93253824 0.92453146 0.91622355 0.28869768 0.90280311]\n",
      " [0.88066709 0.88236031 0.88899262 0.19141346 0.88668611]\n",
      " [0.88888491 0.88218689 0.8909586  0.18063996 0.88349487]] -> [0.89175741]\n",
      "[[0.97613342 0.98213729 0.99034596 0.1332437  1.        ]\n",
      " [1.         0.98858285 0.99606803 0.24437573 0.98974125]\n",
      " [0.99096632 1.         0.97678915 0.26386125 0.96397742]\n",
      " [0.93253824 0.92453146 0.91622355 0.28869768 0.90280311]\n",
      " [0.88066709 0.88236031 0.88899262 0.19141346 0.88668611]\n",
      " [0.88888491 0.88218689 0.8909586  0.18063996 0.88349487]\n",
      " [0.87174992 0.88655144 0.89333544 0.13604912 0.89175741]] -> [0.90038407]\n",
      "[[1.         0.98858285 0.99606803 0.24437573 0.98974125]\n",
      " [0.99096632 1.         0.97678915 0.26386125 0.96397742]\n",
      " [0.93253824 0.92453146 0.91622355 0.28869768 0.90280311]\n",
      " [0.88066709 0.88236031 0.88899262 0.19141346 0.88668611]\n",
      " [0.88888491 0.88218689 0.8909586  0.18063996 0.88349487]\n",
      " [0.87174992 0.88655144 0.89333544 0.13604912 0.89175741]\n",
      " [0.89853061 0.89608976 0.91789617 0.13018733 0.90038407]] -> [0.89994701]\n",
      "[[0.99096632 1.         0.97678915 0.26386125 0.96397742]\n",
      " [0.93253824 0.92453146 0.91622355 0.28869768 0.90280311]\n",
      " [0.88066709 0.88236031 0.88899262 0.19141346 0.88668611]\n",
      " [0.88888491 0.88218689 0.8909586  0.18063996 0.88349487]\n",
      " [0.87174992 0.88655144 0.89333544 0.13604912 0.89175741]\n",
      " [0.89853061 0.89608976 0.91789617 0.13018733 0.90038407]\n",
      " [0.88894325 0.88357424 0.90287217 0.10453527 0.89994701]] -> [0.91635521]\n",
      "[[0.93253824 0.92453146 0.91622355 0.28869768 0.90280311]\n",
      " [0.88066709 0.88236031 0.88899262 0.19141346 0.88668611]\n",
      " [0.88888491 0.88218689 0.8909586  0.18063996 0.88349487]\n",
      " [0.87174992 0.88655144 0.89333544 0.13604912 0.89175741]\n",
      " [0.89853061 0.89608976 0.91789617 0.13018733 0.90038407]\n",
      " [0.88894325 0.88357424 0.90287217 0.10453527 0.89994701]\n",
      " [0.90144472 0.9090966  0.92203374 0.11000269 0.91635521]] -> [0.92046468]\n",
      "[[0.88066709 0.88236031 0.88899262 0.19141346 0.88668611]\n",
      " [0.88888491 0.88218689 0.8909586  0.18063996 0.88349487]\n",
      " [0.87174992 0.88655144 0.89333544 0.13604912 0.89175741]\n",
      " [0.89853061 0.89608976 0.91789617 0.13018733 0.90038407]\n",
      " [0.88894325 0.88357424 0.90287217 0.10453527 0.89994701]\n",
      " [0.90144472 0.9090966  0.92203374 0.11000269 0.91635521]\n",
      " [0.91021623 0.91296982 0.92617114 0.10284127 0.92046468]] -> [0.92390372]\n",
      "[[0.88888491 0.88218689 0.8909586  0.18063996 0.88349487]\n",
      " [0.87174992 0.88655144 0.89333544 0.13604912 0.89175741]\n",
      " [0.89853061 0.89608976 0.91789617 0.13018733 0.90038407]\n",
      " [0.88894325 0.88357424 0.90287217 0.10453527 0.89994701]\n",
      " [0.90144472 0.9090966  0.92203374 0.11000269 0.91635521]\n",
      " [0.91021623 0.91296982 0.92617114 0.10284127 0.92046468]\n",
      " [0.91753068 0.90955899 0.93013248 0.08799857 0.92390372]] -> [0.93588207]\n",
      "[[0.87174992 0.88655144 0.89333544 0.13604912 0.89175741]\n",
      " [0.89853061 0.89608976 0.91789617 0.13018733 0.90038407]\n",
      " [0.88894325 0.88357424 0.90287217 0.10453527 0.89994701]\n",
      " [0.90144472 0.9090966  0.92203374 0.11000269 0.91635521]\n",
      " [0.91021623 0.91296982 0.92617114 0.10284127 0.92046468]\n",
      " [0.91753068 0.90955899 0.93013248 0.08799857 0.92390372]\n",
      " [0.92391259 0.92282604 0.94550876 0.10049296 0.93588207]] -> [0.95211558]\n",
      "[[0.89853061 0.89608976 0.91789617 0.13018733 0.90038407]\n",
      " [0.88894325 0.88357424 0.90287217 0.10453527 0.89994701]\n",
      " [0.90144472 0.9090966  0.92203374 0.11000269 0.91635521]\n",
      " [0.91021623 0.91296982 0.92617114 0.10284127 0.92046468]\n",
      " [0.91753068 0.90955899 0.93013248 0.08799857 0.92390372]\n",
      " [0.92391259 0.92282604 0.94550876 0.10049296 0.93588207]\n",
      " [0.93644323 0.93932734 0.96226394 0.10667742 0.95211558]] -> [0.95564213]\n",
      "[[0.88894325 0.88357424 0.90287217 0.10453527 0.89994701]\n",
      " [0.90144472 0.9090966  0.92203374 0.11000269 0.91635521]\n",
      " [0.91021623 0.91296982 0.92617114 0.10284127 0.92046468]\n",
      " [0.91753068 0.90955899 0.93013248 0.08799857 0.92390372]\n",
      " [0.92391259 0.92282604 0.94550876 0.10049296 0.93588207]\n",
      " [0.93644323 0.93932734 0.96226394 0.10667742 0.95211558]\n",
      " [0.94518557 0.94522671 0.96376051 0.09372591 0.95564213]] -> [0.9513578]\n",
      "[[0.90144472 0.9090966  0.92203374 0.11000269 0.91635521]\n",
      " [0.91021623 0.91296982 0.92617114 0.10284127 0.92046468]\n",
      " [0.91753068 0.90955899 0.93013248 0.08799857 0.92390372]\n",
      " [0.92391259 0.92282604 0.94550876 0.10049296 0.93588207]\n",
      " [0.93644323 0.93932734 0.96226394 0.10667742 0.95211558]\n",
      " [0.94518557 0.94522671 0.96376051 0.09372591 0.95564213]\n",
      " [0.9462346  0.94522671 0.97100833 0.11616922 0.9513578 ]] -> [0.96645463]\n",
      "[[0.91021623 0.91296982 0.92617114 0.10284127 0.92046468]\n",
      " [0.91753068 0.90955899 0.93013248 0.08799857 0.92390372]\n",
      " [0.92391259 0.92282604 0.94550876 0.10049296 0.93588207]\n",
      " [0.93644323 0.93932734 0.96226394 0.10667742 0.95211558]\n",
      " [0.94518557 0.94522671 0.96376051 0.09372591 0.95564213]\n",
      " [0.9462346  0.94522671 0.97100833 0.11616922 0.9513578 ]\n",
      " [0.94789567 0.94927335 0.97250489 0.11417048 0.96645463]] -> [0.97785024]\n",
      "[[0.91753068 0.90955899 0.93013248 0.08799857 0.92390372]\n",
      " [0.92391259 0.92282604 0.94550876 0.10049296 0.93588207]\n",
      " [0.93644323 0.93932734 0.96226394 0.10667742 0.95211558]\n",
      " [0.94518557 0.94522671 0.96376051 0.09372591 0.95564213]\n",
      " [0.9462346  0.94522671 0.97100833 0.11616922 0.9513578 ]\n",
      " [0.94789567 0.94927335 0.97250489 0.11417048 0.96645463]\n",
      " [0.95690035 0.95988111 0.9803545  0.14250246 0.97785024]] -> [0.98831302]\n"
     ]
    }
   ],
   "source": [
    "# 데이터 읽기\n",
    "# 하이퍼파라미터\n",
    "seq_length = 7\n",
    "data_dim = 5\n",
    "hidden_dim = 10\n",
    "output_dim = 1\n",
    "learning_rate = 0.01\n",
    "iterations = 500\n",
    "\n",
    "xy = np.loadtxt(r\"E:\\Develop\\python\\Machine-Learning\\DeepLearning codes\\tensorflow v.1.14\\data\\data-02-stock_daily.csv\", delimiter=',')\n",
    "xy = xy[::-1]                 # 거꾸로 뒤집기\n",
    "xy = MinMaxScaler(xy)       # 정규화\n",
    "x = xy\n",
    "y = xy[:, [-1]]\n",
    "\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(y) - sequence_length):\n",
    "    _x = x[i : i + sequence_length]\n",
    "    _y = y[i + sequence_length]\n",
    "    print(_x, \"->\", _y)\n",
    "    dataX.append(_x)\n",
    "    dataY.append(_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and test dataset\n",
    "train_size = int(len(dataY) * 0.7)\n",
    "test_size = len(dataY) - train_size\n",
    "trainX, testX = np.array(dataX[0:train_size]), np.array(dataX[train_size : len(dataX)])\n",
    "trainY, testY = np.array(dataY[0:train_size]), np.array(dataY[train_size : len(dataY)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM and Loss\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, sequence_length, data_dim])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple=True)\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "Y_pred = tf.contrib.layers.fully_connected(outputs[:, -1], output_dim, activation_fn=None)          # output중에 마지막 거만 사용\n",
    "\n",
    "loss = tf.reduce_sum(tf.square(Y_pred - Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 42.706734\n",
      "10 1.9401764\n",
      "20 1.2791682\n",
      "30 1.2114534\n",
      "40 1.0869759\n",
      "50 0.98251903\n",
      "60 0.9043742\n",
      "70 0.8553667\n",
      "80 0.8310687\n",
      "90 0.81220037\n",
      "100 0.7949725\n",
      "110 0.7806751\n",
      "120 0.7676607\n",
      "130 0.7555418\n",
      "140 0.7439848\n",
      "150 0.73276204\n",
      "160 0.7217408\n",
      "170 0.71083957\n",
      "180 0.7000065\n",
      "190 0.6892145\n",
      "200 0.67844915\n",
      "210 0.667705\n",
      "220 0.6569841\n",
      "230 0.6462967\n",
      "240 0.6356627\n",
      "250 0.6251142\n",
      "260 0.61469793\n",
      "270 0.60447425\n",
      "280 0.59451604\n",
      "290 0.58490014\n",
      "300 0.57569784\n",
      "310 0.5669592\n",
      "320 0.55869985\n",
      "330 0.5508932\n",
      "340 0.5434714\n",
      "350 0.5363381\n",
      "360 0.5293865\n",
      "370 0.5225167\n",
      "380 0.51564896\n",
      "390 0.5087295\n",
      "400 0.5017304\n",
      "410 0.4946475\n",
      "420 0.4875011\n",
      "430 0.4803363\n",
      "440 0.47322616\n",
      "450 0.46627173\n",
      "460 0.45959547\n",
      "470 0.45332614\n",
      "480 0.44757468\n",
      "490 0.44241244\n",
      "500 0.43785787\n",
      "510 0.43387735\n",
      "520 0.43039846\n",
      "530 0.4273336\n",
      "540 0.42459878\n",
      "550 0.42212725\n",
      "560 0.41987196\n",
      "570 0.41780227\n",
      "580 0.41589808\n",
      "590 0.41414458\n",
      "600 0.41252849\n",
      "610 0.4110374\n",
      "620 0.4096594\n",
      "630 0.4083823\n",
      "640 0.4071945\n",
      "650 0.4060854\n",
      "660 0.40504476\n",
      "670 0.40406388\n",
      "680 0.40313405\n",
      "690 0.40224817\n",
      "700 0.40139973\n",
      "710 0.40058303\n",
      "720 0.39979327\n",
      "730 0.3990262\n",
      "740 0.39827794\n",
      "750 0.39754567\n",
      "760 0.39682695\n",
      "770 0.39611933\n",
      "780 0.39542088\n",
      "790 0.39473012\n",
      "800 0.3940458\n",
      "810 0.39336675\n",
      "820 0.39269224\n",
      "830 0.3920209\n",
      "840 0.39135268\n",
      "850 0.39068672\n",
      "860 0.39002225\n",
      "870 0.38936418\n",
      "880 0.401916\n",
      "890 0.388264\n",
      "900 0.3892009\n",
      "910 0.3870185\n",
      "920 0.3866047\n",
      "930 0.38588274\n",
      "940 0.38522094\n",
      "950 0.3845781\n",
      "960 0.38396576\n",
      "970 0.38334823\n",
      "980 0.38272935\n",
      "990 0.38215378\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n",
       "<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 372.103125 248.518125\" width=\"372.103125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       " <defs>\r\n",
       "  <style type=\"text/css\">\r\n",
       "*{stroke-linecap:butt;stroke-linejoin:round;}\r\n",
       "  </style>\r\n",
       " </defs>\r\n",
       " <g id=\"figure_1\">\r\n",
       "  <g id=\"patch_1\">\r\n",
       "   <path d=\"M 0 248.518125 \r\n",
       "L 372.103125 248.518125 \r\n",
       "L 372.103125 0 \r\n",
       "L 0 0 \r\n",
       "z\r\n",
       "\" style=\"fill:none;\"/>\r\n",
       "  </g>\r\n",
       "  <g id=\"axes_1\">\r\n",
       "   <g id=\"patch_2\">\r\n",
       "    <path d=\"M 30.103125 224.64 \r\n",
       "L 364.903125 224.64 \r\n",
       "L 364.903125 7.2 \r\n",
       "L 30.103125 7.2 \r\n",
       "z\r\n",
       "\" style=\"fill:#ffffff;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"matplotlib.axis_1\">\r\n",
       "    <g id=\"xtick_1\">\r\n",
       "     <g id=\"line2d_1\">\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 0 0 \r\n",
       "L 0 3.5 \r\n",
       "\" id=\"md6a8fbfa66\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n",
       "      </defs>\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.321307\" xlink:href=\"#md6a8fbfa66\" y=\"224.64\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_1\">\r\n",
       "      <!-- 0 -->\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 31.78125 66.40625 \r\n",
       "Q 24.171875 66.40625 20.328125 58.90625 \r\n",
       "Q 16.5 51.421875 16.5 36.375 \r\n",
       "Q 16.5 21.390625 20.328125 13.890625 \r\n",
       "Q 24.171875 6.390625 31.78125 6.390625 \r\n",
       "Q 39.453125 6.390625 43.28125 13.890625 \r\n",
       "Q 47.125 21.390625 47.125 36.375 \r\n",
       "Q 47.125 51.421875 43.28125 58.90625 \r\n",
       "Q 39.453125 66.40625 31.78125 66.40625 \r\n",
       "z\r\n",
       "M 31.78125 74.21875 \r\n",
       "Q 44.046875 74.21875 50.515625 64.515625 \r\n",
       "Q 56.984375 54.828125 56.984375 36.375 \r\n",
       "Q 56.984375 17.96875 50.515625 8.265625 \r\n",
       "Q 44.046875 -1.421875 31.78125 -1.421875 \r\n",
       "Q 19.53125 -1.421875 13.0625 8.265625 \r\n",
       "Q 6.59375 17.96875 6.59375 36.375 \r\n",
       "Q 6.59375 54.828125 13.0625 64.515625 \r\n",
       "Q 19.53125 74.21875 31.78125 74.21875 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-48\"/>\r\n",
       "      </defs>\r\n",
       "      <g transform=\"translate(42.140057 239.238437)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"xtick_2\">\r\n",
       "     <g id=\"line2d_2\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"115.451177\" xlink:href=\"#md6a8fbfa66\" y=\"224.64\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_2\">\r\n",
       "      <!-- 50 -->\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 10.796875 72.90625 \r\n",
       "L 49.515625 72.90625 \r\n",
       "L 49.515625 64.59375 \r\n",
       "L 19.828125 64.59375 \r\n",
       "L 19.828125 46.734375 \r\n",
       "Q 21.96875 47.46875 24.109375 47.828125 \r\n",
       "Q 26.265625 48.1875 28.421875 48.1875 \r\n",
       "Q 40.625 48.1875 47.75 41.5 \r\n",
       "Q 54.890625 34.8125 54.890625 23.390625 \r\n",
       "Q 54.890625 11.625 47.5625 5.09375 \r\n",
       "Q 40.234375 -1.421875 26.90625 -1.421875 \r\n",
       "Q 22.3125 -1.421875 17.546875 -0.640625 \r\n",
       "Q 12.796875 0.140625 7.71875 1.703125 \r\n",
       "L 7.71875 11.625 \r\n",
       "Q 12.109375 9.234375 16.796875 8.0625 \r\n",
       "Q 21.484375 6.890625 26.703125 6.890625 \r\n",
       "Q 35.15625 6.890625 40.078125 11.328125 \r\n",
       "Q 45.015625 15.765625 45.015625 23.390625 \r\n",
       "Q 45.015625 31 40.078125 35.4375 \r\n",
       "Q 35.15625 39.890625 26.703125 39.890625 \r\n",
       "Q 22.75 39.890625 18.8125 39.015625 \r\n",
       "Q 14.890625 38.140625 10.796875 36.28125 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-53\"/>\r\n",
       "      </defs>\r\n",
       "      <g transform=\"translate(109.088677 239.238437)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-53\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"xtick_3\">\r\n",
       "     <g id=\"line2d_3\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"185.581047\" xlink:href=\"#md6a8fbfa66\" y=\"224.64\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_3\">\r\n",
       "      <!-- 100 -->\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 12.40625 8.296875 \r\n",
       "L 28.515625 8.296875 \r\n",
       "L 28.515625 63.921875 \r\n",
       "L 10.984375 60.40625 \r\n",
       "L 10.984375 69.390625 \r\n",
       "L 28.421875 72.90625 \r\n",
       "L 38.28125 72.90625 \r\n",
       "L 38.28125 8.296875 \r\n",
       "L 54.390625 8.296875 \r\n",
       "L 54.390625 0 \r\n",
       "L 12.40625 0 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-49\"/>\r\n",
       "      </defs>\r\n",
       "      <g transform=\"translate(176.037297 239.238437)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"xtick_4\">\r\n",
       "     <g id=\"line2d_4\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"255.710917\" xlink:href=\"#md6a8fbfa66\" y=\"224.64\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_4\">\r\n",
       "      <!-- 150 -->\r\n",
       "      <g transform=\"translate(246.167167 239.238437)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"xtick_5\">\r\n",
       "     <g id=\"line2d_5\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"325.840787\" xlink:href=\"#md6a8fbfa66\" y=\"224.64\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_5\">\r\n",
       "      <!-- 200 -->\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 19.1875 8.296875 \r\n",
       "L 53.609375 8.296875 \r\n",
       "L 53.609375 0 \r\n",
       "L 7.328125 0 \r\n",
       "L 7.328125 8.296875 \r\n",
       "Q 12.9375 14.109375 22.625 23.890625 \r\n",
       "Q 32.328125 33.6875 34.8125 36.53125 \r\n",
       "Q 39.546875 41.84375 41.421875 45.53125 \r\n",
       "Q 43.3125 49.21875 43.3125 52.78125 \r\n",
       "Q 43.3125 58.59375 39.234375 62.25 \r\n",
       "Q 35.15625 65.921875 28.609375 65.921875 \r\n",
       "Q 23.96875 65.921875 18.8125 64.3125 \r\n",
       "Q 13.671875 62.703125 7.8125 59.421875 \r\n",
       "L 7.8125 69.390625 \r\n",
       "Q 13.765625 71.78125 18.9375 73 \r\n",
       "Q 24.125 74.21875 28.421875 74.21875 \r\n",
       "Q 39.75 74.21875 46.484375 68.546875 \r\n",
       "Q 53.21875 62.890625 53.21875 53.421875 \r\n",
       "Q 53.21875 48.921875 51.53125 44.890625 \r\n",
       "Q 49.859375 40.875 45.40625 35.40625 \r\n",
       "Q 44.1875 33.984375 37.640625 27.21875 \r\n",
       "Q 31.109375 20.453125 19.1875 8.296875 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-50\"/>\r\n",
       "      </defs>\r\n",
       "      <g transform=\"translate(316.297037 239.238437)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "   </g>\r\n",
       "   <g id=\"matplotlib.axis_2\">\r\n",
       "    <g id=\"ytick_1\">\r\n",
       "     <g id=\"line2d_6\">\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 0 0 \r\n",
       "L -3.5 0 \r\n",
       "\" id=\"m979fc2dcf1\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n",
       "      </defs>\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m979fc2dcf1\" y=\"219.65526\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_6\">\r\n",
       "      <!-- 0.5 -->\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 10.6875 12.40625 \r\n",
       "L 21 12.40625 \r\n",
       "L 21 0 \r\n",
       "L 10.6875 0 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-46\"/>\r\n",
       "      </defs>\r\n",
       "      <g transform=\"translate(7.2 223.454479)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"ytick_2\">\r\n",
       "     <g id=\"line2d_7\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m979fc2dcf1\" y=\"179.140936\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_7\">\r\n",
       "      <!-- 0.6 -->\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 33.015625 40.375 \r\n",
       "Q 26.375 40.375 22.484375 35.828125 \r\n",
       "Q 18.609375 31.296875 18.609375 23.390625 \r\n",
       "Q 18.609375 15.53125 22.484375 10.953125 \r\n",
       "Q 26.375 6.390625 33.015625 6.390625 \r\n",
       "Q 39.65625 6.390625 43.53125 10.953125 \r\n",
       "Q 47.40625 15.53125 47.40625 23.390625 \r\n",
       "Q 47.40625 31.296875 43.53125 35.828125 \r\n",
       "Q 39.65625 40.375 33.015625 40.375 \r\n",
       "z\r\n",
       "M 52.59375 71.296875 \r\n",
       "L 52.59375 62.3125 \r\n",
       "Q 48.875 64.0625 45.09375 64.984375 \r\n",
       "Q 41.3125 65.921875 37.59375 65.921875 \r\n",
       "Q 27.828125 65.921875 22.671875 59.328125 \r\n",
       "Q 17.53125 52.734375 16.796875 39.40625 \r\n",
       "Q 19.671875 43.65625 24.015625 45.921875 \r\n",
       "Q 28.375 48.1875 33.59375 48.1875 \r\n",
       "Q 44.578125 48.1875 50.953125 41.515625 \r\n",
       "Q 57.328125 34.859375 57.328125 23.390625 \r\n",
       "Q 57.328125 12.15625 50.6875 5.359375 \r\n",
       "Q 44.046875 -1.421875 33.015625 -1.421875 \r\n",
       "Q 20.359375 -1.421875 13.671875 8.265625 \r\n",
       "Q 6.984375 17.96875 6.984375 36.375 \r\n",
       "Q 6.984375 53.65625 15.1875 63.9375 \r\n",
       "Q 23.390625 74.21875 37.203125 74.21875 \r\n",
       "Q 40.921875 74.21875 44.703125 73.484375 \r\n",
       "Q 48.484375 72.75 52.59375 71.296875 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-54\"/>\r\n",
       "      </defs>\r\n",
       "      <g transform=\"translate(7.2 182.940154)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"ytick_3\">\r\n",
       "     <g id=\"line2d_8\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m979fc2dcf1\" y=\"138.626611\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_8\">\r\n",
       "      <!-- 0.7 -->\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 8.203125 72.90625 \r\n",
       "L 55.078125 72.90625 \r\n",
       "L 55.078125 68.703125 \r\n",
       "L 28.609375 0 \r\n",
       "L 18.3125 0 \r\n",
       "L 43.21875 64.59375 \r\n",
       "L 8.203125 64.59375 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-55\"/>\r\n",
       "      </defs>\r\n",
       "      <g transform=\"translate(7.2 142.425829)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"ytick_4\">\r\n",
       "     <g id=\"line2d_9\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m979fc2dcf1\" y=\"98.112286\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_9\">\r\n",
       "      <!-- 0.8 -->\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 31.78125 34.625 \r\n",
       "Q 24.75 34.625 20.71875 30.859375 \r\n",
       "Q 16.703125 27.09375 16.703125 20.515625 \r\n",
       "Q 16.703125 13.921875 20.71875 10.15625 \r\n",
       "Q 24.75 6.390625 31.78125 6.390625 \r\n",
       "Q 38.8125 6.390625 42.859375 10.171875 \r\n",
       "Q 46.921875 13.96875 46.921875 20.515625 \r\n",
       "Q 46.921875 27.09375 42.890625 30.859375 \r\n",
       "Q 38.875 34.625 31.78125 34.625 \r\n",
       "z\r\n",
       "M 21.921875 38.8125 \r\n",
       "Q 15.578125 40.375 12.03125 44.71875 \r\n",
       "Q 8.5 49.078125 8.5 55.328125 \r\n",
       "Q 8.5 64.0625 14.71875 69.140625 \r\n",
       "Q 20.953125 74.21875 31.78125 74.21875 \r\n",
       "Q 42.671875 74.21875 48.875 69.140625 \r\n",
       "Q 55.078125 64.0625 55.078125 55.328125 \r\n",
       "Q 55.078125 49.078125 51.53125 44.71875 \r\n",
       "Q 48 40.375 41.703125 38.8125 \r\n",
       "Q 48.828125 37.15625 52.796875 32.3125 \r\n",
       "Q 56.78125 27.484375 56.78125 20.515625 \r\n",
       "Q 56.78125 9.90625 50.3125 4.234375 \r\n",
       "Q 43.84375 -1.421875 31.78125 -1.421875 \r\n",
       "Q 19.734375 -1.421875 13.25 4.234375 \r\n",
       "Q 6.78125 9.90625 6.78125 20.515625 \r\n",
       "Q 6.78125 27.484375 10.78125 32.3125 \r\n",
       "Q 14.796875 37.15625 21.921875 38.8125 \r\n",
       "z\r\n",
       "M 18.3125 54.390625 \r\n",
       "Q 18.3125 48.734375 21.84375 45.5625 \r\n",
       "Q 25.390625 42.390625 31.78125 42.390625 \r\n",
       "Q 38.140625 42.390625 41.71875 45.5625 \r\n",
       "Q 45.3125 48.734375 45.3125 54.390625 \r\n",
       "Q 45.3125 60.0625 41.71875 63.234375 \r\n",
       "Q 38.140625 66.40625 31.78125 66.40625 \r\n",
       "Q 25.390625 66.40625 21.84375 63.234375 \r\n",
       "Q 18.3125 60.0625 18.3125 54.390625 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-56\"/>\r\n",
       "      </defs>\r\n",
       "      <g transform=\"translate(7.2 101.911505)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"ytick_5\">\r\n",
       "     <g id=\"line2d_10\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m979fc2dcf1\" y=\"57.597961\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_10\">\r\n",
       "      <!-- 0.9 -->\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 10.984375 1.515625 \r\n",
       "L 10.984375 10.5 \r\n",
       "Q 14.703125 8.734375 18.5 7.8125 \r\n",
       "Q 22.3125 6.890625 25.984375 6.890625 \r\n",
       "Q 35.75 6.890625 40.890625 13.453125 \r\n",
       "Q 46.046875 20.015625 46.78125 33.40625 \r\n",
       "Q 43.953125 29.203125 39.59375 26.953125 \r\n",
       "Q 35.25 24.703125 29.984375 24.703125 \r\n",
       "Q 19.046875 24.703125 12.671875 31.3125 \r\n",
       "Q 6.296875 37.9375 6.296875 49.421875 \r\n",
       "Q 6.296875 60.640625 12.9375 67.421875 \r\n",
       "Q 19.578125 74.21875 30.609375 74.21875 \r\n",
       "Q 43.265625 74.21875 49.921875 64.515625 \r\n",
       "Q 56.59375 54.828125 56.59375 36.375 \r\n",
       "Q 56.59375 19.140625 48.40625 8.859375 \r\n",
       "Q 40.234375 -1.421875 26.421875 -1.421875 \r\n",
       "Q 22.703125 -1.421875 18.890625 -0.6875 \r\n",
       "Q 15.09375 0.046875 10.984375 1.515625 \r\n",
       "z\r\n",
       "M 30.609375 32.421875 \r\n",
       "Q 37.25 32.421875 41.125 36.953125 \r\n",
       "Q 45.015625 41.5 45.015625 49.421875 \r\n",
       "Q 45.015625 57.28125 41.125 61.84375 \r\n",
       "Q 37.25 66.40625 30.609375 66.40625 \r\n",
       "Q 23.96875 66.40625 20.09375 61.84375 \r\n",
       "Q 16.21875 57.28125 16.21875 49.421875 \r\n",
       "Q 16.21875 41.5 20.09375 36.953125 \r\n",
       "Q 23.96875 32.421875 30.609375 32.421875 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-57\"/>\r\n",
       "      </defs>\r\n",
       "      <g transform=\"translate(7.2 61.39718)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-57\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"ytick_6\">\r\n",
       "     <g id=\"line2d_11\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m979fc2dcf1\" y=\"17.083636\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_11\">\r\n",
       "      <!-- 1.0 -->\r\n",
       "      <g transform=\"translate(7.2 20.882855)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "   </g>\r\n",
       "   <g id=\"line2d_12\">\r\n",
       "    <path clip-path=\"url(#pa28d7154bc)\" d=\"M 45.321307 126.399301 \r\n",
       "L 46.723904 116.209319 \r\n",
       "L 48.126502 114.46173 \r\n",
       "L 49.529099 107.613277 \r\n",
       "L 50.931696 98.627632 \r\n",
       "L 52.334294 113.59979 \r\n",
       "L 53.736891 115.087573 \r\n",
       "L 55.139489 107.447951 \r\n",
       "L 56.542086 155.115643 \r\n",
       "L 57.944683 149.943864 \r\n",
       "L 59.347281 167.667237 \r\n",
       "L 60.749878 170.382995 \r\n",
       "L 62.152476 187.882019 \r\n",
       "L 63.555073 185.532298 \r\n",
       "L 64.95767 179.392278 \r\n",
       "L 66.360268 186.299828 \r\n",
       "L 67.762865 182.356024 \r\n",
       "L 69.165463 175.590232 \r\n",
       "L 70.56806 164.148565 \r\n",
       "L 71.970657 162.04676 \r\n",
       "L 73.373255 149.908478 \r\n",
       "L 74.775852 159.224773 \r\n",
       "L 76.17845 161.562675 \r\n",
       "L 77.581047 164.490963 \r\n",
       "L 78.983644 157.807835 \r\n",
       "L 80.386242 169.92255 \r\n",
       "L 81.788839 169.450212 \r\n",
       "L 83.191437 176.900871 \r\n",
       "L 84.594034 165.778034 \r\n",
       "L 85.996631 172.272271 \r\n",
       "L 87.399229 153.557018 \r\n",
       "L 88.801826 147.440636 \r\n",
       "L 90.204424 148.798551 \r\n",
       "L 91.607021 138.714799 \r\n",
       "L 93.009619 135.101644 \r\n",
       "L 94.412216 136.955391 \r\n",
       "L 95.814813 141.383279 \r\n",
       "L 97.217411 150.900285 \r\n",
       "L 98.620008 157.736991 \r\n",
       "L 100.022606 157.618871 \r\n",
       "L 101.425203 143.886507 \r\n",
       "L 102.8278 143.53229 \r\n",
       "L 104.230398 154.360005 \r\n",
       "L 105.632995 155.5998 \r\n",
       "L 107.035593 155.706028 \r\n",
       "L 108.43819 154.938571 \r\n",
       "L 109.840787 165.045963 \r\n",
       "L 111.243385 187.055538 \r\n",
       "L 112.645982 184.705745 \r\n",
       "L 114.04858 182.072651 \r\n",
       "L 115.451177 180.277856 \r\n",
       "L 116.853774 175.070691 \r\n",
       "L 118.256372 206.538248 \r\n",
       "L 119.658969 214.756364 \r\n",
       "L 121.061567 200.846927 \r\n",
       "L 122.464164 196.041184 \r\n",
       "L 123.866761 186.606839 \r\n",
       "L 125.269359 178.211508 \r\n",
       "L 126.671956 183.241601 \r\n",
       "L 128.074554 179.91182 \r\n",
       "L 129.477151 182.757518 \r\n",
       "L 132.282346 159.46087 \r\n",
       "L 133.684943 152.907608 \r\n",
       "L 135.087541 157.229269 \r\n",
       "L 136.490138 152.541573 \r\n",
       "L 137.892735 153.840463 \r\n",
       "L 139.295333 137.39227 \r\n",
       "L 140.69793 133.637429 \r\n",
       "L 142.100528 128.642794 \r\n",
       "L 143.503125 131.665562 \r\n",
       "L 144.905722 126.812614 \r\n",
       "L 146.30832 130.319466 \r\n",
       "L 147.710917 131.91355 \r\n",
       "L 149.113515 127.957926 \r\n",
       "L 150.516112 123.069592 \r\n",
       "L 151.918709 96.053561 \r\n",
       "L 153.321307 91.224179 \r\n",
       "L 154.723904 93.361371 \r\n",
       "L 156.126502 90.869962 \r\n",
       "L 157.529099 92.723781 \r\n",
       "L 158.931696 80.195824 \r\n",
       "L 160.334294 80.738932 \r\n",
       "L 161.736891 77.787007 \r\n",
       "L 163.139489 77.291103 \r\n",
       "L 164.542086 77.090392 \r\n",
       "L 165.944683 79.015054 \r\n",
       "L 167.347281 79.936018 \r\n",
       "L 168.749878 86.194085 \r\n",
       "L 170.152476 82.923401 \r\n",
       "L 171.555073 85.769026 \r\n",
       "L 172.95767 88.225048 \r\n",
       "L 174.360268 92.086119 \r\n",
       "L 175.762865 92.168781 \r\n",
       "L 177.165463 95.049863 \r\n",
       "L 178.56806 95.32149 \r\n",
       "L 179.970657 95.167983 \r\n",
       "L 181.373255 92.086119 \r\n",
       "L 182.775852 95.699272 \r\n",
       "L 184.17845 98.10809 \r\n",
       "L 185.581047 96.065309 \r\n",
       "L 186.983644 92.900853 \r\n",
       "L 188.386242 82.722618 \r\n",
       "L 189.788839 82.403859 \r\n",
       "L 191.191437 88.343097 \r\n",
       "L 192.594034 106.834 \r\n",
       "L 193.996631 95.781934 \r\n",
       "L 195.399229 106.798543 \r\n",
       "L 196.801826 103.4924 \r\n",
       "L 198.204424 92.546636 \r\n",
       "L 199.607021 95.94726 \r\n",
       "L 201.009619 99.702101 \r\n",
       "L 202.412216 92.959949 \r\n",
       "L 203.814813 87.280446 \r\n",
       "L 205.217411 74.30372 \r\n",
       "L 206.620008 74.669756 \r\n",
       "L 208.022606 89.653734 \r\n",
       "L 209.425203 79.262969 \r\n",
       "L 210.8278 80.975101 \r\n",
       "L 212.230398 88.709132 \r\n",
       "L 213.632995 86.017014 \r\n",
       "L 215.035593 91.602034 \r\n",
       "L 216.43819 87.032458 \r\n",
       "L 217.840787 86.985253 \r\n",
       "L 219.243385 86.524736 \r\n",
       "L 220.645982 88.62647 \r\n",
       "L 222.04858 75.803322 \r\n",
       "L 223.451177 79.192127 \r\n",
       "L 224.853774 75.567152 \r\n",
       "L 226.256372 84.954292 \r\n",
       "L 227.658969 84.552798 \r\n",
       "L 229.061567 82.864305 \r\n",
       "L 230.464164 64.798533 \r\n",
       "L 231.866761 57.430538 \r\n",
       "L 233.269359 62.779462 \r\n",
       "L 234.671956 59.945585 \r\n",
       "L 236.074554 43.721812 \r\n",
       "L 237.477151 50.145205 \r\n",
       "L 238.879748 60.299802 \r\n",
       "L 240.282346 64.692304 \r\n",
       "L 241.684943 64.668666 \r\n",
       "L 243.087541 77.456429 \r\n",
       "L 244.490138 78.554537 \r\n",
       "L 245.892735 96.15979 \r\n",
       "L 247.295333 103.91746 \r\n",
       "L 248.69793 104.047327 \r\n",
       "L 250.100528 79.841535 \r\n",
       "L 251.503125 70.407192 \r\n",
       "L 252.905722 76.547212 \r\n",
       "L 254.30832 103.409737 \r\n",
       "L 255.710917 113.493489 \r\n",
       "L 257.113515 134.676512 \r\n",
       "L 258.516112 108.215481 \r\n",
       "L 259.918709 101.142679 \r\n",
       "L 261.321307 93.172479 \r\n",
       "L 262.723904 105.794916 \r\n",
       "L 264.126502 95.569405 \r\n",
       "L 265.529099 96.667512 \r\n",
       "L 266.931696 105.263556 \r\n",
       "L 268.334294 104.44882 \r\n",
       "L 269.736891 96.702971 \r\n",
       "L 271.139489 93.632924 \r\n",
       "L 272.542086 108.746842 \r\n",
       "L 273.944683 120.696232 \r\n",
       "L 275.347281 117.649824 \r\n",
       "L 276.749878 103.456942 \r\n",
       "L 278.152476 107.48341 \r\n",
       "L 279.555073 93.219684 \r\n",
       "L 280.95767 87.044278 \r\n",
       "L 282.360268 71.84777 \r\n",
       "L 283.762865 71.871335 \r\n",
       "L 285.165463 63.806726 \r\n",
       "L 286.56806 62.661342 \r\n",
       "L 287.970657 61.740378 \r\n",
       "L 289.373255 70.064795 \r\n",
       "L 290.775852 66.050147 \r\n",
       "L 292.17845 63.428871 \r\n",
       "L 293.581047 65.625086 \r\n",
       "L 294.983644 69.521614 \r\n",
       "L 296.386242 71.115698 \r\n",
       "L 297.788839 69.179217 \r\n",
       "L 299.191437 76.854224 \r\n",
       "L 300.594034 79.522777 \r\n",
       "L 301.996631 92.475793 \r\n",
       "L 303.399229 75.567152 \r\n",
       "L 304.801826 74.669756 \r\n",
       "L 306.204424 66.262676 \r\n",
       "L 307.607021 51.939928 \r\n",
       "L 309.009619 51.349543 \r\n",
       "L 310.412216 53.54583 \r\n",
       "L 311.814813 49.861832 \r\n",
       "L 313.217411 51.692012 \r\n",
       "L 314.620008 49.897218 \r\n",
       "L 316.022606 53.75836 \r\n",
       "L 317.425203 52.034409 \r\n",
       "L 318.8278 56.633532 \r\n",
       "L 320.230398 53.274203 \r\n",
       "L 321.632995 36.401021 \r\n",
       "L 323.035593 31.016712 \r\n",
       "L 324.43819 17.083636 \r\n",
       "L 325.840787 21.239899 \r\n",
       "L 327.243385 31.67794 \r\n",
       "L 328.645982 56.462298 \r\n",
       "L 330.04858 62.991992 \r\n",
       "L 331.451177 64.284902 \r\n",
       "L 334.256372 57.442358 \r\n",
       "L 335.658969 57.619429 \r\n",
       "L 337.061567 50.971759 \r\n",
       "L 338.464164 49.306832 \r\n",
       "L 339.866761 47.913532 \r\n",
       "L 341.269359 43.060583 \r\n",
       "L 342.671956 36.483684 \r\n",
       "L 344.074554 35.054926 \r\n",
       "L 345.477151 36.790697 \r\n",
       "L 346.879748 30.674315 \r\n",
       "L 349.684943 21.818537 \r\n",
       "L 349.684943 21.818537 \r\n",
       "\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"line2d_13\">\r\n",
       "    <path clip-path=\"url(#pa28d7154bc)\" d=\"M 45.321307 125.326173 \r\n",
       "L 46.723904 121.137654 \r\n",
       "L 48.126502 109.613014 \r\n",
       "L 49.529099 110.364416 \r\n",
       "L 50.931696 104.357086 \r\n",
       "L 52.334294 99.614168 \r\n",
       "L 53.736891 112.699086 \r\n",
       "L 55.139489 113.487942 \r\n",
       "L 56.542086 103.347441 \r\n",
       "L 57.944683 153.309603 \r\n",
       "L 59.347281 151.273046 \r\n",
       "L 60.749878 152.873072 \r\n",
       "L 62.152476 166.71823 \r\n",
       "L 63.555073 177.666279 \r\n",
       "L 64.95767 174.7938 \r\n",
       "L 66.360268 169.959117 \r\n",
       "L 67.762865 175.452569 \r\n",
       "L 69.165463 173.249991 \r\n",
       "L 70.56806 166.337893 \r\n",
       "L 71.970657 157.718428 \r\n",
       "L 73.373255 154.377519 \r\n",
       "L 74.775852 144.035065 \r\n",
       "L 76.17845 153.819183 \r\n",
       "L 77.581047 155.72913 \r\n",
       "L 78.983644 153.523341 \r\n",
       "L 80.386242 149.484663 \r\n",
       "L 81.788839 158.493399 \r\n",
       "L 83.191437 161.174623 \r\n",
       "L 84.594034 165.454109 \r\n",
       "L 85.996631 154.22519 \r\n",
       "L 87.399229 162.181732 \r\n",
       "L 88.801826 145.090399 \r\n",
       "L 90.204424 141.155969 \r\n",
       "L 91.607021 143.656586 \r\n",
       "L 93.009619 133.491502 \r\n",
       "L 94.412216 130.242816 \r\n",
       "L 95.814813 134.469224 \r\n",
       "L 97.217411 136.122618 \r\n",
       "L 98.620008 142.871811 \r\n",
       "L 100.022606 147.208819 \r\n",
       "L 101.425203 145.731153 \r\n",
       "L 102.8278 133.431107 \r\n",
       "L 104.230398 137.270199 \r\n",
       "L 105.632995 145.600027 \r\n",
       "L 107.035593 144.770118 \r\n",
       "L 108.43819 145.97341 \r\n",
       "L 109.840787 143.905774 \r\n",
       "L 111.243385 156.582873 \r\n",
       "L 112.645982 176.297957 \r\n",
       "L 114.04858 172.803584 \r\n",
       "L 115.451177 167.842557 \r\n",
       "L 116.853774 169.432199 \r\n",
       "L 118.256372 168.062766 \r\n",
       "L 119.658969 196.033446 \r\n",
       "L 121.061567 209.315559 \r\n",
       "L 122.464164 184.822204 \r\n",
       "L 123.866761 185.560904 \r\n",
       "L 125.269359 182.062426 \r\n",
       "L 126.671956 172.188886 \r\n",
       "L 128.074554 178.963314 \r\n",
       "L 129.477151 173.337529 \r\n",
       "L 130.879748 175.435423 \r\n",
       "L 132.282346 161.212101 \r\n",
       "L 133.684943 151.80535 \r\n",
       "L 135.087541 145.390153 \r\n",
       "L 136.490138 151.294852 \r\n",
       "L 137.892735 144.615617 \r\n",
       "L 139.295333 144.09191 \r\n",
       "L 140.69793 131.31573 \r\n",
       "L 142.100528 127.734519 \r\n",
       "L 143.503125 122.291562 \r\n",
       "L 144.905722 126.920259 \r\n",
       "L 146.30832 121.239826 \r\n",
       "L 147.710917 124.195713 \r\n",
       "L 149.113515 124.89836 \r\n",
       "L 150.516112 120.153703 \r\n",
       "L 151.918709 116.887991 \r\n",
       "L 153.321307 96.536524 \r\n",
       "L 154.723904 102.238811 \r\n",
       "L 156.126502 98.988434 \r\n",
       "L 157.529099 95.272258 \r\n",
       "L 158.931696 94.231872 \r\n",
       "L 160.334294 82.636187 \r\n",
       "L 161.736891 85.050353 \r\n",
       "L 163.139489 78.825228 \r\n",
       "L 164.542086 79.631037 \r\n",
       "L 165.944683 77.115496 \r\n",
       "L 167.347281 80.540418 \r\n",
       "L 168.749878 78.570293 \r\n",
       "L 170.152476 86.073908 \r\n",
       "L 171.555073 82.305281 \r\n",
       "L 172.95767 82.5075 \r\n",
       "L 175.762865 89.254786 \r\n",
       "L 177.165463 87.914452 \r\n",
       "L 178.56806 91.634419 \r\n",
       "L 179.970657 93.242342 \r\n",
       "L 181.373255 90.36897 \r\n",
       "L 182.775852 90.294182 \r\n",
       "L 184.17845 91.7179 \r\n",
       "L 185.581047 95.071681 \r\n",
       "L 186.983644 93.739872 \r\n",
       "L 188.386242 89.135782 \r\n",
       "L 189.788839 82.582553 \r\n",
       "L 191.191437 84.224887 \r\n",
       "L 192.594034 87.400815 \r\n",
       "L 193.996631 104.123958 \r\n",
       "L 195.399229 96.497645 \r\n",
       "L 196.801826 101.29881 \r\n",
       "L 198.204424 98.722005 \r\n",
       "L 199.607021 90.474981 \r\n",
       "L 201.009619 94.727204 \r\n",
       "L 202.412216 100.078083 \r\n",
       "L 203.814813 89.870546 \r\n",
       "L 205.217411 86.975924 \r\n",
       "L 206.620008 75.995251 \r\n",
       "L 208.022606 78.358294 \r\n",
       "L 209.425203 92.069356 \r\n",
       "L 210.8278 80.967893 \r\n",
       "L 212.230398 81.710602 \r\n",
       "L 213.632995 87.185121 \r\n",
       "L 215.035593 85.520764 \r\n",
       "L 216.43819 91.639756 \r\n",
       "L 217.840787 85.144169 \r\n",
       "L 219.243385 84.826618 \r\n",
       "L 220.645982 87.004225 \r\n",
       "L 222.04858 88.520746 \r\n",
       "L 223.451177 75.8868 \r\n",
       "L 224.853774 79.594597 \r\n",
       "L 226.256372 78.589564 \r\n",
       "L 227.658969 85.942033 \r\n",
       "L 229.061567 84.070941 \r\n",
       "L 230.464164 80.048708 \r\n",
       "L 231.866761 66.247234 \r\n",
       "L 233.269359 64.794828 \r\n",
       "L 234.671956 70.272776 \r\n",
       "L 236.074554 68.457322 \r\n",
       "L 237.477151 50.912867 \r\n",
       "L 238.879748 60.844943 \r\n",
       "L 240.282346 69.780728 \r\n",
       "L 241.684943 70.485717 \r\n",
       "L 243.087541 71.732934 \r\n",
       "L 244.490138 90.561288 \r\n",
       "L 245.892735 84.183763 \r\n",
       "L 247.295333 99.67763 \r\n",
       "L 248.69793 102.558416 \r\n",
       "L 250.100528 102.321471 \r\n",
       "L 251.503125 79.370958 \r\n",
       "L 252.905722 75.313396 \r\n",
       "L 254.30832 84.625051 \r\n",
       "L 255.710917 109.782295 \r\n",
       "L 257.113515 120.735172 \r\n",
       "L 258.516112 128.683359 \r\n",
       "L 259.918709 106.42209 \r\n",
       "L 261.321307 102.957227 \r\n",
       "L 262.723904 94.517958 \r\n",
       "L 264.126502 105.680661 \r\n",
       "L 265.529099 97.163441 \r\n",
       "L 266.931696 93.272503 \r\n",
       "L 268.334294 106.174158 \r\n",
       "L 269.736891 100.414036 \r\n",
       "L 271.139489 89.466688 \r\n",
       "L 272.542086 94.333778 \r\n",
       "L 275.347281 121.434849 \r\n",
       "L 276.749878 114.522871 \r\n",
       "L 278.152476 97.055112 \r\n",
       "L 279.555073 104.483262 \r\n",
       "L 282.360268 89.18065 \r\n",
       "L 283.762865 75.625611 \r\n",
       "L 285.165463 78.141297 \r\n",
       "L 286.56806 70.510396 \r\n",
       "L 287.970657 71.51951 \r\n",
       "L 289.373255 69.879036 \r\n",
       "L 290.775852 75.424889 \r\n",
       "L 292.17845 75.672556 \r\n",
       "L 293.581047 66.38191 \r\n",
       "L 294.983644 70.99561 \r\n",
       "L 296.386242 73.494924 \r\n",
       "L 297.788839 72.756561 \r\n",
       "L 299.191437 69.205005 \r\n",
       "L 300.594034 77.355024 \r\n",
       "L 301.996631 80.874487 \r\n",
       "L 303.399229 88.195129 \r\n",
       "L 304.801826 76.81748 \r\n",
       "L 306.204424 76.098485 \r\n",
       "L 307.607021 69.915596 \r\n",
       "L 309.009619 59.886589 \r\n",
       "L 310.412216 60.308389 \r\n",
       "L 311.814813 61.453604 \r\n",
       "L 313.217411 58.803967 \r\n",
       "L 314.620008 59.996585 \r\n",
       "L 316.022606 56.433969 \r\n",
       "L 317.425203 61.02241 \r\n",
       "L 318.8278 59.904193 \r\n",
       "L 320.230398 61.570265 \r\n",
       "L 321.632995 58.486729 \r\n",
       "L 323.035593 49.018231 \r\n",
       "L 324.43819 44.677262 \r\n",
       "L 325.840787 34.35589 \r\n",
       "L 327.243385 38.731559 \r\n",
       "L 328.645982 50.811999 \r\n",
       "L 330.04858 72.581099 \r\n",
       "L 331.451177 75.376375 \r\n",
       "L 332.853774 69.511159 \r\n",
       "L 334.256372 68.250201 \r\n",
       "L 335.658969 63.081497 \r\n",
       "L 337.061567 66.778524 \r\n",
       "L 338.464164 57.700384 \r\n",
       "L 339.866761 57.73552 \r\n",
       "L 341.269359 56.875981 \r\n",
       "L 342.671956 50.742862 \r\n",
       "L 344.074554 45.936047 \r\n",
       "L 345.477151 46.608894 \r\n",
       "L 346.879748 46.300278 \r\n",
       "L 349.684943 39.463353 \r\n",
       "L 349.684943 39.463353 \r\n",
       "\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_3\">\r\n",
       "    <path d=\"M 30.103125 224.64 \r\n",
       "L 30.103125 7.2 \r\n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_4\">\r\n",
       "    <path d=\"M 364.903125 224.64 \r\n",
       "L 364.903125 7.2 \r\n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_5\">\r\n",
       "    <path d=\"M 30.103125 224.64 \r\n",
       "L 364.903125 224.64 \r\n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_6\">\r\n",
       "    <path d=\"M 30.103125 7.2 \r\n",
       "L 364.903125 7.2 \r\n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n",
       "   </g>\r\n",
       "  </g>\r\n",
       " </g>\r\n",
       " <defs>\r\n",
       "  <clipPath id=\"pa28d7154bc\">\r\n",
       "   <rect height=\"217.44\" width=\"334.8\" x=\"30.103125\" y=\"7.2\"/>\r\n",
       "  </clipPath>\r\n",
       " </defs>\r\n",
       "</svg>\r\n"
      ],
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training and Results\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(1000):\n",
    "    _, l = sess.run([train, loss], feed_dict={X: trainX, Y: trainY})\n",
    "    if i % 10 == 0:\n",
    "        print(i, l)\n",
    "        \n",
    "# 예측하기\n",
    "test_predict = sess.run(Y_pred, feed_dict={X: testX})\n",
    "\n",
    "plt.plot(testY)\n",
    "plt.plot(test_predict)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
