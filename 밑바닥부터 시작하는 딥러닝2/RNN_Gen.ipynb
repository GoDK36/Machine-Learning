{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597715332515",
   "display_name": "Python 3.7.7 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RnnlmGen (문장 생성)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.functions import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.time_layers import *\n",
    "import pickle\n",
    "\n",
    "class Rnnlm:\n",
    "    def __init__(self, vocab_size=10000, wordvec_size=100, hidden_size=100):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        # 가중치 초기화\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = [\n",
    "            TimeEmbedding(embed_W),\n",
    "            TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True),\n",
    "            TimeAffine(affine_W, affine_b)\n",
    "        ]\n",
    "        self.loss_layer = TimeSoftmaxWithLoss()\n",
    "        self.lstm_layer = self.layers[1]\n",
    "\n",
    "        # 모든 가중치와 기울기를 리스트에 모으기\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def predict(self, xs):\n",
    "        for layer in self.layers:\n",
    "            xs = layer.forward(xs)\n",
    "        return xs\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        score = self.predict(xs)\n",
    "        loss = self.loss_layer.forward(score, ts)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.lstm_layer.reset_state()\n",
    "\n",
    "    def save_params(self, file_name=\"Rnnlm.pkl\"):\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(self.params, f)\n",
    "    \n",
    "    def load_params(self, file_name=\"Rnnlm.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            self.params = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.np import *\n",
    "from common.base_model import BaseModel\n",
    "\n",
    "class BetterRnnlm(BaseModel):\n",
    "    def __init__(self, vocab_size=10000, wordvec_size=650, hidden_size=650, dropout_ratio=0.5):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        # 가중치 초기화\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx1 = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh1 = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b1 = np.zeros(4 * H).astype('f')\n",
    "        lstm_Wx2 = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_Wh2 = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b2 = np.zeros(4 * H).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = [\n",
    "            TimeEmbedding(embed_W),\n",
    "            TimeDropout(dropout_ratio),\n",
    "            TimeLSTM(lstm_Wx1, lstm_Wh1, lstm_b1, stateful=True),\n",
    "            TimeDropout(dropout_ratio),\n",
    "            TimeLSTM(lstm_Wx2, lstm_Wh2, lstm_b2, stateful=True),\n",
    "            TimeDropout(dropout_ratio),\n",
    "            TimeAffine(embed_W.T, affine_b)                             # 가중치 공유\n",
    "        ]\n",
    "        self.loss_layer = TimeSoftmaxWithLoss()\n",
    "        self.lstm_layers = [self.layers[2], self.layers[4]]\n",
    "        self.drop_layers = [self.layers[1], self.layers[3], self.layers[5]]\n",
    "\n",
    "        # 모든 가중치와 기울기를 리스트에 모으기\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def predict(self, xs, train_flg=False):\n",
    "        for layer in self.drop_layers:\n",
    "            layer.train_flg = train_flg\n",
    "        for layer in self.layers:\n",
    "            xs = layer.forward(xs)\n",
    "        return xs\n",
    "\n",
    "    def forward(self, xs, ts, train_flg=True):\n",
    "        score = self.predict(xs, train_flg)\n",
    "        loss = self.loss_layer(score, ts)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "\n",
    "    def reset_state(self):\n",
    "        for layer in self.lstm_layers:\n",
    "            layer.reset_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RnnlmGen(Rnnlm):\n",
    "    def generate(self, start_id, skip_ids=None, sample_size=100):\n",
    "        word_ids = [start_id]\n",
    "\n",
    "        x = start_id\n",
    "        while len(word_ids) < sample_size:\n",
    "            x = np.array(x).reshape(1, 1)\n",
    "            score = self.predict(x)\n",
    "            p = softmax(score.flatten())\n",
    "\n",
    "            sampled = np.random.choice(len(p), size=1, p=p)\n",
    "            if (skip_ids is None) or (sampled not in skip_ids):\n",
    "                x = sampled\n",
    "                word_ids.append(int(x))\n",
    "        \n",
    "        return word_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetterRnnlmGen(BetterRnnlm):\n",
    "    def generate(self, start_id, skip_ids=None, sample_size=100):\n",
    "        word_ids = [start_id]\n",
    "\n",
    "        x = start_id\n",
    "        while len(word_ids) < sample_size:\n",
    "            x = np.array(x).reshape(1, 1)\n",
    "            score = self.predict(x).flatten()\n",
    "            p = softmax(score).flatten()\n",
    "\n",
    "            sampled = np.random.choice(len(p), size=1, p=p)\n",
    "            if (skip_ids is None) or (sampled not in skip_ids):\n",
    "                x = sampled\n",
    "                word_ids.append(int(x))\n",
    "\n",
    "        return word_ids\n",
    "\n",
    "    def get_state(self):\n",
    "        states = []\n",
    "        for layer in self.lstm_layers:\n",
    "            states.append((layer.h, layer.c))\n",
    "        return states\n",
    "\n",
    "    def set_state(self, states):\n",
    "        for layer, state in zip(self.lstm_layers, states):\n",
    "            layer.set_state(*state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문장 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "you 'll they want resort solo music says mr. roth.\n one of the stick we could 'll know e. singer gerald.\n it has no casual problem this will be a bearing the drug and no other that comes by the ages of the century.\n the ex-dividend included the san francisco bay area series stock-index floor volume as five others.\n some causes also lagging in part more get said ms. macdonald will aggressively be sir alan jenrette.\n the balance had enough extension to a nam in ideal 2-for-1.\n but arbitragers said that the new matter\n--------------------------------------------------\nthe meaning of life is to place the living of the domestic electronics and minnesota.\n reins show the company report.\n the british engineering franchise is expected to be installed tomorrow because it people sees proposals for ford.\n however at several other businessmen have sent wrongdoing on news.\n campeau corp. the brazilian petroleum and industrial company also in the u.k. hong kong is n't on the progress of the full reactions.\n the contract is barely weakened by the dollar slipping to stocks an ounce.\n contract coastal said gorbachev has had promised a sale in utilities that should the drop\n"
    }
   ],
   "source": [
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "corpus_size = len(corpus)\n",
    "\n",
    "\n",
    "model = BetterRnnlmGen()\n",
    "model.load_params(r\"E:\\Develop\\Machine-Learning\\밑바닥부터 시작하는 딥러닝2\\BetterRnnlm.pkl\")\n",
    "\n",
    "# start 문자와 skip 문자 설정\n",
    "start_word = 'you'\n",
    "start_id = word_to_id[start_word]\n",
    "skip_words = ['N', '<unk>', '$']\n",
    "skip_ids = [word_to_id[w] for w in skip_words]\n",
    "# 문장 생성\n",
    "word_ids = model.generate(start_id, skip_ids)\n",
    "txt = ' '.join([id_to_word[i] for i in word_ids])\n",
    "txt = txt.replace(' <eos>', '.\\n')\n",
    "\n",
    "print(txt)\n",
    "\n",
    "\n",
    "model.reset_state()\n",
    "\n",
    "start_words = 'the meaning of life is'\n",
    "start_ids = [word_to_id[w] for w in start_words.split(' ')]\n",
    "\n",
    "for x in start_ids[:-1]:\n",
    "    x = np.array(x).reshape(1, 1)\n",
    "    model.predict(x)\n",
    "\n",
    "word_ids = model.generate(start_ids[-1], skip_ids)\n",
    "word_ids = start_ids[:-1] + word_ids\n",
    "txt = ' '.join([id_to_word[i] for i in word_ids])\n",
    "txt = txt.replace(' <eos>', '.\\n')\n",
    "print('-' * 50)\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "you h&r ferdinand wedtech turbulence attracting theirs wholly banks rep fundamentals bursts mortality 500-stock colors tax-exempt lincoln owns paribas luxury-car devices richter cypress aging rates commonwealth unwilling supplied dubious london suburban structured pollution lifting reoffered rumored match maneuver steal fruit coda connecting marvin veteran evening voters anti-nuclear fear pleased generous aggressive distributor philippines homelessness crucial attributed pipeline friendship carla byrd stay offshore condition supercomputer municipals jumped floating eight uncertain jewelers nicholas majority bus n.j. painted users alberta subsequently exercisable activity taylor transplant noriega quina larsen torrijos gangs probing contemporary outsiders simple marginal wildly salinger bank-holding ddb depositary unavailable robertson annual\n"
    }
   ],
   "source": [
    "from dataset import ptb\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "corpus_size = len(corpus)\n",
    "\n",
    "model = RnnlmGen()\n",
    "model.load_params(r\"E:\\Develop\\Machine-Learning\\밑바닥부터 시작하는 딥러닝2\\BetterRnnlm.pkl\")        # 가중치 가져오기\n",
    "\n",
    "# 시작문자와 넘어갈 문자 설정\n",
    "start_word = 'you'\n",
    "start_id = word_to_id[start_word]\n",
    "skip_words = ['N', '<unk>', '$']\n",
    "skip_ids = [word_to_id[w] for w in skip_words]\n",
    "\n",
    "# 문장 생성\n",
    "word_ids = model.generate(start_id, skip_ids)\n",
    "txt = ' '.join([id_to_word[i] for i in word_ids])\n",
    "txt = txt.replace(' <eos>', '.\\n')\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(45000, 7) (45000, 5)\n(5000, 7) (5000, 5)\n[[ 3  0  2 ...  0 11  5]\n [ 4  0  9 ...  8  8 10]\n [ 1  1  2 ...  9  0  5]\n ...\n [ 3  1 10 ...  8  0  3]\n [ 1  2  8 ...  0  5  5]\n [ 8  2  4 ... 10  5  5]]\n[[ 6  0 11  7  5]\n [ 6  3 10 10  5]\n [ 6  3  1  3  5]\n ...\n [ 6  7 11  9  5]\n [ 6  8  3  3  5]\n [ 6  4  1  4  5]]\n"
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, list found",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-7b62c132e22c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid_to_char\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid_to_char\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mt_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: sequence item 0: expected str instance, list found"
     ]
    }
   ],
   "source": [
    "from dataset import sequence\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    sequence.load_data('addition.txt', seed=1984)\n",
    "char_to_id, id_to_char = sequence.get_vocab()\n",
    "\n",
    "print(x_train.shape, t_train.shape)\n",
    "print(x_test.shape, t_test.shape)\n",
    "\n",
    "print(x_train)\n",
    "print(t_train)\n",
    "\n",
    "print(''.join([id_to_char[c]] for c in x_train[0]))\n",
    "print(''.join([id_to_char[c]] for c in t_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "\n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=False)\n",
    "\n",
    "        self.params = self.embed.params + self.lstm.params\n",
    "        self.grads = self.embed.grads + self.lstm.grads\n",
    "        self.hs = None\n",
    "        \n",
    "\n",
    "    def forward(self, xs):\n",
    "        xs = self.embed.forward(xs)\n",
    "        hs = self.lstm.forward(xs)\n",
    "        self.hs = hs\n",
    "\n",
    "        return hs[:, -1, :]\n",
    "\n",
    "    def backward(self, dh):\n",
    "        dhs = np.zeros_like(self.hs)\n",
    "        dhs[:, -1, :] = dh\n",
    "\n",
    "        dout = self.lstm.backward(dhs)\n",
    "        dout = self.embed.backward(dout)\n",
    "\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b)\n",
    "        self.affine = TimeAffine(affine_W, affine_b)\n",
    "        \n",
    "        self.params, self.grads = [], []\n",
    "\n",
    "        for layer in (self.embed, self.lstm, self.affine):\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def forward(self, xs, h):\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        out = self.embed.forward(xs)\n",
    "        out = self.lstm.forward(out)\n",
    "        score = self.affine.forward(out)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def backward(self, dscore):\n",
    "        dout = self.affine.backward(dscore)\n",
    "        dout = self.lstm.backward(dout)\n",
    "        dout = self.embed.backward(dout)\n",
    "        dh = self.lstm.dh\n",
    "\n",
    "        return dh\n",
    "\n",
    "    def generate(self, h, start_id, sample_size):\n",
    "        sampled = []\n",
    "        sample_id = start_id\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        for _ in range(sample_size):\n",
    "            x = np.array(sample_id).reshape((1, 1))\n",
    "            out = self.embed.forward(x)\n",
    "            out = self.lstm.forward(out)\n",
    "            score = self.affine.forward(out)\n",
    "\n",
    "            sample_id = np.argmax(score.flatten())\n",
    "            sampled.append(int(sample_id))\n",
    "\n",
    "        return sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2seq(BaseModel):\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        self.encoder = Encoder(V, D, H)\n",
    "        self.decoder = Decoder(V, D, H)\n",
    "        self.softmax = TimeSoftmaxWithLoss()\n",
    "\n",
    "        self.params = self.encoder.params + self.decoder.params\n",
    "        self.grads = self.encoder.grads + self.decoder.grads\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        decoder_xs, decoder_ts = ts[:, :-1], ts[:, 1:]\n",
    "        h = self.encoder.forward(xs)\n",
    "        score = self.decoder.forward(decoder_xs, h)\n",
    "        loss = self.softmax.forward(score, decoder_ts)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        dout = self.softmax.backward(dout)\n",
    "        dh = self.decoder.backward(dout)\n",
    "        dout = self.encoder.backward(dh)\n",
    "\n",
    "        return dout\n",
    "\n",
    "    def generate(self, xs, start_id, sample_size):\n",
    "        h = self.encoder.forward(xs)\n",
    "        sampled = self.decoder.generate(h, start_id, sample_size)\n",
    "\n",
    "        return sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.time_layers import *\n",
    "\n",
    "\n",
    "class PeekyDecoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx = (rn(H + D, 4 * H) / np.sqrt(H + D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "        affine_W = (rn(H + H, V) / np.sqrt(H + H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
    "        self.affine = TimeAffine(affine_W, affine_b)\n",
    "\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in (self.embed, self.lstm, self.affine):\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, xs, h):\n",
    "        N, T = xs.shape\n",
    "        N, H = h.shape\n",
    "\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        out = self.embed.forward(xs)\n",
    "        hs = np.repeat(h, T, axis=0).reshape(N, T, H)\n",
    "        out = np.concatenate((hs, out), axis=2)\n",
    "\n",
    "        out = self.lstm.forward(out)\n",
    "        out = np.concatenate((hs, out), axis=2)\n",
    "\n",
    "        score = self.affine.forward(out)\n",
    "        self.cache = H\n",
    "        return score\n",
    "\n",
    "    def backward(self, dscore):\n",
    "        H = self.cache\n",
    "\n",
    "        dout = self.affine.backward(dscore)\n",
    "        dout, dhs0 = dout[:, :, H:], dout[:, :, :H]\n",
    "        dout = self.lstm.backward(dout)\n",
    "        dembed, dhs1 = dout[:, :, H:], dout[:, :, :H]\n",
    "        self.embed.backward(dembed)\n",
    "\n",
    "        dhs = dhs0 + dhs1\n",
    "        dh = self.lstm.dh + np.sum(dhs, axis=1)\n",
    "        return dh\n",
    "\n",
    "    def generate(self, h, start_id, sample_size):\n",
    "        sampled = []\n",
    "        char_id = start_id\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        H = h.shape[1]\n",
    "        peeky_h = h.reshape(1, 1, H)\n",
    "        for _ in range(sample_size):\n",
    "            x = np.array([char_id]).reshape((1, 1))\n",
    "            out = self.embed.forward(x)\n",
    "\n",
    "            out = np.concatenate((peeky_h, out), axis=2)\n",
    "            out = self.lstm.forward(out)\n",
    "            out = np.concatenate((peeky_h, out), axis=2)\n",
    "            score = self.affine.forward(out)\n",
    "\n",
    "            char_id = np.argmax(score.flatten())\n",
    "            sampled.append(char_id)\n",
    "\n",
    "        return sampled\n",
    "\n",
    "\n",
    "class PeekySeq2seq(Seq2seq):\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        self.encoder = Encoder(V, D, H)\n",
    "        self.decoder = PeekyDecoder(V, D, H)\n",
    "        self.softmax = TimeSoftmaxWithLoss()\n",
    "\n",
    "        self.params = self.encoder.params + self.decoder.params\n",
    "        self.grads = self.encoder.grads + self.decoder.grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "/ 351 | 시간 15[s] | 손실 0.79\n| 에폭 7 |  반복 321 / 351 | 시간 16[s] | 손실 0.79\n| 에폭 7 |  반복 341 / 351 | 시간 17[s] | 손실 0.79\nQ 77+85  \nT 162 \nX 165 \n---\nQ 975+164\nT 1139\nX 1108\n---\nQ 582+84 \nT 666 \nX 665 \n---\nQ 8+155  \nT 163 \nX 160 \n---\nQ 367+55 \nT 422 \nO 422 \n---\nQ 600+257\nT 857 \nX 848 \n---\nQ 761+292\nT 1053\nX 1068\n---\nQ 830+597\nT 1427\nX 1460\n---\nQ 26+838 \nT 864 \nX 868 \n---\nQ 143+93 \nT 236 \nX 240 \n---\n검증 정확도 9.220%\n| 에폭 8 |  반복 1 / 351 | 시간 0[s] | 손실 0.81\n| 에폭 8 |  반복 21 / 351 | 시간 1[s] | 손실 0.77\n| 에폭 8 |  반복 41 / 351 | 시간 2[s] | 손실 0.77\n| 에폭 8 |  반복 61 / 351 | 시간 3[s] | 손실 0.78\n| 에폭 8 |  반복 81 / 351 | 시간 4[s] | 손실 0.77\n| 에폭 8 |  반복 101 / 351 | 시간 5[s] | 손실 0.76\n| 에폭 8 |  반복 121 / 351 | 시간 6[s] | 손실 0.76\n| 에폭 8 |  반복 141 / 351 | 시간 7[s] | 손실 0.75\n| 에폭 8 |  반복 161 / 351 | 시간 8[s] | 손실 0.75\n| 에폭 8 |  반복 181 / 351 | 시간 9[s] | 손실 0.75\n| 에폭 8 |  반복 201 / 351 | 시간 10[s] | 손실 0.75\n| 에폭 8 |  반복 221 / 351 | 시간 11[s] | 손실 0.74\n| 에폭 8 |  반복 241 / 351 | 시간 12[s] | 손실 0.72\n| 에폭 8 |  반복 261 / 351 | 시간 13[s] | 손실 0.72\n| 에폭 8 |  반복 281 / 351 | 시간 14[s] | 손실 0.74\n| 에폭 8 |  반복 301 / 351 | 시간 15[s] | 손실 0.74\n| 에폭 8 |  반복 321 / 351 | 시간 16[s] | 손실 0.73\n| 에폭 8 |  반복 341 / 351 | 시간 17[s] | 손실 0.71\nQ 77+85  \nT 162 \nX 164 \n---\nQ 975+164\nT 1139\nX 1137\n---\nQ 582+84 \nT 666 \nX 665 \n---\nQ 8+155  \nT 163 \nX 160 \n---\nQ 367+55 \nT 422 \nO 422 \n---\nQ 600+257\nT 857 \nX 849 \n---\nQ 761+292\nT 1053\nX 1037\n---\nQ 830+597\nT 1427\nX 1437\n---\nQ 26+838 \nT 864 \nX 869 \n---\nQ 143+93 \nT 236 \nX 231 \n---\n검증 정확도 12.480%\n| 에폭 9 |  반복 1 / 351 | 시간 0[s] | 손실 0.70\n| 에폭 9 |  반복 21 / 351 | 시간 1[s] | 손실 0.70\n| 에폭 9 |  반복 41 / 351 | 시간 2[s] | 손실 0.70\n| 에폭 9 |  반복 61 / 351 | 시간 3[s] | 손실 0.70\n| 에폭 9 |  반복 81 / 351 | 시간 4[s] | 손실 0.70\n| 에폭 9 |  반복 101 / 351 | 시간 5[s] | 손실 0.69\n| 에폭 9 |  반복 121 / 351 | 시간 6[s] | 손실 0.69\n| 에폭 9 |  반복 141 / 351 | 시간 7[s] | 손실 0.68\n| 에폭 9 |  반복 161 / 351 | 시간 8[s] | 손실 0.68\n| 에폭 9 |  반복 181 / 351 | 시간 9[s] | 손실 0.67\n| 에폭 9 |  반복 201 / 351 | 시간 10[s] | 손실 0.68\n| 에폭 9 |  반복 221 / 351 | 시간 11[s] | 손실 0.67\n| 에폭 9 |  반복 241 / 351 | 시간 12[s] | 손실 0.67\n| 에폭 9 |  반복 261 / 351 | 시간 13[s] | 손실 0.67\n| 에폭 9 |  반복 281 / 351 | 시간 14[s] | 손실 0.66\n| 에폭 9 |  반복 301 / 351 | 시간 15[s] | 손실 0.66\n| 에폭 9 |  반복 321 / 351 | 시간 16[s] | 손실 0.66\n| 에폭 9 |  반복 341 / 351 | 시간 17[s] | 손실 0.66\nQ 77+85  \nT 162 \nX 155 \n---\nQ 975+164\nT 1139\nX 1155\n---\nQ 582+84 \nT 666 \nX 665 \n---\nQ 8+155  \nT 163 \nX 162 \n---\nQ 367+55 \nT 422 \nO 422 \n---\nQ 600+257\nT 857 \nX 855 \n---\nQ 761+292\nT 1053\nX 1055\n---\nQ 830+597\nT 1427\nX 1405\n---\nQ 26+838 \nT 864 \nX 865 \n---\nQ 143+93 \nT 236 \nX 239 \n---\n검증 정확도 16.120%\n| 에폭 10 |  반복 1 / 351 | 시간 0[s] | 손실 0.69\n| 에폭 10 |  반복 21 / 351 | 시간 1[s] | 손실 0.64\n| 에폭 10 |  반복 41 / 351 | 시간 2[s] | 손실 0.65\n| 에폭 10 |  반복 61 / 351 | 시간 3[s] | 손실 0.64\n| 에폭 10 |  반복 81 / 351 | 시간 4[s] | 손실 0.63\n| 에폭 10 |  반복 101 / 351 | 시간 5[s] | 손실 0.63\n| 에폭 10 |  반복 121 / 351 | 시간 6[s] | 손실 0.63\n| 에폭 10 |  반복 141 / 351 | 시간 7[s] | 손실 0.63\n| 에폭 10 |  반복 161 / 351 | 시간 8[s] | 손실 0.63\n| 에폭 10 |  반복 181 / 351 | 시간 9[s] | 손실 0.64\n| 에폭 10 |  반복 201 / 351 | 시간 10[s] | 손실 0.63\n| 에폭 10 |  반복 221 / 351 | 시간 11[s] | 손실 0.65\n| 에폭 10 |  반복 241 / 351 | 시간 12[s] | 손실 0.63\n| 에폭 10 |  반복 261 / 351 | 시간 13[s] | 손실 0.63\n| 에폭 10 |  반복 281 / 351 | 시간 14[s] | 손실 0.61\n| 에폭 10 |  반복 301 / 351 | 시간 15[s] | 손실 0.61\n| 에폭 10 |  반복 321 / 351 | 시간 16[s] | 손실 0.61\n| 에폭 10 |  반복 341 / 351 | 시간 17[s] | 손실 0.61\nQ 77+85  \nT 162 \nX 164 \n---\nQ 975+164\nT 1139\nX 1140\n---\nQ 582+84 \nT 666 \nX 668 \n---\nQ 8+155  \nT 163 \nX 161 \n---\nQ 367+55 \nT 422 \nO 422 \n---\nQ 600+257\nT 857 \nX 856 \n---\nQ 761+292\nT 1053\nX 1047\n---\nQ 830+597\nT 1427\nX 1428\n---\nQ 26+838 \nT 864 \nX 865 \n---\nQ 143+93 \nT 236 \nX 235 \n---\n검증 정확도 18.860%\n| 에폭 11 |  반복 1 / 351 | 시간 0[s] | 손실 0.57\n| 에폭 11 |  반복 21 / 351 | 시간 1[s] | 손실 0.59\n| 에폭 11 |  반복 41 / 351 | 시간 2[s] | 손실 0.58\n| 에폭 11 |  반복 61 / 351 | 시간 3[s] | 손실 0.59\n| 에폭 11 |  반복 81 / 351 | 시간 5[s] | 손실 0.59\n| 에폭 11 |  반복 101 / 351 | 시간 6[s] | 손실 0.58\n| 에폭 11 |  반복 121 / 351 | 시간 7[s] | 손실 0.58\n| 에폭 11 |  반복 141 / 351 | 시간 8[s] | 손실 0.59\n| 에폭 11 |  반복 161 / 351 | 시간 9[s] | 손실 0.58\n| 에폭 11 |  반복 181 / 351 | 시간 10[s] | 손실 0.59\n| 에폭 11 |  반복 201 / 351 | 시간 11[s] | 손실 0.59\n| 에폭 11 |  반복 221 / 351 | 시간 12[s] | 손실 0.58\n| 에폭 11 |  반복 241 / 351 | 시간 13[s] | 손실 0.58\n| 에폭 11 |  반복 261 / 351 | 시간 14[s] | 손실 0.57\n| 에폭 11 |  반복 281 / 351 | 시간 16[s] | 손실 0.57\n| 에폭 11 |  반복 301 / 351 | 시간 17[s] | 손실 0.57\n| 에폭 11 |  반복 321 / 351 | 시간 18[s] | 손실 0.57\n| 에폭 11 |  반복 341 / 351 | 시간 19[s] | 손실 0.57\nQ 77+85  \nT 162 \nX 160 \n---\nQ 975+164\nT 1139\nX 1147\n---\nQ 582+84 \nT 666 \nX 665 \n---\nQ 8+155  \nT 163 \nX 166 \n---\nQ 367+55 \nT 422 \nO 422 \n---\nQ 600+257\nT 857 \nX 855 \n---\nQ 761+292\nT 1053\nX 1051\n---\nQ 830+597\nT 1427\nX 1429\n---\nQ 26+838 \nT 864 \nX 868 \n---\nQ 143+93 \nT 236 \nX 235 \n---\n검증 정확도 16.800%\n| 에폭 12 |  반복 1 / 351 | 시간 0[s] | 손실 0.58\n| 에폭 12 |  반복 21 / 351 | 시간 1[s] | 손실 0.55\n| 에폭 12 |  반복 41 / 351 | 시간 2[s] | 손실 0.56\n| 에폭 12 |  반복 61 / 351 | 시간 3[s] | 손실 0.58\n| 에폭 12 |  반복 81 / 351 | 시간 4[s] | 손실 0.56\n| 에폭 12 |  반복 101 / 351 | 시간 5[s] | 손실 0.55\n| 에폭 12 |  반복 121 / 351 | 시간 6[s] | 손실 0.54\n| 에폭 12 |  반복 141 / 351 | 시간 7[s] | 손실 0.54\n| 에폭 12 |  반복 161 / 351 | 시간 8[s] | 손실 0.55\n| 에폭 12 |  반복 181 / 351 | 시간 10[s] | 손실 0.55\n| 에폭 12 |  반복 201 / 351 | 시간 11[s] | 손실 0.55\n| 에폭 12 |  반복 221 / 351 | 시간 12[s] | 손실 0.54\n| 에폭 12 |  반복 241 / 351 | 시간 14[s] | 손실 0.55\n| 에폭 12 |  반복 261 / 351 | 시간 16[s] | 손실 0.55\n| 에폭 12 |  반복 281 / 351 | 시간 17[s] | 손실 0.55\n| 에폭 12 |  반복 301 / 351 | 시간 18[s] | 손실 0.53\n| 에폭 12 |  반복 321 / 351 | 시간 20[s] | 손실 0.53\n| 에폭 12 |  반복 341 / 351 | 시간 22[s] | 손실 0.54\nQ 77+85  \nT 162 \nX 161 \n---\nQ 975+164\nT 1139\nX 1147\n---\nQ 582+84 \nT 666 \nO 666 \n---\nQ 8+155  \nT 163 \nX 164 \n---\nQ 367+55 \nT 422 \nO 422 \n---\nQ 600+257\nT 857 \nX 855 \n---\nQ 761+292\nT 1053\nX 1047\n---\nQ 830+597\nT 1427\nX 1429\n---\nQ 26+838 \nT 864 \nX 865 \n---\nQ 143+93 \nT 236 \nX 235 \n---\n검증 정확도 21.200%\n| 에폭 13 |  반복 1 / 351 | 시간 0[s] | 손실 0.51\n| 에폭 13 |  반복 21 / 351 | 시간 2[s] | 손실 0.53\n| 에폭 13 |  반복 41 / 351 | 시간 3[s] | 손실 0.52\n| 에폭 13 |  반복 61 / 351 | 시간 6[s] | 손실 0.52\n| 에폭 13 |  반복 81 / 351 | 시간 9[s] | 손실 0.52\n| 에폭 13 |  반복 101 / 351 | 시간 11[s] | 손실 0.51\n| 에폭 13 |  반복 121 / 351 | 시간 13[s] | 손실 0.51\n| 에폭 13 |  반복 141 / 351 | 시간 15[s] | 손실 0.51\n| 에폭 13 |  반복 161 / 351 | 시간 17[s] | 손실 0.50\n| 에폭 13 |  반복 181 / 351 | 시간 18[s] | 손실 0.51\n| 에폭 13 |  반복 201 / 351 | 시간 20[s] | 손실 0.51\n| 에폭 13 |  반복 221 / 351 | 시간 22[s] | 손실 0.51\n| 에폭 13 |  반복 241 / 351 | 시간 23[s] | 손실 0.51\n| 에폭 13 |  반복 261 / 351 | 시간 24[s] | 손실 0.51\n| 에폭 13 |  반복 281 / 351 | 시간 26[s] | 손실 0.50\n| 에폭 13 |  반복 301 / 351 | 시간 28[s] | 손실 0.50\n| 에폭 13 |  반복 321 / 351 | 시간 30[s] | 손실 0.49\n| 에폭 13 |  반복 341 / 351 | 시간 32[s] | 손실 0.49\nQ 77+85  \nT 162 \nX 160 \n---\nQ 975+164\nT 1139\nX 1138\n---\nQ 582+84 \nT 666 \nX 668 \n---\nQ 8+155  \nT 163 \nX 160 \n---\nQ 367+55 \nT 422 \nX 423 \n---\nQ 600+257\nT 857 \nX 856 \n---\nQ 761+292\nT 1053\nX 1048\n---\nQ 830+597\nT 1427\nX 1429\n---\nQ 26+838 \nT 864 \nO 864 \n---\nQ 143+93 \nT 236 \nX 233 \n---\n검증 정확도 26.140%\n| 에폭 14 |  반복 1 / 351 | 시간 0[s] | 손실 0.48\n| 에폭 14 |  반복 21 / 351 | 시간 2[s] | 손실 0.48\n| 에폭 14 |  반복 41 / 351 | 시간 3[s] | 손실 0.49\n| 에폭 14 |  반복 61 / 351 | 시간 6[s] | 손실 0.49\n| 에폭 14 |  반복 81 / 351 | 시간 9[s] | 손실 0.50\n| 에폭 14 |  반복 101 / 351 | 시간 11[s] | 손실 0.49\n| 에폭 14 |  반복 121 / 351 | 시간 14[s] | 손실 0.49\n| 에폭 14 |  반복 141 / 351 | 시간 16[s] | 손실 0.50\n| 에폭 14 |  반복 161 / 351 | 시간 19[s] | 손실 0.49\n| 에폭 14 |  반복 181 / 351 | 시간 21[s] | 손실 0.49\n| 에폭 14 |  반복 201 / 351 | 시간 22[s] | 손실 0.49\n| 에폭 14 |  반복 221 / 351 | 시간 25[s] | 손실 0.49\n| 에폭 14 |  반복 241 / 351 | 시간 28[s] | 손실 0.48\n| 에폭 14 |  반복 261 / 351 | 시간 29[s] | 손실 0.49\n| 에폭 14 |  반복 281 / 351 | 시간 31[s] | 손실 0.48\n| 에폭 14 |  반복 301 / 351 | 시간 32[s] | 손실 0.48\n| 에폭 14 |  반복 321 / 351 | 시간 33[s] | 손실 0.48\n| 에폭 14 |  반복 341 / 351 | 시간 35[s] | 손실 0.48\nQ 77+85  \nT 162 \nX 160 \n---\nQ 975+164\nT 1139\nX 1144\n---\nQ 582+84 \nT 666 \nX 667 \n---\nQ 8+155  \nT 163 \nX 160 \n---\nQ 367+55 \nT 422 \nX 423 \n---\nQ 600+257\nT 857 \nX 855 \n---\nQ 761+292\nT 1053\nO 1053\n---\nQ 830+597\nT 1427\nX 1429\n---\nQ 26+838 \nT 864 \nO 864 \n---\nQ 143+93 \nT 236 \nX 235 \n---\n검증 정확도 23.560%\n| 에폭 15 |  반복 1 / 351 | 시간 0[s] | 손실 0.49\n| 에폭 15 |  반복 21 / 351 | 시간 1[s] | 손실 0.47\n| 에폭 15 |  반복 41 / 351 | 시간 2[s] | 손실 0.46\n| 에폭 15 |  반복 61 / 351 | 시간 3[s] | 손실 0.48\n| 에폭 15 |  반복 81 / 351 | 시간 4[s] | 손실 0.47\n| 에폭 15 |  반복 101 / 351 | 시간 5[s] | 손실 0.47\n| 에폭 15 |  반복 121 / 351 | 시간 6[s] | 손실 0.46\n| 에폭 15 |  반복 141 / 351 | 시간 7[s] | 손실 0.46\n| 에폭 15 |  반복 161 / 351 | 시간 8[s] | 손실 0.45\n| 에폭 15 |  반복 181 / 351 | 시간 9[s] | 손실 0.45\n| 에폭 15 |  반복 201 / 351 | 시간 10[s] | 손실 0.45\n| 에폭 15 |  반복 221 / 351 | 시간 11[s] | 손실 0.45\n| 에폭 15 |  반복 241 / 351 | 시간 12[s] | 손실 0.46\n| 에폭 15 |  반복 261 / 351 | 시간 13[s] | 손실 0.47\n| 에폭 15 |  반복 281 / 351 | 시간 14[s] | 손실 0.47\n| 에폭 15 |  반복 301 / 351 | 시간 15[s] | 손실 0.46\n| 에폭 15 |  반복 321 / 351 | 시간 16[s] | 손실 0.45\n| 에폭 15 |  반복 341 / 351 | 시간 17[s] | 손실 0.46\nQ 77+85  \nT 162 \nX 163 \n---\nQ 975+164\nT 1139\nO 1139\n---\nQ 582+84 \nT 666 \nX 668 \n---\nQ 8+155  \nT 163 \nX 160 \n---\nQ 367+55 \nT 422 \nX 423 \n---\nQ 600+257\nT 857 \nX 856 \n---\nQ 761+292\nT 1053\nX 1048\n---\nQ 830+597\nT 1427\nX 1429\n---\nQ 26+838 \nT 864 \nX 865 \n---\nQ 143+93 \nT 236 \nX 235 \n---\n검증 정확도 27.240%\n| 에폭 16 |  반복 1 / 351 | 시간 0[s] | 손실 0.47\n| 에폭 16 |  반복 21 / 351 | 시간 1[s] | 손실 0.44\n| 에폭 16 |  반복 41 / 351 | 시간 1[s] | 손실 0.45\n| 에폭 16 |  반복 61 / 351 | 시간 3[s] | 손실 0.45\n| 에폭 16 |  반복 81 / 351 | 시간 4[s] | 손실 0.44\n| 에폭 16 |  반복 101 / 351 | 시간 5[s] | 손실 0.44\n| 에폭 16 |  반복 121 / 351 | 시간 6[s] | 손실 0.43\n| 에폭 16 |  반복 141 / 351 | 시간 7[s] | 손실 0.44\n| 에폭 16 |  반복 161 / 351 | 시간 8[s] | 손실 0.43\n| 에폭 16 |  반복 181 / 351 | 시간 9[s] | 손실 0.43\n| 에폭 16 |  반복 201 / 351 | 시간 10[s] | 손실 0.42\n| 에폭 16 |  반복 221 / 351 | 시간 11[s] | 손실 0.43\n| 에폭 16 |  반복 241 / 351 | 시간 12[s] | 손실 0.44\n| 에폭 16 |  반복 261 / 351 | 시간 13[s] | 손실 0.43\n| 에폭 16 |  반복 281 / 351 | 시간 14[s] | 손실 0.42\n| 에폭 16 |  반복 301 / 351 | 시간 15[s] | 손실 0.43\n| 에폭 16 |  반복 321 / 351 | 시간 16[s] | 손실 0.45\n| 에폭 16 |  반복 341 / 351 | 시간 17[s] | 손실 0.44\nQ 77+85  \nT 162 \nX 163 \n---\nQ 975+164\nT 1139\nX 1137\n---\nQ 582+84 \nT 666 \nX 667 \n---\nQ 8+155  \nT 163 \nX 162 \n---\nQ 367+55 \nT 422 \nO 422 \n---\nQ 600+257\nT 857 \nX 856 \n---\nQ 761+292\nT 1053\nX 1048\n---\nQ 830+597\nT 1427\nX 1429\n---\nQ 26+838 \nT 864 \nO 864 \n---\nQ 143+93 \nT 236 \nO 236 \n---\n검증 정확도 31.180%\n| 에폭 17 |  반복 1 / 351 | 시간 0[s] | 손실 0.41\n| 에폭 17 |  반복 21 / 351 | 시간 1[s] | 손실 0.41\n| 에폭 17 |  반복 41 / 351 | 시간 2[s] | 손실 0.41\n| 에폭 17 |  반복 61 / 351 | 시간 3[s] | 손실 0.42\n| 에폭 17 |  반복 81 / 351 | 시간 4[s] | 손실 0.42\n| 에폭 17 |  반복 101 / 351 | 시간 5[s] | 손실 0.42\n| 에폭 17 |  반복 121 / 351 | 시간 6[s] | 손실 0.42\n| 에폭 17 |  반복 141 / 351 | 시간 7[s] | 손실 0.44\n| 에폭 17 |  반복 161 / 351 | 시간 8[s] | 손실 0.42\n| 에폭 17 |  반복 181 / 351 | 시간 9[s] | 손실 0.42\n| 에폭 17 |  반복 201 / 351 | 시간 10[s] | 손실 0.41\n| 에폭 17 |  반복 221 / 351 | 시간 11[s] | 손실 0.41\n| 에폭 17 |  반복 241 / 351 | 시간 12[s] | 손실 0.42\n| 에폭 17 |  반복 261 / 351 | 시간 13[s] | 손실 0.41\n| 에폭 17 |  반복 281 / 351 | 시간 14[s] | 손실 0.42\n| 에폭 17 |  반복 301 / 351 | 시간 16[s] | 손실 0.41\n| 에폭 17 |  반복 321 / 351 | 시간 17[s] | 손실 0.40\n| 에폭 17 |  반복 341 / 351 | 시간 18[s] | 손실 0.40\nQ 77+85  \nT 162 \nX 163 \n---\nQ 975+164\nT 1139\nX 1140\n---\nQ 582+84 \nT 666 \nX 667 \n---\nQ 8+155  \nT 163 \nX 162 \n---\nQ 367+55 \nT 422 \nO 422 \n---\nQ 600+257\nT 857 \nX 856 \n---\nQ 761+292\nT 1053\nX 1052\n---\nQ 830+597\nT 1427\nX 1429\n---\nQ 26+838 \nT 864 \nO 864 \n---\nQ 143+93 \nT 236 \nO 236 \n---\n검증 정확도 36.740%\n| 에폭 18 |  반복 1 / 351 | 시간 0[s] | 손실 0.40\n| 에폭 18 |  반복 21 / 351 | 시간 1[s] | 손실 0.39\n| 에폭 18 |  반복 41 / 351 | 시간 2[s] | 손실 0.41\n| 에폭 18 |  반복 61 / 351 | 시간 3[s] | 손실 0.40\n| 에폭 18 |  반복 81 / 351 | 시간 4[s] | 손실 0.39\n| 에폭 18 |  반복 101 / 351 | 시간 5[s] | 손실 0.40\n| 에폭 18 |  반복 121 / 351 | 시간 6[s] | 손실 0.40\n| 에폭 18 |  반복 141 / 351 | 시간 7[s] | 손실 0.39\n| 에폭 18 |  반복 161 / 351 | 시간 9[s] | 손실 0.39\n| 에폭 18 |  반복 181 / 351 | 시간 10[s] | 손실 0.39\n| 에폭 18 |  반복 201 / 351 | 시간 11[s] | 손실 0.38\n| 에폭 18 |  반복 221 / 351 | 시간 12[s] | 손실 0.39\n| 에폭 18 |  반복 241 / 351 | 시간 13[s] | 손실 0.39\n| 에폭 18 |  반복 261 / 351 | 시간 14[s] | 손실 0.40\n| 에폭 18 |  반복 281 / 351 | 시간 15[s] | 손실 0.39\n| 에폭 18 |  반복 301 / 351 | 시간 16[s] | 손실 0.39\n| 에폭 18 |  반복 321 / 351 | 시간 17[s] | 손실 0.38\n| 에폭 18 |  반복 341 / 351 | 시간 18[s] | 손실 0.39\nQ 77+85  \nT 162 \nX 163 \n---\nQ 975+164\nT 1139\nO 1139\n---\nQ 582+84 \nT 666 \nX 668 \n---\nQ 8+155  \nT 163 \nX 164 \n---\nQ 367+55 \nT 422 \nO 422 \n---\nQ 600+257\nT 857 \nO 857 \n---\nQ 761+292\nT 1053\nX 1051\n---\nQ 830+597\nT 1427\nX 1429\n---\nQ 26+838 \nT 864 \nX 865 \n---\nQ 143+93 \nT 236 \nO 236 \n---\n검증 정확도 38.240%\n| 에폭 19 |  반복 1 / 351 | 시간 0[s] | 손실 0.36\n| 에폭 19 |  반복 21 / 351 | 시간 1[s] | 손실 0.38\n| 에폭 19 |  반복 41 / 351 | 시간 2[s] | 손실 0.36\n| 에폭 19 |  반복 61 / 351 | 시간 3[s] | 손실 0.36\n| 에폭 19 |  반복 81 / 351 | 시간 5[s] | 손실 0.38\n| 에폭 19 |  반복 101 / 351 | 시간 7[s] | 손실 0.38\n| 에폭 19 |  반복 121 / 351 | 시간 9[s] | 손실 0.37\n| 에폭 19 |  반복 141 / 351 | 시간 11[s] | 손실 0.36\n| 에폭 19 |  반복 161 / 351 | 시간 13[s] | 손실 0.37\n| 에폭 19 |  반복 181 / 351 | 시간 14[s] | 손실 0.38\n| 에폭 19 |  반복 201 / 351 | 시간 16[s] | 손실 0.36\n| 에폭 19 |  반복 221 / 351 | 시간 17[s] | 손실 0.35\n| 에폭 19 |  반복 241 / 351 | 시간 18[s] | 손실 0.36\n| 에폭 19 |  반복 261 / 351 | 시간 20[s] | 손실 0.36\n| 에폭 19 |  반복 281 / 351 | 시간 21[s] | 손실 0.37\n| 에폭 19 |  반복 301 / 351 | 시간 22[s] | 손실 0.36\n| 에폭 19 |  반복 321 / 351 | 시간 23[s] | 손실 0.35\n| 에폭 19 |  반복 341 / 351 | 시간 24[s] | 손실 0.35\nQ 77+85  \nT 162 \nX 163 \n---\nQ 975+164\nT 1139\nX 1147\n---\nQ 582+84 \nT 666 \nX 667 \n---\nQ 8+155  \nT 163 \nX 162 \n---\nQ 367+55 \nT 422 \nO 422 \n---\nQ 600+257\nT 857 \nO 857 \n---\nQ 761+292\nT 1053\nO 1053\n---\nQ 830+597\nT 1427\nX 1431\n---\nQ 26+838 \nT 864 \nX 865 \n---\nQ 143+93 \nT 236 \nO 236 \n---\n검증 정확도 41.860%\n| 에폭 20 |  반복 1 / 351 | 시간 0[s] | 손실 0.37\n| 에폭 20 |  반복 21 / 351 | 시간 1[s] | 손실 0.35\n| 에폭 20 |  반복 41 / 351 | 시간 2[s] | 손실 0.35\n| 에폭 20 |  반복 61 / 351 | 시간 3[s] | 손실 0.34\n| 에폭 20 |  반복 81 / 351 | 시간 4[s] | 손실 0.34\n| 에폭 20 |  반복 101 / 351 | 시간 5[s] | 손실 0.34\n| 에폭 20 |  반복 121 / 351 | 시간 6[s] | 손실 0.34\n| 에폭 20 |  반복 141 / 351 | 시간 8[s] | 손실 0.34\n| 에폭 20 |  반복 161 / 351 | 시간 9[s] | 손실 0.33\n| 에폭 20 |  반복 181 / 351 | 시간 10[s] | 손실 0.32\n| 에폭 20 |  반복 201 / 351 | 시간 11[s] | 손실 0.33\n| 에폭 20 |  반복 221 / 351 | 시간 12[s] | 손실 0.34\n| 에폭 20 |  반복 241 / 351 | 시간 13[s] | 손실 0.34\n| 에폭 20 |  반복 261 / 351 | 시간 14[s] | 손실 0.34\n| 에폭 20 |  반복 281 / 351 | 시간 15[s] | 손실 0.34\n| 에폭 20 |  반복 301 / 351 | 시간 16[s] | 손실 0.33\n| 에폭 20 |  반복 321 / 351 | 시간 17[s] | 손실 0.32\n| 에폭 20 |  반복 341 / 351 | 시간 18[s] | 손실 0.32\nQ 77+85  \nT 162 \nX 163 \n---\nQ 975+164\nT 1139\nO 1139\n---\nQ 582+84 \nT 666 \nX 667 \n---\nQ 8+155  \nT 163 \nX 162 \n---\nQ 367+55 \nT 422 \nO 422 \n---\nQ 600+257\nT 857 \nO 857 \n---\nQ 761+292\nT 1053\nX 1047\n---\nQ 830+597\nT 1427\nX 1431\n---\nQ 26+838 \nT 864 \nX 865 \n---\nQ 143+93 \nT 236 \nX 237 \n---\n검증 정확도 47.080%\n| 에폭 21 |  반복 1 / 351 | 시간 0[s] | 손실 0.32\n| 에폭 21 |  반복 21 / 351 | 시간 1[s] | 손실 0.31\n| 에폭 21 |  반복 41 / 351 | 시간 2[s] | 손실 0.31\n| 에폭 21 |  반복 61 / 351 | 시간 4[s] | 손실 0.30\n| 에폭 21 |  반복 81 / 351 | 시간 5[s] | 손실 0.30\n| 에폭 21 |  반복 101 / 351 | 시간 6[s] | 손실 0.31\n| 에폭 21 |  반복 121 / 351 | 시간 8[s] | 손실 0.30\n| 에폭 21 |  반복 141 / 351 | 시간 9[s] | 손실 0.29\n| 에폭 21 |  반복 161 / 351 | 시간 11[s] | 손실 0.30\n| 에폭 21 |  반복 181 / 351 | 시간 12[s] | 손실 0.29\n| 에폭 21 |  반복 201 / 351 | 시간 13[s] | 손실 0.29\n| 에폭 21 |  반복 221 / 351 | 시간 14[s] | 손실 0.32\n| 에폭 21 |  반복 241 / 351 | 시간 15[s] | 손실 0.32\n| 에폭 21 |  반복 261 / 351 | 시간 17[s] | 손실 0.30\n| 에폭 21 |  반복 281 / 351 | 시간 18[s] | 손실 0.29\n| 에폭 21 |  반복 301 / 351 | 시간 19[s] | 손실 0.29\n| 에폭 21 |  반복 321 / 351 | 시간 20[s] | 손실 0.28\n| 에폭 21 |  반복 341 / 351 | 시간 21[s] | 손실 0.29\nQ 77+85  \nT 162 \nX 163 \n---\nQ 975+164\nT 1139\nO 1139\n---\nQ 582+84 \nT 666 \nX 667 \n---\nQ 8+155  \nT 163 \nO 163 \n---\nQ 367+55 \nT 422 \nX 423 \n---\nQ 600+257\nT 857 \nO 857 \n---\nQ 761+292\nT 1053\nX 1054\n---\nQ 830+597\nT 1427\nX 1430\n---\nQ 26+838 \nT 864 \nO 864 \n---\nQ 143+93 \nT 236 \nO 236 \n---\n검증 정확도 54.960%\n| 에폭 22 |  반복 1 / 351 | 시간 0[s] | 손실 0.27\n| 에폭 22 |  반복 21 / 351 | 시간 1[s] | 손실 0.27\n| 에폭 22 |  반복 41 / 351 | 시간 2[s] | 손실 0.26\n| 에폭 22 |  반복 61 / 351 | 시간 3[s] | 손실 0.26\n| 에폭 22 |  반복 81 / 351 | 시간 4[s] | 손실 0.26\n| 에폭 22 |  반복 101 / 351 | 시간 5[s] | 손실 0.25\n| 에폭 22 |  반복 121 / 351 | 시간 6[s] | 손실 0.25\n| 에폭 22 |  반복 141 / 351 | 시간 7[s] | 손실 0.26\n| 에폭 22 |  반복 161 / 351 | 시간 9[s] | 손실 0.27\n| 에폭 22 |  반복 181 / 351 | 시간 10[s] | 손실 0.26\n| 에폭 22 |  반복 201 / 351 | 시간 11[s] | 손실 0.25\n| 에폭 22 |  반복 221 / 351 | 시간 12[s] | 손실 0.25\n| 에폭 22 |  반복 241 / 351 | 시간 13[s] | 손실 0.26\n| 에폭 22 |  반복 261 / 351 | 시간 15[s] | 손실 0.25\n| 에폭 22 |  반복 281 / 351 | 시간 16[s] | 손실 0.24\n| 에폭 22 |  반복 301 / 351 | 시간 18[s] | 손실 0.23\n| 에폭 22 |  반복 321 / 351 | 시간 21[s] | 손실 0.23\n| 에폭 22 |  반복 341 / 351 | 시간 22[s] | 손실 0.24\nQ 77+85  \nT 162 \nX 163 \n---\nQ 975+164\nT 1139\nX 1138\n---\nQ 582+84 \nT 666 \nX 667 \n---\nQ 8+155  \nT 163 \nX 162 \n---\nQ 367+55 \nT 422 \nO 422 \n---\nQ 600+257\nT 857 \nX 856 \n---\nQ 761+292\nT 1053\nX 1048\n---\nQ 830+597\nT 1427\nX 1430\n---\nQ 26+838 \nT 864 \nO 864 \n---\nQ 143+93 \nT 236 \nO 236 \n---\n검증 정확도 62.100%\n| 에폭 23 |  반복 1 / 351 | 시간 0[s] | 손실 0.21\n| 에폭 23 |  반복 21 / 351 | 시간 1[s] | 손실 0.23\n| 에폭 23 |  반복 41 / 351 | 시간 2[s] | 손실 0.22\n| 에폭 23 |  반복 61 / 351 | 시간 3[s] | 손실 0.22\n| 에폭 23 |  반복 81 / 351 | 시간 5[s] | 손실 0.24\n| 에폭 23 |  반복 101 / 351 | 시간 6[s] | 손실 0.22\n| 에폭 23 |  반복 121 / 351 | 시간 7[s] | 손실 0.22\n| 에폭 23 |  반복 141 / 351 | 시간 9[s] | 손실 0.21\n| 에폭 23 |  반복 161 / 351 | 시간 10[s] | 손실 0.20\n| 에폭 23 |  반복 181 / 351 | 시간 12[s] | 손실 0.21\n| 에폭 23 |  반복 201 / 351 | 시간 13[s] | 손실 0.21\n| 에폭 23 |  반복 221 / 351 | 시간 15[s] | 손실 0.21\n| 에폭 23 |  반복 241 / 351 | 시간 17[s] | 손실 0.20\n| 에폭 23 |  반복 261 / 351 | 시간 19[s] | 손실 0.21\n| 에폭 23 |  반복 281 / 351 | 시간 21[s] | 손실 0.20\n| 에폭 23 |  반복 301 / 351 | 시간 22[s] | 손실 0.20\n| 에폭 23 |  반복 321 / 351 | 시간 23[s] | 손실 0.20\n| 에폭 23 |  반복 341 / 351 | 시간 24[s] | 손실 0.19\nQ 77+85  \nT 162 \nO 162 \n---\nQ 975+164\nT 1139\nO 1139\n---\nQ 582+84 \nT 666 \nX 667 \n---\nQ 8+155  \nT 163 \nO 163 \n---\nQ 367+55 \nT 422 \nO 422 \n---\nQ 600+257\nT 857 \nX 858 \n---\nQ 761+292\nT 1053\nX 1054\n---\nQ 830+597\nT 1427\nX 1436\n---\nQ 26+838 \nT 864 \nO 864 \n---\nQ 143+93 \nT 236 \nO 236 \n---\n검증 정확도 70.660%\n| 에폭 24 |  반복 1 / 351 | 시간 0[s] | 손실 0.18\n| 에폭 24 |  반복 21 / 351 | 시간 1[s] | 손실 0.18\n| 에폭 24 |  반복 41 / 351 | 시간 2[s] | 손실 0.18\n| 에폭 24 |  반복 61 / 351 | 시간 3[s] | 손실 0.18\n| 에폭 24 |  반복 81 / 351 | 시간 4[s] | 손실 0.17\n| 에폭 24 |  반복 101 / 351 | 시간 6[s] | 손실 0.18\n| 에폭 24 |  반복 121 / 351 | 시간 7[s] | 손실 0.17\n| 에폭 24 |  반복 141 / 351 | 시간 8[s] | 손실 0.17\n| 에폭 24 |  반복 161 / 351 | 시간 9[s] | 손실 0.17\n| 에폭 24 |  반복 181 / 351 | 시간 10[s] | 손실 0.18\n| 에폭 24 |  반복 201 / 351 | 시간 11[s] | 손실 0.18\n| 에폭 24 |  반복 221 / 351 | 시간 13[s] | 손실 0.17\n| 에폭 24 |  반복 241 / 351 | 시간 14[s] | 손실 0.18\n| 에폭 24 |  반복 261 / 351 | 시간 15[s] | 손실 0.18\n| 에폭 24 |  반복 281 / 351 | 시간 16[s] | 손실 0.17\n| 에폭 24 |  반복 301 / 351 | 시간 18[s] | 손실 0.17\n| 에폭 24 |  반복 321 / 351 | 시간 19[s] | 손실 0.16\n| 에폭 24 |  반복 341 / 351 | 시간 20[s] | 손실 0.16\nQ 77+85  \nT 162 \nO 162 \n---\nQ 975+164\nT 1139\nO 1139\n---\nQ 582+84 \nT 666 \nO 666 \n---\nQ 8+155  \nT 163 \nX 164 \n---\nQ 367+55 \nT 422 \nO 422 \n---\nQ 600+257\nT 857 \nO 857 \n---\nQ 761+292\nT 1053\nX 1054\n---\nQ 830+597\nT 1427\nX 1430\n---\nQ 26+838 \nT 864 \nO 864 \n---\nQ 143+93 \nT 236 \nO 236 \n---\n검증 정확도 76.460%\n| 에폭 25 |  반복 1 / 351 | 시간 0[s] | 손실 0.14\n| 에폭 25 |  반복 21 / 351 | 시간 1[s] | 손실 0.15\n| 에폭 25 |  반복 41 / 351 | 시간 2[s] | 손실 0.15\n| 에폭 25 |  반복 61 / 351 | 시간 3[s] | 손실 0.14\n| 에폭 25 |  반복 81 / 351 | 시간 5[s] | 손실 0.14\n| 에폭 25 |  반복 101 / 351 | 시간 6[s] | 손실 0.14\n| 에폭 25 |  반복 121 / 351 | 시간 7[s] | 손실 0.14\n| 에폭 25 |  반복 141 / 351 | 시간 9[s] | 손실 0.14\n| 에폭 25 |  반복 161 / 351 | 시간 10[s] | 손실 0.13\n| 에폭 25 |  반복 181 / 351 | 시간 11[s] | 손실 0.14\n| 에폭 25 |  반복 201 / 351 | 시간 12[s] | 손실 0.15\n| 에폭 25 |  반복 221 / 351 | 시간 14[s] | 손실 0.14\n| 에폭 25 |  반복 241 / 351 | 시간 15[s] | 손실 0.14\n| 에폭 25 |  반복 261 / 351 | 시간 16[s] | 손실 0.14\n| 에폭 25 |  반복 281 / 351 | 시간 18[s] | 손실 0.14\n| 에폭 25 |  반복 301 / 351 | 시간 19[s] | 손실 0.13\n| 에폭 25 |  반복 321 / 351 | 시간 20[s] | 손실 0.13\n| 에폭 25 |  반복 341 / 351 | 시간 21[s] | 손실 0.12\nQ 77+85  \nT 162 \nO 162 \n---\nQ 975+164\nT 1139\nO 1139\n---\nQ 582+84 \nT 666 \nO 666 \n---\nQ 8+155  \nT 163 \nO 163 \n---\nQ 367+55 \nT 422 \nO 422 \n---\nQ 600+257\nT 857 \nO 857 \n---\nQ 761+292\nT 1053\nO 1053\n---\nQ 830+597\nT 1427\nX 1430\n---\nQ 26+838 \nT 864 \nO 864 \n---\nQ 143+93 \nT 236 \nO 236 \n---\n검증 정확도 81.580%\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"265.995469pt\" version=\"1.1\" viewBox=\"0 0 389.607386 265.995469\" width=\"389.607386pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 265.995469 \r\nL 389.607386 265.995469 \r\nL 389.607386 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 43.78125 228.439219 \r\nL 378.58125 228.439219 \r\nL 378.58125 10.999219 \r\nL 43.78125 10.999219 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"ma52fe2b67e\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"58.999432\" xlink:href=\"#ma52fe2b67e\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g transform=\"translate(55.818182 243.037656)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"122.408523\" xlink:href=\"#ma52fe2b67e\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 5 -->\r\n      <defs>\r\n       <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n      </defs>\r\n      <g transform=\"translate(119.227273 243.037656)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"185.817614\" xlink:href=\"#ma52fe2b67e\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 10 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(179.455114 243.037656)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"249.226705\" xlink:href=\"#ma52fe2b67e\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 15 -->\r\n      <g transform=\"translate(242.864205 243.037656)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"312.635795\" xlink:href=\"#ma52fe2b67e\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 20 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(306.273295 243.037656)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"376.044886\" xlink:href=\"#ma52fe2b67e\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 25 -->\r\n      <g transform=\"translate(369.682386 243.037656)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_7\">\r\n     <!-- 에폭 -->\r\n     <defs>\r\n      <path d=\"M 4.984375 -17.671875 \r\nL 4.984375 70.515625 \r\nL 54.984375 70.515625 \r\nL 54.984375 -17.671875 \r\nz\r\nM 10.59375 -12.109375 \r\nL 49.421875 -12.109375 \r\nL 49.421875 64.890625 \r\nL 10.59375 64.890625 \r\nz\r\n\" id=\"DejaVuSans-50640\"/>\r\n      <path d=\"M 4.984375 -17.671875 \r\nL 4.984375 70.515625 \r\nL 54.984375 70.515625 \r\nL 54.984375 -17.671875 \r\nz\r\nM 10.59375 -12.109375 \r\nL 49.421875 -12.109375 \r\nL 49.421875 64.890625 \r\nL 10.59375 64.890625 \r\nz\r\n\" id=\"DejaVuSans-54253\"/>\r\n     </defs>\r\n     <g transform=\"translate(205.179688 256.715781)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-50640\"/>\r\n      <use x=\"60.009766\" xlink:href=\"#DejaVuSans-54253\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"mbd6c554a82\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#mbd6c554a82\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0.0 -->\r\n      <defs>\r\n       <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n      </defs>\r\n      <g transform=\"translate(20.878125 232.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#mbd6c554a82\" y=\"184.951219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 0.2 -->\r\n      <g transform=\"translate(20.878125 188.750437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#mbd6c554a82\" y=\"141.463219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0.4 -->\r\n      <defs>\r\n       <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n      </defs>\r\n      <g transform=\"translate(20.878125 145.262437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#mbd6c554a82\" y=\"97.975219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 0.6 -->\r\n      <defs>\r\n       <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n      </defs>\r\n      <g transform=\"translate(20.878125 101.774437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#mbd6c554a82\" y=\"54.487219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 0.8 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n      </defs>\r\n      <g transform=\"translate(20.878125 58.286437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#mbd6c554a82\" y=\"10.999219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 1.0 -->\r\n      <g transform=\"translate(20.878125 14.798437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_14\">\r\n     <!-- 정확도 -->\r\n     <defs>\r\n      <path d=\"M 4.984375 -17.671875 \r\nL 4.984375 70.515625 \r\nL 54.984375 70.515625 \r\nL 54.984375 -17.671875 \r\nz\r\nM 10.59375 -12.109375 \r\nL 49.421875 -12.109375 \r\nL 49.421875 64.890625 \r\nL 10.59375 64.890625 \r\nz\r\n\" id=\"DejaVuSans-51221\"/>\r\n      <path d=\"M 4.984375 -17.671875 \r\nL 4.984375 70.515625 \r\nL 54.984375 70.515625 \r\nL 54.984375 -17.671875 \r\nz\r\nM 10.59375 -12.109375 \r\nL 49.421875 -12.109375 \r\nL 49.421875 64.890625 \r\nL 10.59375 64.890625 \r\nz\r\n\" id=\"DejaVuSans-54869\"/>\r\n      <path d=\"M 4.984375 -17.671875 \r\nL 4.984375 70.515625 \r\nL 54.984375 70.515625 \r\nL 54.984375 -17.671875 \r\nz\r\nM 10.59375 -12.109375 \r\nL 49.421875 -12.109375 \r\nL 49.421875 64.890625 \r\nL 10.59375 64.890625 \r\nz\r\n\" id=\"DejaVuSans-46020\"/>\r\n     </defs>\r\n     <g transform=\"translate(14.798438 128.721562)rotate(-90)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-51221\"/>\r\n      <use x=\"60.009766\" xlink:href=\"#DejaVuSans-54869\"/>\r\n      <use x=\"120.019531\" xlink:href=\"#DejaVuSans-46020\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_13\">\r\n    <path clip-path=\"url(#p1400a9bff5)\" d=\"M 58.999432 228.047827 \r\nL 71.68125 227.960851 \r\nL 84.363068 225.395059 \r\nL 97.044886 221.916019 \r\nL 109.726705 217.741171 \r\nL 122.408523 210.435187 \r\nL 135.090341 208.391251 \r\nL 147.772159 201.302707 \r\nL 160.453977 193.387891 \r\nL 173.135795 187.430035 \r\nL 185.817614 191.909299 \r\nL 198.499432 182.341939 \r\nL 211.18125 171.600403 \r\nL 223.863068 177.210355 \r\nL 236.544886 169.208563 \r\nL 249.226705 160.641427 \r\nL 261.908523 148.551763 \r\nL 274.590341 145.290163 \r\nL 287.272159 137.418835 \r\nL 299.953977 126.068467 \r\nL 312.635795 108.934195 \r\nL 325.317614 93.408979 \r\nL 337.999432 74.796115 \r\nL 350.68125 62.184595 \r\nL 363.363068 51.051667 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    <defs>\r\n     <path d=\"M 0 3 \r\nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \r\nC 2.683901 1.55874 3 0.795609 3 0 \r\nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \r\nC 1.55874 -2.683901 0.795609 -3 0 -3 \r\nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \r\nC -2.683901 -1.55874 -3 -0.795609 -3 0 \r\nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \r\nC -1.55874 2.683901 -0.795609 3 0 3 \r\nz\r\n\" id=\"m3821a84420\" style=\"stroke:#1f77b4;\"/>\r\n    </defs>\r\n    <g clip-path=\"url(#p1400a9bff5)\">\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"58.999432\" xlink:href=\"#m3821a84420\" y=\"228.047827\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"71.68125\" xlink:href=\"#m3821a84420\" y=\"227.960851\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"84.363068\" xlink:href=\"#m3821a84420\" y=\"225.395059\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"97.044886\" xlink:href=\"#m3821a84420\" y=\"221.916019\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"109.726705\" xlink:href=\"#m3821a84420\" y=\"217.741171\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"122.408523\" xlink:href=\"#m3821a84420\" y=\"210.435187\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"135.090341\" xlink:href=\"#m3821a84420\" y=\"208.391251\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"147.772159\" xlink:href=\"#m3821a84420\" y=\"201.302707\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"160.453977\" xlink:href=\"#m3821a84420\" y=\"193.387891\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"173.135795\" xlink:href=\"#m3821a84420\" y=\"187.430035\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"185.817614\" xlink:href=\"#m3821a84420\" y=\"191.909299\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"198.499432\" xlink:href=\"#m3821a84420\" y=\"182.341939\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"211.18125\" xlink:href=\"#m3821a84420\" y=\"171.600403\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"223.863068\" xlink:href=\"#m3821a84420\" y=\"177.210355\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"236.544886\" xlink:href=\"#m3821a84420\" y=\"169.208563\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"249.226705\" xlink:href=\"#m3821a84420\" y=\"160.641427\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"261.908523\" xlink:href=\"#m3821a84420\" y=\"148.551763\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"274.590341\" xlink:href=\"#m3821a84420\" y=\"145.290163\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"287.272159\" xlink:href=\"#m3821a84420\" y=\"137.418835\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"299.953977\" xlink:href=\"#m3821a84420\" y=\"126.068467\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"312.635795\" xlink:href=\"#m3821a84420\" y=\"108.934195\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"325.317614\" xlink:href=\"#m3821a84420\" y=\"93.408979\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"337.999432\" xlink:href=\"#m3821a84420\" y=\"74.796115\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"350.68125\" xlink:href=\"#m3821a84420\" y=\"62.184595\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"363.363068\" xlink:href=\"#m3821a84420\" y=\"51.051667\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 43.78125 228.439219 \r\nL 43.78125 10.999219 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 378.58125 228.439219 \r\nL 378.58125 10.999219 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 43.78125 228.439219 \r\nL 378.58125 228.439219 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 43.78125 10.999219 \r\nL 378.58125 10.999219 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p1400a9bff5\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"43.78125\" y=\"10.999219\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from common.optimizer import Adam\n",
    "from common.trainer import Trainer\n",
    "from common.util import eval_seq2seq\n",
    "\n",
    "# 데이터셋 읽기\n",
    "(x_train, t_train), (x_test, t_test) = sequence.load_data('addition.txt')\n",
    "char_to_id, id_to_char = sequence.get_vocab()\n",
    "\n",
    "# 입력 데이터 반전\n",
    "is_reverse = False  # True\n",
    "if is_reverse:\n",
    "    x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "vocab_size = len(char_to_id)\n",
    "wordvec_size = 16\n",
    "hidden_size = 128\n",
    "batch_size = 128\n",
    "max_epoch = 25\n",
    "max_grad = 5.0\n",
    "\n",
    "# 모델 / 옵티마이저 / 트레이너 생성\n",
    "# model = Seq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "model = PeekySeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "acc_list = []\n",
    "for epoch in range(max_epoch):\n",
    "    trainer.fit(x_train, t_train, max_epoch=1, batch_size=batch_size, max_grad=max_grad)\n",
    "\n",
    "    correct_num = 0\n",
    "    for i in range(len(x_test)):\n",
    "        question, correct = x_test[[i]], t_test[[i]]\n",
    "        verbose = i < 10\n",
    "        correct_num += eval_seq2seq(model, question, correct, id_to_char, verbose, is_reverse)\n",
    "    acc = float(correct_num) / len(x_test)\n",
    "    acc_list.append(acc)\n",
    "    print('검증 정확도 %.3f%%' % (acc * 100))\n",
    "\n",
    "# 그래프 그리기\n",
    "x = np.arange(len(acc_list))\n",
    "plt.plot(x, acc_list, marker='o')\n",
    "plt.xlabel('에폭')\n",
    "plt.ylabel('정확도')\n",
    "plt.ylim(0, 1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}