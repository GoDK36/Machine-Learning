{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597715332515",
   "display_name": "Python 3.7.7 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RnnlmGen (문장 생성)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.functions import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.time_layers import *\n",
    "import pickle\n",
    "\n",
    "class Rnnlm:\n",
    "    def __init__(self, vocab_size=10000, wordvec_size=100, hidden_size=100):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        # 가중치 초기화\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = [\n",
    "            TimeEmbedding(embed_W),\n",
    "            TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True),\n",
    "            TimeAffine(affine_W, affine_b)\n",
    "        ]\n",
    "        self.loss_layer = TimeSoftmaxWithLoss()\n",
    "        self.lstm_layer = self.layers[1]\n",
    "\n",
    "        # 모든 가중치와 기울기를 리스트에 모으기\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def predict(self, xs):\n",
    "        for layer in self.layers:\n",
    "            xs = layer.forward(xs)\n",
    "        return xs\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        score = self.predict(xs)\n",
    "        loss = self.loss_layer.forward(score, ts)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.lstm_layer.reset_state()\n",
    "\n",
    "    def save_params(self, file_name=\"Rnnlm.pkl\"):\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(self.params, f)\n",
    "    \n",
    "    def load_params(self, file_name=\"Rnnlm.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            self.params = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.np import *\n",
    "from common.base_model import BaseModel\n",
    "\n",
    "class BetterRnnlm(BaseModel):\n",
    "    def __init__(self, vocab_size=10000, wordvec_size=650, hidden_size=650, dropout_ratio=0.5):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        # 가중치 초기화\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx1 = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh1 = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b1 = np.zeros(4 * H).astype('f')\n",
    "        lstm_Wx2 = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_Wh2 = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b2 = np.zeros(4 * H).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = [\n",
    "            TimeEmbedding(embed_W),\n",
    "            TimeDropout(dropout_ratio),\n",
    "            TimeLSTM(lstm_Wx1, lstm_Wh1, lstm_b1, stateful=True),\n",
    "            TimeDropout(dropout_ratio),\n",
    "            TimeLSTM(lstm_Wx2, lstm_Wh2, lstm_b2, stateful=True),\n",
    "            TimeDropout(dropout_ratio),\n",
    "            TimeAffine(embed_W.T, affine_b)                             # 가중치 공유\n",
    "        ]\n",
    "        self.loss_layer = TimeSoftmaxWithLoss()\n",
    "        self.lstm_layers = [self.layers[2], self.layers[4]]\n",
    "        self.drop_layers = [self.layers[1], self.layers[3], self.layers[5]]\n",
    "\n",
    "        # 모든 가중치와 기울기를 리스트에 모으기\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def predict(self, xs, train_flg=False):\n",
    "        for layer in self.drop_layers:\n",
    "            layer.train_flg = train_flg\n",
    "        for layer in self.layers:\n",
    "            xs = layer.forward(xs)\n",
    "        return xs\n",
    "\n",
    "    def forward(self, xs, ts, train_flg=True):\n",
    "        score = self.predict(xs, train_flg)\n",
    "        loss = self.loss_layer(score, ts)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "\n",
    "    def reset_state(self):\n",
    "        for layer in self.lstm_layers:\n",
    "            layer.reset_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RnnlmGen(Rnnlm):\n",
    "    def generate(self, start_id, skip_ids=None, sample_size=100):\n",
    "        word_ids = [start_id]\n",
    "\n",
    "        x = start_id\n",
    "        while len(word_ids) < sample_size:\n",
    "            x = np.array(x).reshape(1, 1)\n",
    "            score = self.predict(x)\n",
    "            p = softmax(score.flatten())\n",
    "\n",
    "            sampled = np.random.choice(len(p), size=1, p=p)\n",
    "            if (skip_ids is None) or (sampled not in skip_ids):\n",
    "                x = sampled\n",
    "                word_ids.append(int(x))\n",
    "        \n",
    "        return word_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetterRnnlmGen(BetterRnnlm):\n",
    "    def generate(self, start_id, skip_ids=None, sample_size=100):\n",
    "        word_ids = [start_id]\n",
    "\n",
    "        x = start_id\n",
    "        while len(word_ids) < sample_size:\n",
    "            x = np.array(x).reshape(1, 1)\n",
    "            score = self.predict(x).flatten()\n",
    "            p = softmax(score).flatten()\n",
    "\n",
    "            sampled = np.random.choice(len(p), size=1, p=p)\n",
    "            if (skip_ids is None) or (sampled not in skip_ids):\n",
    "                x = sampled\n",
    "                word_ids.append(int(x))\n",
    "\n",
    "        return word_ids\n",
    "\n",
    "    def get_state(self):\n",
    "        states = []\n",
    "        for layer in self.lstm_layers:\n",
    "            states.append((layer.h, layer.c))\n",
    "        return states\n",
    "\n",
    "    def set_state(self, states):\n",
    "        for layer, state in zip(self.lstm_layers, states):\n",
    "            layer.set_state(*state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문장 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "you 'll they want resort solo music says mr. roth.\n one of the stick we could 'll know e. singer gerald.\n it has no casual problem this will be a bearing the drug and no other that comes by the ages of the century.\n the ex-dividend included the san francisco bay area series stock-index floor volume as five others.\n some causes also lagging in part more get said ms. macdonald will aggressively be sir alan jenrette.\n the balance had enough extension to a nam in ideal 2-for-1.\n but arbitragers said that the new matter\n--------------------------------------------------\nthe meaning of life is to place the living of the domestic electronics and minnesota.\n reins show the company report.\n the british engineering franchise is expected to be installed tomorrow because it people sees proposals for ford.\n however at several other businessmen have sent wrongdoing on news.\n campeau corp. the brazilian petroleum and industrial company also in the u.k. hong kong is n't on the progress of the full reactions.\n the contract is barely weakened by the dollar slipping to stocks an ounce.\n contract coastal said gorbachev has had promised a sale in utilities that should the drop\n"
    }
   ],
   "source": [
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "corpus_size = len(corpus)\n",
    "\n",
    "\n",
    "model = BetterRnnlmGen()\n",
    "model.load_params(r\"E:\\Develop\\Machine-Learning\\밑바닥부터 시작하는 딥러닝2\\BetterRnnlm.pkl\")\n",
    "\n",
    "# start 문자와 skip 문자 설정\n",
    "start_word = 'you'\n",
    "start_id = word_to_id[start_word]\n",
    "skip_words = ['N', '<unk>', '$']\n",
    "skip_ids = [word_to_id[w] for w in skip_words]\n",
    "# 문장 생성\n",
    "word_ids = model.generate(start_id, skip_ids)\n",
    "txt = ' '.join([id_to_word[i] for i in word_ids])\n",
    "txt = txt.replace(' <eos>', '.\\n')\n",
    "\n",
    "print(txt)\n",
    "\n",
    "\n",
    "model.reset_state()\n",
    "\n",
    "start_words = 'the meaning of life is'\n",
    "start_ids = [word_to_id[w] for w in start_words.split(' ')]\n",
    "\n",
    "for x in start_ids[:-1]:\n",
    "    x = np.array(x).reshape(1, 1)\n",
    "    model.predict(x)\n",
    "\n",
    "word_ids = model.generate(start_ids[-1], skip_ids)\n",
    "word_ids = start_ids[:-1] + word_ids\n",
    "txt = ' '.join([id_to_word[i] for i in word_ids])\n",
    "txt = txt.replace(' <eos>', '.\\n')\n",
    "print('-' * 50)\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "you h&r ferdinand wedtech turbulence attracting theirs wholly banks rep fundamentals bursts mortality 500-stock colors tax-exempt lincoln owns paribas luxury-car devices richter cypress aging rates commonwealth unwilling supplied dubious london suburban structured pollution lifting reoffered rumored match maneuver steal fruit coda connecting marvin veteran evening voters anti-nuclear fear pleased generous aggressive distributor philippines homelessness crucial attributed pipeline friendship carla byrd stay offshore condition supercomputer municipals jumped floating eight uncertain jewelers nicholas majority bus n.j. painted users alberta subsequently exercisable activity taylor transplant noriega quina larsen torrijos gangs probing contemporary outsiders simple marginal wildly salinger bank-holding ddb depositary unavailable robertson annual\n"
    }
   ],
   "source": [
    "from dataset import ptb\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "corpus_size = len(corpus)\n",
    "\n",
    "model = RnnlmGen()\n",
    "model.load_params(r\"E:\\Develop\\Machine-Learning\\밑바닥부터 시작하는 딥러닝2\\BetterRnnlm.pkl\")        # 가중치 가져오기\n",
    "\n",
    "# 시작문자와 넘어갈 문자 설정\n",
    "start_word = 'you'\n",
    "start_id = word_to_id[start_word]\n",
    "skip_words = ['N', '<unk>', '$']\n",
    "skip_ids = [word_to_id[w] for w in skip_words]\n",
    "\n",
    "# 문장 생성\n",
    "word_ids = model.generate(start_id, skip_ids)\n",
    "txt = ' '.join([id_to_word[i] for i in word_ids])\n",
    "txt = txt.replace(' <eos>', '.\\n')\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(45000, 7) (45000, 5)\n(5000, 7) (5000, 5)\n[[ 3  0  2 ...  0 11  5]\n [ 4  0  9 ...  8  8 10]\n [ 1  1  2 ...  9  0  5]\n ...\n [ 3  1 10 ...  8  0  3]\n [ 1  2  8 ...  0  5  5]\n [ 8  2  4 ... 10  5  5]]\n[[ 6  0 11  7  5]\n [ 6  3 10 10  5]\n [ 6  3  1  3  5]\n ...\n [ 6  7 11  9  5]\n [ 6  8  3  3  5]\n [ 6  4  1  4  5]]\n"
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, list found",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-7b62c132e22c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid_to_char\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid_to_char\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mt_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: sequence item 0: expected str instance, list found"
     ]
    }
   ],
   "source": [
    "from dataset import sequence\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    sequence.load_data('addition.txt', seed=1984)\n",
    "char_to_id, id_to_char = sequence.get_vocab()\n",
    "\n",
    "print(x_train.shape, t_train.shape)\n",
    "print(x_test.shape, t_test.shape)\n",
    "\n",
    "print(x_train)\n",
    "print(t_train)\n",
    "\n",
    "print(''.join([id_to_char[c]] for c in x_train[0]))\n",
    "print(''.join([id_to_char[c]] for c in t_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "\n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=False)\n",
    "\n",
    "        self.params = self.embed.params + self.lstm.params\n",
    "        self.grads = self.embed.grads + self.lstm.grads\n",
    "        self.hs = None\n",
    "        \n",
    "\n",
    "    def forward(self, xs):\n",
    "        xs = self.embed.forward(xs)\n",
    "        hs = self.lstm.forward(xs)\n",
    "        self.hs = hs\n",
    "\n",
    "        return hs[:, -1, :]\n",
    "\n",
    "    def backward(self, dh):\n",
    "        dhs = np.zeros_like(self.hs)\n",
    "        dhs[:, -1, :] = dh\n",
    "\n",
    "        dout = self.lstm.backward(dhs)\n",
    "        dout = self.embed.backward(dout)\n",
    "\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b)\n",
    "        self.affine = TimeAffine(affine_W, affine_b)\n",
    "        \n",
    "        self.params, self.grads = [], []\n",
    "\n",
    "        for layer in (self.embed, self.lstm, self.affine):\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def forward(self, xs, h):\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        out = self.embed.forward(xs)\n",
    "        out = self.lstm.forward(out)\n",
    "        score = self.affine.forward(out)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def backward(self, dscore):\n",
    "        dout = self.affine.backward(dscore)\n",
    "        dout = self.lstm.backward(dout)\n",
    "        dout = self.embed.backward(dout)\n",
    "        dh = self.lstm.dh\n",
    "\n",
    "        return dh\n",
    "\n",
    "    def generate(self, h, start_id, sample_size):\n",
    "        sampled = []\n",
    "        sample_id = start_id\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        for _ in range(sample_size):\n",
    "            x = np.array(sample_id).reshape((1, 1))\n",
    "            out = self.embed.forward(x)\n",
    "            out = self.lstm.forward(out)\n",
    "            score = self.affine.forward(out)\n",
    "\n",
    "            sample_id = np.argmax(score.flatten())\n",
    "            sampled.append(int(sample_id))\n",
    "\n",
    "        return sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2seq(BaseModel):\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        self.encoder = Encoder(V, D, H)\n",
    "        self.decoder = Decoder(V, D, H)\n",
    "        self.softmax = TimeSoftmaxWithLoss()\n",
    "\n",
    "        self.params = self.encoder.params + self.decoder.params\n",
    "        self.grads = self.encoder.grads + self.decoder.grads\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        decoder_xs, decoder_ts = ts[:, :-1], ts[:, 1:]\n",
    "        h = self.encoder.forward(xs)\n",
    "        score = self.decoder.forward(decoder_xs, h)\n",
    "        loss = self.softmax.forward(score, decoder_ts)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        dout = self.softmax.backward(dout)\n",
    "        dh = self.decoder.backward(dout)\n",
    "        dout = self.encoder.backward(dh)\n",
    "\n",
    "        return dout\n",
    "\n",
    "    def generate(self, xs, start_id, sample_size):\n",
    "        h = self.encoder.forward(xs)\n",
    "        sampled = self.decoder.generate(h, start_id, sample_size)\n",
    "\n",
    "        return sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.time_layers import *\n",
    "\n",
    "\n",
    "class PeekyDecoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx = (rn(H + D, 4 * H) / np.sqrt(H + D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "        affine_W = (rn(H + H, V) / np.sqrt(H + H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
    "        self.affine = TimeAffine(affine_W, affine_b)\n",
    "\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in (self.embed, self.lstm, self.affine):\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, xs, h):\n",
    "        N, T = xs.shape\n",
    "        N, H = h.shape\n",
    "\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        out = self.embed.forward(xs)\n",
    "        hs = np.repeat(h, T, axis=0).reshape(N, T, H)\n",
    "        out = np.concatenate((hs, out), axis=2)\n",
    "\n",
    "        out = self.lstm.forward(out)\n",
    "        out = np.concatenate((hs, out), axis=2)\n",
    "\n",
    "        score = self.affine.forward(out)\n",
    "        self.cache = H\n",
    "        return score\n",
    "\n",
    "    def backward(self, dscore):\n",
    "        H = self.cache\n",
    "\n",
    "        dout = self.affine.backward(dscore)\n",
    "        dout, dhs0 = dout[:, :, H:], dout[:, :, :H]\n",
    "        dout = self.lstm.backward(dout)\n",
    "        dembed, dhs1 = dout[:, :, H:], dout[:, :, :H]\n",
    "        self.embed.backward(dembed)\n",
    "\n",
    "        dhs = dhs0 + dhs1\n",
    "        dh = self.lstm.dh + np.sum(dhs, axis=1)\n",
    "        return dh\n",
    "\n",
    "    def generate(self, h, start_id, sample_size):\n",
    "        sampled = []\n",
    "        char_id = start_id\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        H = h.shape[1]\n",
    "        peeky_h = h.reshape(1, 1, H)\n",
    "        for _ in range(sample_size):\n",
    "            x = np.array([char_id]).reshape((1, 1))\n",
    "            out = self.embed.forward(x)\n",
    "\n",
    "            out = np.concatenate((peeky_h, out), axis=2)\n",
    "            out = self.lstm.forward(out)\n",
    "            out = np.concatenate((peeky_h, out), axis=2)\n",
    "            score = self.affine.forward(out)\n",
    "\n",
    "            char_id = np.argmax(score.flatten())\n",
    "            sampled.append(char_id)\n",
    "\n",
    "        return sampled\n",
    "\n",
    "\n",
    "class PeekySeq2seq(Seq2seq):\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        self.encoder = Encoder(V, D, H)\n",
    "        self.decoder = PeekyDecoder(V, D, H)\n",
    "        self.softmax = TimeSoftmaxWithLoss()\n",
    "\n",
    "        self.params = self.encoder.params + self.decoder.params\n",
    "        self.grads = self.encoder.grads + self.decoder.grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "| 손실 1.89\n| 에폭 1 |  반복 241 / 351 | 시간 13[s] | 손실 1.89\n| 에폭 1 |  반복 261 / 351 | 시간 14[s] | 손실 1.87\n| 에폭 1 |  반복 281 / 351 | 시간 15[s] | 손실 1.88\n| 에폭 1 |  반복 301 / 351 | 시간 16[s] | 손실 1.86\n| 에폭 1 |  반복 321 / 351 | 시간 17[s] | 손실 1.87\n| 에폭 1 |  반복 341 / 351 | 시간 19[s] | 손실 1.85\nQ 77+85  \nT 162 \nX 1111\n---\nQ 975+164\nT 1139\nX 1111\n---\nQ 582+84 \nT 666 \nX 1111\n---\nQ 8+155  \nT 163 \nX 1111\n---\nQ 367+55 \nT 422 \nX 1111\n---\nQ 600+257\nT 857 \nX 1111\n---\nQ 761+292\nT 1053\nX 1111\n---\nQ 830+597\nT 1427\nX 1111\n---\nQ 26+838 \nT 864 \nX 1111\n---\nQ 143+93 \nT 236 \nX 1111\n---\n검증 정확도 0.020%\n| 에폭 2 |  반복 1 / 351 | 시간 0[s] | 손실 1.85\n| 에폭 2 |  반복 21 / 351 | 시간 1[s] | 손실 1.85\n| 에폭 2 |  반복 41 / 351 | 시간 2[s] | 손실 1.85\n| 에폭 2 |  반복 61 / 351 | 시간 3[s] | 손실 1.84\n| 에폭 2 |  반복 81 / 351 | 시간 4[s] | 손실 1.85\n| 에폭 2 |  반복 101 / 351 | 시간 5[s] | 손실 1.86\n| 에폭 2 |  반복 121 / 351 | 시간 6[s] | 손실 1.86\n| 에폭 2 |  반복 141 / 351 | 시간 7[s] | 손실 1.87\n| 에폭 2 |  반복 161 / 351 | 시간 8[s] | 손실 1.85\n| 에폭 2 |  반복 181 / 351 | 시간 9[s] | 손실 1.85\n| 에폭 2 |  반복 201 / 351 | 시간 10[s] | 손실 1.84\n| 에폭 2 |  반복 221 / 351 | 시간 11[s] | 손실 1.84\n| 에폭 2 |  반복 241 / 351 | 시간 12[s] | 손실 1.84\n| 에폭 2 |  반복 261 / 351 | 시간 13[s] | 손실 1.83\n| 에폭 2 |  반복 281 / 351 | 시간 14[s] | 손실 1.84\n| 에폭 2 |  반복 301 / 351 | 시간 15[s] | 손실 1.84\n| 에폭 2 |  반복 321 / 351 | 시간 16[s] | 손실 1.84\n| 에폭 2 |  반복 341 / 351 | 시간 17[s] | 손실 1.84\nQ 77+85  \nT 162 \nX 1111\n---\nQ 975+164\nT 1139\nX 1111\n---\nQ 582+84 \nT 666 \nX 1111\n---\nQ 8+155  \nT 163 \nX 1111\n---\nQ 367+55 \nT 422 \nX 1111\n---\nQ 600+257\nT 857 \nX 1111\n---\nQ 761+292\nT 1053\nX 1111\n---\nQ 830+597\nT 1427\nX 1111\n---\nQ 26+838 \nT 864 \nX 1111\n---\nQ 143+93 \nT 236 \nX 1111\n---\n검증 정확도 0.020%\n| 에폭 3 |  반복 1 / 351 | 시간 0[s] | 손실 1.80\n| 에폭 3 |  반복 21 / 351 | 시간 1[s] | 손실 1.84\n| 에폭 3 |  반복 41 / 351 | 시간 2[s] | 손실 1.84\n| 에폭 3 |  반복 61 / 351 | 시간 3[s] | 손실 1.83\n| 에폭 3 |  반복 81 / 351 | 시간 4[s] | 손실 1.83\n| 에폭 3 |  반복 101 / 351 | 시간 5[s] | 손실 1.83\n| 에폭 3 |  반복 121 / 351 | 시간 6[s] | 손실 1.83\n| 에폭 3 |  반복 141 / 351 | 시간 7[s] | 손실 1.84\n| 에폭 3 |  반복 161 / 351 | 시간 9[s] | 손실 1.83\n| 에폭 3 |  반복 181 / 351 | 시간 9[s] | 손실 1.84\n| 에폭 3 |  반복 201 / 351 | 시간 10[s] | 손실 1.83\n| 에폭 3 |  반복 221 / 351 | 시간 12[s] | 손실 1.83\n| 에폭 3 |  반복 241 / 351 | 시간 13[s] | 손실 1.83\n| 에폭 3 |  반복 261 / 351 | 시간 14[s] | 손실 1.84\n| 에폭 3 |  반복 281 / 351 | 시간 15[s] | 손실 1.84\n| 에폭 3 |  반복 301 / 351 | 시간 16[s] | 손실 1.83\n| 에폭 3 |  반복 321 / 351 | 시간 17[s] | 손실 1.84\n| 에폭 3 |  반복 341 / 351 | 시간 18[s] | 손실 1.84\nQ 77+85  \nT 162 \nX 1111\n---\nQ 975+164\nT 1139\nX 1111\n---\nQ 582+84 \nT 666 \nX 1111\n---\nQ 8+155  \nT 163 \nX 1111\n---\nQ 367+55 \nT 422 \nX 1111\n---\nQ 600+257\nT 857 \nX 1111\n---\nQ 761+292\nT 1053\nX 1111\n---\nQ 830+597\nT 1427\nX 1111\n---\nQ 26+838 \nT 864 \nX 1111\n---\nQ 143+93 \nT 236 \nX 1111\n---\n검증 정확도 0.020%\n| 에폭 4 |  반복 1 / 351 | 시간 0[s] | 손실 1.85\n| 에폭 4 |  반복 21 / 351 | 시간 1[s] | 손실 1.83\n| 에폭 4 |  반복 41 / 351 | 시간 2[s] | 손실 1.83\n| 에폭 4 |  반복 61 / 351 | 시간 3[s] | 손실 1.83\n| 에폭 4 |  반복 81 / 351 | 시간 4[s] | 손실 1.84\n| 에폭 4 |  반복 101 / 351 | 시간 5[s] | 손실 1.83\n| 에폭 4 |  반복 121 / 351 | 시간 7[s] | 손실 1.83\n| 에폭 4 |  반복 141 / 351 | 시간 8[s] | 손실 1.83\n| 에폭 4 |  반복 161 / 351 | 시간 9[s] | 손실 1.83\n| 에폭 4 |  반복 181 / 351 | 시간 10[s] | 손실 1.83\n| 에폭 4 |  반복 201 / 351 | 시간 11[s] | 손실 1.83\n| 에폭 4 |  반복 221 / 351 | 시간 12[s] | 손실 1.83\n| 에폭 4 |  반복 241 / 351 | 시간 13[s] | 손실 1.83\n| 에폭 4 |  반복 261 / 351 | 시간 15[s] | 손실 1.83\n| 에폭 4 |  반복 281 / 351 | 시간 16[s] | 손실 1.83\n| 에폭 4 |  반복 301 / 351 | 시간 17[s] | 손실 1.84\n| 에폭 4 |  반복 321 / 351 | 시간 19[s] | 손실 1.83\n| 에폭 4 |  반복 341 / 351 | 시간 20[s] | 손실 1.83\nQ 77+85  \nT 162 \nX 1111\n---\nQ 975+164\nT 1139\nX 1111\n---\nQ 582+84 \nT 666 \nX 1111\n---\nQ 8+155  \nT 163 \nX 1111\n---\nQ 367+55 \nT 422 \nX 1111\n---\nQ 600+257\nT 857 \nX 1111\n---\nQ 761+292\nT 1053\nX 1111\n---\nQ 830+597\nT 1427\nX 1111\n---\nQ 26+838 \nT 864 \nX 1111\n---\nQ 143+93 \nT 236 \nX 1111\n---\n검증 정확도 0.020%\n| 에폭 5 |  반복 1 / 351 | 시간 0[s] | 손실 1.83\n| 에폭 5 |  반복 21 / 351 | 시간 1[s] | 손실 1.83\n| 에폭 5 |  반복 41 / 351 | 시간 2[s] | 손실 1.83\n| 에폭 5 |  반복 61 / 351 | 시간 3[s] | 손실 1.83\n| 에폭 5 |  반복 81 / 351 | 시간 4[s] | 손실 1.83\n| 에폭 5 |  반복 101 / 351 | 시간 5[s] | 손실 1.83\n| 에폭 5 |  반복 121 / 351 | 시간 6[s] | 손실 1.83\n| 에폭 5 |  반복 141 / 351 | 시간 7[s] | 손실 1.83\n| 에폭 5 |  반복 161 / 351 | 시간 8[s] | 손실 1.83\n| 에폭 5 |  반복 181 / 351 | 시간 9[s] | 손실 1.83\n| 에폭 5 |  반복 201 / 351 | 시간 10[s] | 손실 1.83\n| 에폭 5 |  반복 221 / 351 | 시간 11[s] | 손실 1.83\n| 에폭 5 |  반복 241 / 351 | 시간 12[s] | 손실 1.83\n| 에폭 5 |  반복 261 / 351 | 시간 13[s] | 손실 1.83\n| 에폭 5 |  반복 281 / 351 | 시간 14[s] | 손실 1.83\n| 에폭 5 |  반복 301 / 351 | 시간 16[s] | 손실 1.83\n| 에폭 5 |  반복 321 / 351 | 시간 17[s] | 손실 1.83\n| 에폭 5 |  반복 341 / 351 | 시간 18[s] | 손실 1.84\nQ 77+85  \nT 162 \nX 1111\n---\nQ 975+164\nT 1139\nX 1111\n---\nQ 582+84 \nT 666 \nX 1111\n---\nQ 8+155  \nT 163 \nX 1111\n---\nQ 367+55 \nT 422 \nX 1111\n---\nQ 600+257\nT 857 \nX 1111\n---\nQ 761+292\nT 1053\nX 1111\n---\nQ 830+597\nT 1427\nX 1111\n---\nQ 26+838 \nT 864 \nX 1111\n---\nQ 143+93 \nT 236 \nX 1111\n---\n검증 정확도 0.020%\n| 에폭 6 |  반복 1 / 351 | 시간 0[s] | 손실 1.84\n| 에폭 6 |  반복 21 / 351 | 시간 1[s] | 손실 1.82\n| 에폭 6 |  반복 41 / 351 | 시간 2[s] | 손실 1.83\n| 에폭 6 |  반복 61 / 351 | 시간 3[s] | 손실 1.83\n| 에폭 6 |  반복 81 / 351 | 시간 4[s] | 손실 1.83\n| 에폭 6 |  반복 101 / 351 | 시간 5[s] | 손실 1.83\n| 에폭 6 |  반복 121 / 351 | 시간 6[s] | 손실 1.83\n| 에폭 6 |  반복 141 / 351 | 시간 7[s] | 손실 1.83\n| 에폭 6 |  반복 161 / 351 | 시간 8[s] | 손실 1.83\n| 에폭 6 |  반복 181 / 351 | 시간 10[s] | 손실 1.83\n| 에폭 6 |  반복 201 / 351 | 시간 11[s] | 손실 1.83\n| 에폭 6 |  반복 221 / 351 | 시간 12[s] | 손실 1.83\n| 에폭 6 |  반복 241 / 351 | 시간 13[s] | 손실 1.83\n| 에폭 6 |  반복 261 / 351 | 시간 14[s] | 손실 1.83\n| 에폭 6 |  반복 281 / 351 | 시간 15[s] | 손실 1.83\n| 에폭 6 |  반복 301 / 351 | 시간 16[s] | 손실 1.83\n| 에폭 6 |  반복 321 / 351 | 시간 17[s] | 손실 1.83\n| 에폭 6 |  반복 341 / 351 | 시간 18[s] | 손실 1.83\nQ 77+85  \nT 162 \nX 1111\n---\nQ 975+164\nT 1139\nX 1111\n---\nQ 582+84 \nT 666 \nX 1111\n---\nQ 8+155  \nT 163 \nX 1111\n---\nQ 367+55 \nT 422 \nX 1111\n---\nQ 600+257\nT 857 \nX 1111\n---\nQ 761+292\nT 1053\nX 1111\n---\nQ 830+597\nT 1427\nX 1111\n---\nQ 26+838 \nT 864 \nX 1111\n---\nQ 143+93 \nT 236 \nX 1111\n---\n검증 정확도 0.020%\n| 에폭 7 |  반복 1 / 351 | 시간 0[s] | 손실 1.84\n| 에폭 7 |  반복 21 / 351 | 시간 1[s] | 손실 1.83\n| 에폭 7 |  반복 41 / 351 | 시간 2[s] | 손실 1.83\n| 에폭 7 |  반복 61 / 351 | 시간 3[s] | 손실 1.83\n| 에폭 7 |  반복 81 / 351 | 시간 5[s] | 손실 1.82\n| 에폭 7 |  반복 101 / 351 | 시간 6[s] | 손실 1.83\n| 에폭 7 |  반복 121 / 351 | 시간 7[s] | 손실 1.83\n| 에폭 7 |  반복 141 / 351 | 시간 8[s] | 손실 1.83\n| 에폭 7 |  반복 161 / 351 | 시간 9[s] | 손실 1.83\n| 에폭 7 |  반복 181 / 351 | 시간 10[s] | 손실 1.83\n| 에폭 7 |  반복 201 / 351 | 시간 11[s] | 손실 1.83\n| 에폭 7 |  반복 221 / 351 | 시간 12[s] | 손실 1.83\n| 에폭 7 |  반복 241 / 351 | 시간 13[s] | 손실 1.82\n| 에폭 7 |  반복 261 / 351 | 시간 15[s] | 손실 1.83\n| 에폭 7 |  반복 281 / 351 | 시간 16[s] | 손실 1.82\n| 에폭 7 |  반복 301 / 351 | 시간 17[s] | 손실 1.83\n| 에폭 7 |  반복 321 / 351 | 시간 18[s] | 손실 1.83\n| 에폭 7 |  반복 341 / 351 | 시간 19[s] | 손실 1.83\nQ 77+85  \nT 162 \nX 1111\n---\nQ 975+164\nT 1139\nX 1111\n---\nQ 582+84 \nT 666 \nX 1111\n---\nQ 8+155  \nT 163 \nX 1111\n---\nQ 367+55 \nT 422 \nX 1111\n---\nQ 600+257\nT 857 \nX 1111\n---\nQ 761+292\nT 1053\nX 1111\n---\nQ 830+597\nT 1427\nX 1111\n---\nQ 26+838 \nT 864 \nX 1111\n---\nQ 143+93 \nT 236 \nX 1111\n---\n검증 정확도 0.020%\n| 에폭 8 |  반복 1 / 351 | 시간 0[s] | 손실 1.85\n| 에폭 8 |  반복 21 / 351 | 시간 1[s] | 손실 1.83\n| 에폭 8 |  반복 41 / 351 | 시간 2[s] | 손실 1.83\n| 에폭 8 |  반복 61 / 351 | 시간 3[s] | 손실 1.83\n| 에폭 8 |  반복 81 / 351 | 시간 4[s] | 손실 1.83\n| 에폭 8 |  반복 101 / 351 | 시간 5[s] | 손실 1.83\n| 에폭 8 |  반복 121 / 351 | 시간 6[s] | 손실 1.83\n| 에폭 8 |  반복 141 / 351 | 시간 7[s] | 손실 1.83\n| 에폭 8 |  반복 161 / 351 | 시간 8[s] | 손실 1.83\n| 에폭 8 |  반복 181 / 351 | 시간 9[s] | 손실 1.83\n| 에폭 8 |  반복 201 / 351 | 시간 10[s] | 손실 1.83\n| 에폭 8 |  반복 221 / 351 | 시간 11[s] | 손실 1.83\n| 에폭 8 |  반복 241 / 351 | 시간 12[s] | 손실 1.83\n| 에폭 8 |  반복 261 / 351 | 시간 14[s] | 손실 1.83\n| 에폭 8 |  반복 281 / 351 | 시간 15[s] | 손실 1.83\n| 에폭 8 |  반복 301 / 351 | 시간 16[s] | 손실 1.83\n| 에폭 8 |  반복 321 / 351 | 시간 17[s] | 손실 1.83\n| 에폭 8 |  반복 341 / 351 | 시간 18[s] | 손실 1.83\nQ 77+85  \nT 162 \nX 1111\n---\nQ 975+164\nT 1139\nX 1111\n---\nQ 582+84 \nT 666 \nX 1111\n---\nQ 8+155  \nT 163 \nX 1111\n---\nQ 367+55 \nT 422 \nX 1111\n---\nQ 600+257\nT 857 \nX 1111\n---\nQ 761+292\nT 1053\nX 1111\n---\nQ 830+597\nT 1427\nX 1111\n---\nQ 26+838 \nT 864 \nX 1111\n---\nQ 143+93 \nT 236 \nX 1111\n---\n검증 정확도 0.020%\n| 에폭 9 |  반복 1 / 351 | 시간 0[s] | 손실 1.81\n| 에폭 9 |  반복 21 / 351 | 시간 1[s] | 손실 1.83\n| 에폭 9 |  반복 41 / 351 | 시간 2[s] | 손실 1.83\n| 에폭 9 |  반복 61 / 351 | 시간 3[s] | 손실 1.83\n| 에폭 9 |  반복 81 / 351 | 시간 4[s] | 손실 1.83\n| 에폭 9 |  반복 101 / 351 | 시간 5[s] | 손실 1.83\n| 에폭 9 |  반복 121 / 351 | 시간 7[s] | 손실 1.83\n| 에폭 9 |  반복 141 / 351 | 시간 8[s] | 손실 1.83\n| 에폭 9 |  반복 161 / 351 | 시간 9[s] | 손실 1.83\n| 에폭 9 |  반복 181 / 351 | 시간 10[s] | 손실 1.83\n| 에폭 9 |  반복 201 / 351 | 시간 11[s] | 손실 1.83\n| 에폭 9 |  반복 221 / 351 | 시간 13[s] | 손실 1.83\n| 에폭 9 |  반복 241 / 351 | 시간 14[s] | 손실 1.83\n| 에폭 9 |  반복 261 / 351 | 시간 15[s] | 손실 1.83\n| 에폭 9 |  반복 281 / 351 | 시간 16[s] | 손실 1.83\n| 에폭 9 |  반복 301 / 351 | 시간 18[s] | 손실 1.83\n| 에폭 9 |  반복 321 / 351 | 시간 19[s] | 손실 1.83\n| 에폭 9 |  반복 341 / 351 | 시간 20[s] | 손실 1.82\nQ 77+85  \nT 162 \nX 1+1+\n---\nQ 975+164\nT 1139\nX 1+1+\n---\nQ 582+84 \nT 666 \nX 1+1+\n---\nQ 8+155  \nT 163 \nX 1+1+\n---\nQ 367+55 \nT 422 \nX 1+1+\n---\nQ 600+257\nT 857 \nX 1+1+\n---\nQ 761+292\nT 1053\nX 1+1+\n---\nQ 830+597\nT 1427\nX 1+1+\n---\nQ 26+838 \nT 864 \nX 1+1+\n---\nQ 143+93 \nT 236 \nX 1+1+\n---\n검증 정확도 0.000%\n| 에폭 10 |  반복 1 / 351 | 시간 0[s] | 손실 1.82\n| 에폭 10 |  반복 21 / 351 | 시간 1[s] | 손실 1.83\n| 에폭 10 |  반복 41 / 351 | 시간 2[s] | 손실 1.83\n| 에폭 10 |  반복 61 / 351 | 시간 3[s] | 손실 1.83\n| 에폭 10 |  반복 81 / 351 | 시간 4[s] | 손실 1.83\n| 에폭 10 |  반복 101 / 351 | 시간 5[s] | 손실 1.83\n| 에폭 10 |  반복 121 / 351 | 시간 7[s] | 손실 1.83\n| 에폭 10 |  반복 141 / 351 | 시간 8[s] | 손실 1.83\n| 에폭 10 |  반복 161 / 351 | 시간 9[s] | 손실 1.83\n| 에폭 10 |  반복 181 / 351 | 시간 10[s] | 손실 1.83\n| 에폭 10 |  반복 201 / 351 | 시간 11[s] | 손실 1.83\n| 에폭 10 |  반복 221 / 351 | 시간 12[s] | 손실 1.83\n| 에폭 10 |  반복 241 / 351 | 시간 13[s] | 손실 1.83\n| 에폭 10 |  반복 261 / 351 | 시간 14[s] | 손실 1.82\n| 에폭 10 |  반복 281 / 351 | 시간 15[s] | 손실 1.83\n| 에폭 10 |  반복 301 / 351 | 시간 16[s] | 손실 1.83\n| 에폭 10 |  반복 321 / 351 | 시간 17[s] | 손실 1.83\n| 에폭 10 |  반복 341 / 351 | 시간 18[s] | 손실 1.83\nQ 77+85  \nT 162 \nX 1+1+\n---\nQ 975+164\nT 1139\nX 1+1+\n---\nQ 582+84 \nT 666 \nX 1+1+\n---\nQ 8+155  \nT 163 \nX 1+1+\n---\nQ 367+55 \nT 422 \nX 1+1+\n---\nQ 600+257\nT 857 \nX 1+1+\n---\nQ 761+292\nT 1053\nX 1+1+\n---\nQ 830+597\nT 1427\nX 1+1+\n---\nQ 26+838 \nT 864 \nX 1+1+\n---\nQ 143+93 \nT 236 \nX 1+1+\n---\n검증 정확도 0.000%\n| 에폭 11 |  반복 1 / 351 | 시간 0[s] | 손실 1.83\n| 에폭 11 |  반복 21 / 351 | 시간 1[s] | 손실 1.83\n| 에폭 11 |  반복 41 / 351 | 시간 2[s] | 손실 1.83\n| 에폭 11 |  반복 61 / 351 | 시간 3[s] | 손실 1.83\n| 에폭 11 |  반복 81 / 351 | 시간 4[s] | 손실 1.83\n| 에폭 11 |  반복 101 / 351 | 시간 5[s] | 손실 1.83\n| 에폭 11 |  반복 121 / 351 | 시간 7[s] | 손실 1.83\n| 에폭 11 |  반복 141 / 351 | 시간 8[s] | 손실 1.83\n| 에폭 11 |  반복 161 / 351 | 시간 9[s] | 손실 1.83\n| 에폭 11 |  반복 181 / 351 | 시간 11[s] | 손실 1.83\n| 에폭 11 |  반복 201 / 351 | 시간 12[s] | 손실 1.83\n| 에폭 11 |  반복 221 / 351 | 시간 14[s] | 손실 1.83\n| 에폭 11 |  반복 241 / 351 | 시간 15[s] | 손실 1.82\n| 에폭 11 |  반복 261 / 351 | 시간 16[s] | 손실 1.83\n| 에폭 11 |  반복 281 / 351 | 시간 18[s] | 손실 1.83\n| 에폭 11 |  반복 301 / 351 | 시간 19[s] | 손실 1.83\n| 에폭 11 |  반복 321 / 351 | 시간 20[s] | 손실 1.83\n| 에폭 11 |  반복 341 / 351 | 시간 21[s] | 손실 1.83\nQ 77+85  \nT 162 \nX 1+1+\n---\nQ 975+164\nT 1139\nX 1+1+\n---\nQ 582+84 \nT 666 \nX 1+1+\n---\nQ 8+155  \nT 163 \nX 1+1+\n---\nQ 367+55 \nT 422 \nX 1+1+\n---\nQ 600+257\nT 857 \nX 1+1+\n---\nQ 761+292\nT 1053\nX 1+1+\n---\nQ 830+597\nT 1427\nX 1+1+\n---\nQ 26+838 \nT 864 \nX 1+1+\n---\nQ 143+93 \nT 236 \nX 1+1+\n---\n검증 정확도 0.000%\n| 에폭 12 |  반복 1 / 351 | 시간 0[s] | 손실 1.82\n| 에폭 12 |  반복 21 / 351 | 시간 1[s] | 손실 1.83\n| 에폭 12 |  반복 41 / 351 | 시간 2[s] | 손실 1.83\n| 에폭 12 |  반복 61 / 351 | 시간 3[s] | 손실 1.83\n| 에폭 12 |  반복 81 / 351 | 시간 4[s] | 손실 1.82\n| 에폭 12 |  반복 101 / 351 | 시간 5[s] | 손실 1.83\n| 에폭 12 |  반복 121 / 351 | 시간 6[s] | 손실 1.83\n| 에폭 12 |  반복 141 / 351 | 시간 7[s] | 손실 1.83\n| 에폭 12 |  반복 161 / 351 | 시간 8[s] | 손실 1.83\n| 에폭 12 |  반복 181 / 351 | 시간 9[s] | 손실 1.83\n| 에폭 12 |  반복 201 / 351 | 시간 10[s] | 손실 1.83\n| 에폭 12 |  반복 221 / 351 | 시간 11[s] | 손실 1.82\n| 에폭 12 |  반복 241 / 351 | 시간 12[s] | 손실 1.83\n| 에폭 12 |  반복 261 / 351 | 시간 14[s] | 손실 1.83\n| 에폭 12 |  반복 281 / 351 | 시간 15[s] | 손실 1.83\n| 에폭 12 |  반복 301 / 351 | 시간 16[s] | 손실 1.83\n| 에폭 12 |  반복 321 / 351 | 시간 17[s] | 손실 1.82\n| 에폭 12 |  반복 341 / 351 | 시간 18[s] | 손실 1.83\nQ 77+85  \nT 162 \nX 1+1+\n---\nQ 975+164\nT 1139\nX 1+1+\n---\nQ 582+84 \nT 666 \nX 1+1+\n---\nQ 8+155  \nT 163 \nX 1+1+\n---\nQ 367+55 \nT 422 \nX 1+1+\n---\nQ 600+257\nT 857 \nX 1+1+\n---\nQ 761+292\nT 1053\nX 1+1+\n---\nQ 830+597\nT 1427\nX 1+1+\n---\nQ 26+838 \nT 864 \nX 1+1+\n---\nQ 143+93 \nT 236 \nX 1+1+\n---\n검증 정확도 0.000%\n| 에폭 13 |  반복 1 / 351 | 시간 0[s] | 손실 1.82\n| 에폭 13 |  반복 21 / 351 | 시간 1[s] | 손실 1.83\n| 에폭 13 |  반복 41 / 351 | 시간 2[s] | 손실 1.83\n| 에폭 13 |  반복 61 / 351 | 시간 3[s] | 손실 1.83\n| 에폭 13 |  반복 81 / 351 | 시간 4[s] | 손실 1.83\n| 에폭 13 |  반복 101 / 351 | 시간 5[s] | 손실 1.83\n| 에폭 13 |  반복 121 / 351 | 시간 6[s] | 손실 1.83\n| 에폭 13 |  반복 141 / 351 | 시간 7[s] | 손실 1.83\n| 에폭 13 |  반복 161 / 351 | 시간 8[s] | 손실 1.82\n| 에폭 13 |  반복 181 / 351 | 시간 9[s] | 손실 1.83\n| 에폭 13 |  반복 201 / 351 | 시간 10[s] | 손실 1.83\n| 에폭 13 |  반복 221 / 351 | 시간 11[s] | 손실 1.83\n| 에폭 13 |  반복 241 / 351 | 시간 13[s] | 손실 1.82\n| 에폭 13 |  반복 261 / 351 | 시간 13[s] | 손실 1.83\n| 에폭 13 |  반복 281 / 351 | 시간 14[s] | 손실 1.82\n| 에폭 13 |  반복 301 / 351 | 시간 16[s] | 손실 1.83\n| 에폭 13 |  반복 321 / 351 | 시간 17[s] | 손실 1.83\n| 에폭 13 |  반복 341 / 351 | 시간 18[s] | 손실 1.83\nQ 77+85  \nT 162 \nX 1+1+\n---\nQ 975+164\nT 1139\nX 1+1+\n---\nQ 582+84 \nT 666 \nX 1+1+\n---\nQ 8+155  \nT 163 \nX 1+1+\n---\nQ 367+55 \nT 422 \nX 1+1+\n---\nQ 600+257\nT 857 \nX 1+1+\n---\nQ 761+292\nT 1053\nX 1+1+\n---\nQ 830+597\nT 1427\nX 1+1+\n---\nQ 26+838 \nT 864 \nX 1+1+\n---\nQ 143+93 \nT 236 \nX 1+1+\n---\n검증 정확도 0.000%\n| 에폭 14 |  반복 1 / 351 | 시간 0[s] | 손실 1.83\n| 에폭 14 |  반복 21 / 351 | 시간 1[s] | 손실 1.83\n| 에폭 14 |  반복 41 / 351 | 시간 2[s] | 손실 1.83\n| 에폭 14 |  반복 61 / 351 | 시간 3[s] | 손실 1.83\n| 에폭 14 |  반복 81 / 351 | 시간 3[s] | 손실 1.83\n| 에폭 14 |  반복 101 / 351 | 시간 5[s] | 손실 1.83\n| 에폭 14 |  반복 121 / 351 | 시간 6[s] | 손실 1.83\n| 에폭 14 |  반복 141 / 351 | 시간 7[s] | 손실 1.83\n| 에폭 14 |  반복 161 / 351 | 시간 8[s] | 손실 1.82\n| 에폭 14 |  반복 181 / 351 | 시간 9[s] | 손실 1.82\n| 에폭 14 |  반복 201 / 351 | 시간 10[s] | 손실 1.82\n| 에폭 14 |  반복 221 / 351 | 시간 11[s] | 손실 1.82\n| 에폭 14 |  반복 241 / 351 | 시간 12[s] | 손실 1.83\n| 에폭 14 |  반복 261 / 351 | 시간 13[s] | 손실 1.83\n| 에폭 14 |  반복 281 / 351 | 시간 14[s] | 손실 1.83\n| 에폭 14 |  반복 301 / 351 | 시간 15[s] | 손실 1.83\n| 에폭 14 |  반복 321 / 351 | 시간 16[s] | 손실 1.83\n| 에폭 14 |  반복 341 / 351 | 시간 17[s] | 손실 1.82\nQ 77+85  \nT 162 \nX 1+1+\n---\nQ 975+164\nT 1139\nX 1+1+\n---\nQ 582+84 \nT 666 \nX 1+1+\n---\nQ 8+155  \nT 163 \nX 1+1+\n---\nQ 367+55 \nT 422 \nX 1+1+\n---\nQ 600+257\nT 857 \nX 1+1+\n---\nQ 761+292\nT 1053\nX 1+1+\n---\nQ 830+597\nT 1427\nX 1+1+\n---\nQ 26+838 \nT 864 \nX 1+1+\n---\nQ 143+93 \nT 236 \nX 1+1+\n---\n검증 정확도 0.000%\n| 에폭 15 |  반복 1 / 351 | 시간 0[s] | 손실 1.83\n| 에폭 15 |  반복 21 / 351 | 시간 1[s] | 손실 1.83\n| 에폭 15 |  반복 41 / 351 | 시간 2[s] | 손실 1.83\n| 에폭 15 |  반복 61 / 351 | 시간 2[s] | 손실 1.82\n| 에폭 15 |  반복 81 / 351 | 시간 3[s] | 손실 1.83\n| 에폭 15 |  반복 101 / 351 | 시간 4[s] | 손실 1.83\n| 에폭 15 |  반복 121 / 351 | 시간 6[s] | 손실 1.83\n| 에폭 15 |  반복 141 / 351 | 시간 7[s] | 손실 1.83\n| 에폭 15 |  반복 161 / 351 | 시간 8[s] | 손실 1.83\n| 에폭 15 |  반복 181 / 351 | 시간 9[s] | 손실 1.83\n| 에폭 15 |  반복 201 / 351 | 시간 10[s] | 손실 1.83\n| 에폭 15 |  반복 221 / 351 | 시간 11[s] | 손실 1.83\n| 에폭 15 |  반복 241 / 351 | 시간 13[s] | 손실 1.83\n| 에폭 15 |  반복 261 / 351 | 시간 13[s] | 손실 1.83\n| 에폭 15 |  반복 281 / 351 | 시간 15[s] | 손실 1.83\n| 에폭 15 |  반복 301 / 351 | 시간 16[s] | 손실 1.82\n| 에폭 15 |  반복 321 / 351 | 시간 17[s] | 손실 1.83\n| 에폭 15 |  반복 341 / 351 | 시간 18[s] | 손실 1.83\nQ 77+85  \nT 162 \nX 1+1+\n---\nQ 975+164\nT 1139\nX 1+1+\n---\nQ 582+84 \nT 666 \nX 1+1+\n---\nQ 8+155  \nT 163 \nX 1+1+\n---\nQ 367+55 \nT 422 \nX 1+1+\n---\nQ 600+257\nT 857 \nX 1+1+\n---\nQ 761+292\nT 1053\nX 1+1+\n---\nQ 830+597\nT 1427\nX 1+1+\n---\nQ 26+838 \nT 864 \nX 1+1+\n---\nQ 143+93 \nT 236 \nX 1+1+\n---\n검증 정확도 0.000%\n| 에폭 16 |  반복 1 / 351 | 시간 0[s] | 손실 1.81\n| 에폭 16 |  반복 21 / 351 | 시간 1[s] | 손실 1.83\n| 에폭 16 |  반복 41 / 351 | 시간 2[s] | 손실 1.83\n| 에폭 16 |  반복 61 / 351 | 시간 3[s] | 손실 1.83\n| 에폭 16 |  반복 81 / 351 | 시간 3[s] | 손실 1.83\n| 에폭 16 |  반복 101 / 351 | 시간 4[s] | 손실 1.83\n| 에폭 16 |  반복 121 / 351 | 시간 5[s] | 손실 1.83\n| 에폭 16 |  반복 141 / 351 | 시간 7[s] | 손실 1.83\n| 에폭 16 |  반복 161 / 351 | 시간 8[s] | 손실 1.83\n| 에폭 16 |  반복 181 / 351 | 시간 9[s] | 손실 1.83\n| 에폭 16 |  반복 201 / 351 | 시간 10[s] | 손실 1.83\n| 에폭 16 |  반복 221 / 351 | 시간 11[s] | 손실 1.83\n| 에폭 16 |  반복 241 / 351 | 시간 12[s] | 손실 1.82\n| 에폭 16 |  반복 261 / 351 | 시간 13[s] | 손실 1.83\n| 에폭 16 |  반복 281 / 351 | 시간 14[s] | 손실 1.82\n| 에폭 16 |  반복 301 / 351 | 시간 15[s] | 손실 1.83\n| 에폭 16 |  반복 321 / 351 | 시간 16[s] | 손실 1.83\n| 에폭 16 |  반복 341 / 351 | 시간 17[s] | 손실 1.83\nQ 77+85  \nT 162 \nX 1+1+\n---\nQ 975+164\nT 1139\nX 1+1+\n---\nQ 582+84 \nT 666 \nX 1+1+\n---\nQ 8+155  \nT 163 \nX 1+1+\n---\nQ 367+55 \nT 422 \nX 1+1+\n---\nQ 600+257\nT 857 \nX 1+1+\n---\nQ 761+292\nT 1053\nX 1+1+\n---\nQ 830+597\nT 1427\nX 1+1+\n---\nQ 26+838 \nT 864 \nX 1+1+\n---\nQ 143+93 \nT 236 \nX 1+1+\n---\n검증 정확도 0.000%\n| 에폭 17 |  반복 1 / 351 | 시간 0[s] | 손실 1.81\n| 에폭 17 |  반복 21 / 351 | 시간 1[s] | 손실 1.83\n| 에폭 17 |  반복 41 / 351 | 시간 2[s] | 손실 1.83\n| 에폭 17 |  반복 61 / 351 | 시간 3[s] | 손실 1.83\n| 에폭 17 |  반복 81 / 351 | 시간 4[s] | 손실 1.83\n| 에폭 17 |  반복 101 / 351 | 시간 5[s] | 손실 1.83\n| 에폭 17 |  반복 121 / 351 | 시간 6[s] | 손실 1.83\n| 에폭 17 |  반복 141 / 351 | 시간 7[s] | 손실 1.82\n| 에폭 17 |  반복 161 / 351 | 시간 8[s] | 손실 1.83\n| 에폭 17 |  반복 181 / 351 | 시간 9[s] | 손실 1.83\n| 에폭 17 |  반복 201 / 351 | 시간 11[s] | 손실 1.83\n| 에폭 17 |  반복 221 / 351 | 시간 12[s] | 손실 1.83\n| 에폭 17 |  반복 241 / 351 | 시간 14[s] | 손실 1.83\n| 에폭 17 |  반복 261 / 351 | 시간 15[s] | 손실 1.82\n| 에폭 17 |  반복 281 / 351 | 시간 16[s] | 손실 1.83\n| 에폭 17 |  반복 301 / 351 | 시간 17[s] | 손실 1.82\n| 에폭 17 |  반복 321 / 351 | 시간 18[s] | 손실 1.83\n| 에폭 17 |  반복 341 / 351 | 시간 20[s] | 손실 1.83\nQ 77+85  \nT 162 \nX 1+1+\n---\nQ 975+164\nT 1139\nX 1+1+\n---\nQ 582+84 \nT 666 \nX 1+1+\n---\nQ 8+155  \nT 163 \nX 1+1+\n---\nQ 367+55 \nT 422 \nX 1+1+\n---\nQ 600+257\nT 857 \nX 1+1+\n---\nQ 761+292\nT 1053\nX 1+1+\n---\nQ 830+597\nT 1427\nX 1+1+\n---\nQ 26+838 \nT 864 \nX 1+1+\n---\nQ 143+93 \nT 236 \nX 1+1+\n---\n검증 정확도 0.000%\n| 에폭 18 |  반복 1 / 351 | 시간 0[s] | 손실 1.83\n| 에폭 18 |  반복 21 / 351 | 시간 1[s] | 손실 1.82\n| 에폭 18 |  반복 41 / 351 | 시간 2[s] | 손실 1.83\n| 에폭 18 |  반복 61 / 351 | 시간 3[s] | 손실 1.83\n| 에폭 18 |  반복 81 / 351 | 시간 4[s] | 손실 1.82\n| 에폭 18 |  반복 101 / 351 | 시간 5[s] | 손실 1.83\n| 에폭 18 |  반복 121 / 351 | 시간 6[s] | 손실 1.83\n| 에폭 18 |  반복 141 / 351 | 시간 8[s] | 손실 1.83\n| 에폭 18 |  반복 161 / 351 | 시간 9[s] | 손실 1.83\n| 에폭 18 |  반복 181 / 351 | 시간 11[s] | 손실 1.82\n| 에폭 18 |  반복 201 / 351 | 시간 12[s] | 손실 1.83\n| 에폭 18 |  반복 221 / 351 | 시간 14[s] | 손실 1.83\n| 에폭 18 |  반복 241 / 351 | 시간 15[s] | 손실 1.83\n| 에폭 18 |  반복 261 / 351 | 시간 16[s] | 손실 1.83\n| 에폭 18 |  반복 281 / 351 | 시간 17[s] | 손실 1.82\n| 에폭 18 |  반복 301 / 351 | 시간 18[s] | 손실 1.83\n| 에폭 18 |  반복 321 / 351 | 시간 20[s] | 손실 1.82\n| 에폭 18 |  반복 341 / 351 | 시간 21[s] | 손실 1.83\nQ 77+85  \nT 162 \nX 1+1+\n---\nQ 975+164\nT 1139\nX 1+1+\n---\nQ 582+84 \nT 666 \nX 1+1+\n---\nQ 8+155  \nT 163 \nX 1+1+\n---\nQ 367+55 \nT 422 \nX 1+1+\n---\nQ 600+257\nT 857 \nX 1+1+\n---\nQ 761+292\nT 1053\nX 1+1+\n---\nQ 830+597\nT 1427\nX 1+1+\n---\nQ 26+838 \nT 864 \nX 1+1+\n---\nQ 143+93 \nT 236 \nX 1+1+\n---\n검증 정확도 0.000%\n| 에폭 19 |  반복 1 / 351 | 시간 0[s] | 손실 1.85\n| 에폭 19 |  반복 21 / 351 | 시간 1[s] | 손실 1.83\n| 에폭 19 |  반복 41 / 351 | 시간 1[s] | 손실 1.83\n| 에폭 19 |  반복 61 / 351 | 시간 2[s] | 손실 1.83\n| 에폭 19 |  반복 81 / 351 | 시간 3[s] | 손실 1.83\n| 에폭 19 |  반복 101 / 351 | 시간 4[s] | 손실 1.82\n| 에폭 19 |  반복 121 / 351 | 시간 5[s] | 손실 1.83\n| 에폭 19 |  반복 141 / 351 | 시간 6[s] | 손실 1.83\n| 에폭 19 |  반복 161 / 351 | 시간 7[s] | 손실 1.83\n| 에폭 19 |  반복 181 / 351 | 시간 8[s] | 손실 1.82\n| 에폭 19 |  반복 201 / 351 | 시간 9[s] | 손실 1.83\n| 에폭 19 |  반복 221 / 351 | 시간 10[s] | 손실 1.82\n| 에폭 19 |  반복 241 / 351 | 시간 10[s] | 손실 1.83\n| 에폭 19 |  반복 261 / 351 | 시간 11[s] | 손실 1.83\n| 에폭 19 |  반복 281 / 351 | 시간 12[s] | 손실 1.83\n| 에폭 19 |  반복 301 / 351 | 시간 13[s] | 손실 1.83\n| 에폭 19 |  반복 321 / 351 | 시간 14[s] | 손실 1.83\n| 에폭 19 |  반복 341 / 351 | 시간 15[s] | 손실 1.83\nQ 77+85  \nT 162 \nX 1+1+\n---\nQ 975+164\nT 1139\nX 1+1+\n---\nQ 582+84 \nT 666 \nX 1+1+\n---\nQ 8+155  \nT 163 \nX 1+1+\n---\nQ 367+55 \nT 422 \nX 1+1+\n---\nQ 600+257\nT 857 \nX 1+1+\n---\nQ 761+292\nT 1053\nX 1+1+\n---\nQ 830+597\nT 1427\nX 1+1+\n---\nQ 26+838 \nT 864 \nX 1+1+\n---\nQ 143+93 \nT 236 \nX 1+1+\n---\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-11c8fc89cd4f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mquestion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mcorrect_num\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0meval_seq2seq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid_to_char\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m     \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorrect_num\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0macc_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Develop\\Machine-Learning\\밑바닥부터 시작하는 딥러닝2\\common\\util.py\u001b[0m in \u001b[0;36meval_seq2seq\u001b[1;34m(model, question, correct, id_to_char, verbos, is_reverse)\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[0mstart_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m     \u001b[0mguess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m     \u001b[1;31m# 문자열로 변환\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-5be39656500a>\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self, xs, start_id, sample_size)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0msampled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msampled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-0b1763298d12>\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self, h, start_id, sample_size)\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m             \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maffine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Develop\\Machine-Learning\\밑바닥부터 시작하는 딥러닝2\\common\\time_layers.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, xs)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 192\u001b[1;33m             \u001b[0mlayer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m             \u001b[0mhs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Develop\\Machine-Learning\\밑바닥부터 시작하는 딥러닝2\\common\\time_layers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, Wx, Wh, b)\u001b[0m\n\u001b[0;32m    105\u001b[0m         '''\n\u001b[0;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mWx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mzeros_like\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36mzeros_like\u001b[1;34m(a, dtype, order, subok, shape)\u001b[0m\n\u001b[0;32m    153\u001b[0m     \u001b[1;31m# needed instead of a 0 to get same result as zeros for for string dtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m     \u001b[0mmultiarray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopyto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'unsafe'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mcopyto\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from common.optimizer import Adam\n",
    "from common.trainer import Trainer\n",
    "from common.util import eval_seq2seq\n",
    "\n",
    "# 데이터셋 읽기\n",
    "(x_train, t_train), (x_test, t_test) = sequence.load_data('addition.txt')\n",
    "char_to_id, id_to_char = sequence.get_vocab()\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "vocab_size = len(char_to_id)\n",
    "wordvec_size = 16\n",
    "hidden_size = 128\n",
    "batch_size = 128\n",
    "max_epoch = 25\n",
    "max_grad = 5.0\n",
    "\n",
    "# 모델 / 옵티마이저 / 트레이너 생성\n",
    "model = Seq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "acc_list = []\n",
    "for epoch in range(max_epoch):\n",
    "    trainer.fit(x_train, t_train, max_epoch=max_epoch, batch_size=batch_size, max_grad=max_grad)\n",
    "\n",
    "    correct_num = 0\n",
    "    for i in range(len(x_test)):\n",
    "        question, correct = x_test[[i]], t_test[[i]]\n",
    "        verbose = i < 10\n",
    "        correct_num += eval_seq2seq(model, question, correct, id_to_char, verbose)\n",
    "    acc = float(correct_num) / len(x_test)\n",
    "    acc_list.append(acc)\n",
    "    print('검증 정확도 %.3f%%' % (acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}